\documentclass{tufte-handout}

%\geometry{showframe}% for debugging purposes -- displays the margins

\usepackage{amsmath}


% Set up the images/graphics package
\usepackage{graphicx}
\setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
\graphicspath{{graphics/}}

\usepackage{tikz-cd}


% The following package makes prettier tables.  We're all about the bling!
\usepackage{booktabs}

% The units package provides nice, non-stacked fractions and better spacing
% for units.
\usepackage{units}

% The fancyvrb package lets us customize the formatting of verbatim
% environments.  We use a slightly smaller font.
\usepackage{fancyvrb}
\fvset{fontsize=\normalsize}

% Small sections of multiple columns
\usepackage{multicol}

% Provides paragraphs of dummy text
\usepackage{lipsum}

% Provides annotated music scores
\usepackage{musicography}

% Fancy header
\usepackage{fancyhdr}

\usepackage{array, boldline, makecell, booktabs}

\usepackage{mathtools}


\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proof}{Proof}


\newcommand{\bmp}[1]{\begin{minipage}{#1}}
\newcommand{\emp}{\end{minipage}}
\newcommand{\tw}{\textwidth}

% These commands are used to pretty-print LaTeX commands
\newcommand{\doccmd}[1]{\texttt{\textbackslash#1}}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newenvironment{docspec}{\begin{quote}\noindent}{\end{quote}}% command specification environment
\newcommand{\docenv}[1]{\textsf{#1}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}}% document class option name


%%% Additions to template by DSL
\usepackage{hyperref} % provides \url{}
% remove separation between list items http://tex.stackexchange.com/a/10689/1783
\usepackage{enumitem}
\setlist{nosep}

%%-- another way
\usepackage{tikzpagenodes}
\newcommand{\mylogo}[1]{%
\tikz[remember picture,overlay] {%
  \node[inner sep=0pt,anchor=east] at ([yshift=+1.0cm, xshift=+7.0cm]current page text area.north east){#1};}
  }

\usepackage{mathrsfs}
\usepackage{tcolorbox}
\tcbuselibrary{theorems}
\usepackage{xcolor}
\usepackage{epsdice}
\usepackage{pgfplots}
\usepackage{bm}

\usepackage{graphicx,xspace,color,cancel}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes,backgrounds,through,shadows}
\usetikzlibrary{decorations.pathmorphing}
\usetikzlibrary{calc}

\newcommand{\norm}[1]{\left\| #1 \right\|}

\newcommand{\boxit}[4]
{
\path #2+(-#4,-#4) node(bottomleft){};
\path #3+(#4,#4) node(upperright){};
\draw[rounded corners,black!80] (bottomleft) rectangle (upperright);
%\path (#2-|#3) node(bottomright){};
\path (bottomleft-|upperright) node(bottomright){};
\path (bottomright) node[above left]{#1};
}

\tikzstyle{cont}=[circle, draw,% a shading that is white at the top...
thick,minimum size=6mm,line width=1pt,>=stealth]  % continuous  node
\tikzstyle{dgraph}=[->, line width=1.5pt]

\tikzstyle{contblk}=[circle, draw=black,top color=black,bottom color=black,% a shading that is white at the top...
thick,minimum size=1mm,line width=1pt,>=stealth]  % continuous  node
\tikzstyle{dgraph}=[->, line width=1.5pt]



\newtcbtheorem[number within=]{mydef}{Definition}%
{colback=DarkOrchid!5,colframe=DarkOrchid!90!black,fonttitle=\bfseries}{th}

\newtcbtheorem[number within=]{mydef2}{Definition$^*$}%
{colback=Rhodamine!5,colframe=Rhodamine!90!black,fonttitle=\bfseries}{th}


\newtcbtheorem[number within=]{mybox}{Box}%
{colback=Aquamarine!5,colframe=Aquamarine!90!black,fonttitle=\bfseries}{th}

\newtcbtheorem[number within=]{mybox2}{Box$^*$}%
{colback=gray!5,colframe=gray!90!black,fonttitle=\bfseries}{th}

\newtcbtheorem[number within=]{mythe}{Theorem}%
{colback=DarkOrchid!5,colframe=DarkOrchid!90!black,fonttitle=\bfseries}{th}



% \bigcupdot
\makeatletter
\def\moverlay{\mathpalette\mov@rlay}
\def\mov@rlay#1#2{\leavevmode\vtop{%
   \baselineskip\z@skip \lineskiplimit-\maxdimen
   \ialign{\hfil$\m@th#1##$\hfil\cr#2\crcr}}}
\newcommand{\charfusion}[3][\mathord]{
    #1{\ifx#1\mathop\vphantom{#2}\fi
        \mathpalette\mov@rlay{#2\cr#3}
      }
    \ifx#1\mathop\expandafter\displaylimits\fi}
\makeatother
\newcommand{\cupdot}{\charfusion[\mathbin]{\cup}{\cdot}}
\newcommand{\bigcupdot}{\charfusion[\mathop]{\bigcup}{\boldsymbol{\cdot}}}

%\rhead{\includegraphics[width=1cm]{fig/T1.png}}
\def\ci{\perp\!\!\!\perp}

\title{Machine Learning  }
\author{Salvador Ruiz Correa}
\date{January 19, 2026}  % if the \date{} command is left out, the current date will be used
\begin{document}

\maketitle% this prints the handout title, author, and date
\mylogo{\includegraphics[height=15mm]{fig/T2.png}}

\begin{abstract}
\noindent \textsc{ This brief review} synthesizes key mathematical structures that form the conceptual backbone of machine learning. We begin with fundamental algebraic structures—groups, rings, and fields—culminating in the definition of a vector space, where data are typically represented. Essential vector space concepts such as linear independence, span, and basis are introduced, highlighting their role in feature representation.

The discussion then progresses to structures that equip vector spaces with analytical tools: metric spaces for measuring similarity, normed spaces for vector magnitude, and inner product spaces for angles and orthogonality. This sequence leads to Hilbert spaces—complete inner product spaces central to functional analysis and machine learning theory. Within Hilbert spaces, we outline the orthogonal projection theorem, which underpins optimization and approximation methods, and the Riesz representation theorem, linking linear functionals to inner products, a foundation for kernel methods.

Finally, we introduce Mercer’s theorem, which connects positive-definite kernels to inner products in Hilbert spaces, thereby enabling the kernel trick—a pivotal technique for transforming nonlinear problems into linear ones in higher-dimensional spaces. This concise journey illustrates how abstract mathematical frameworks provide the rigorous foundations for many algorithms and theoretical guarantees in machine learning.\end{abstract}
\marginnote[-5cm]{
\textsc{Agenda:}
\begin{description}
\item[1] Algebraic Structures. 
\item[2] Spaces (metric, normed, inner-product, Hilbert.
\item[3] Mercer Theorem.
\end{description}
}



\section*{Introduction}
Machine Learning (ML) models data mathematically. To understand the core algorithms (linear regression, SVMs, neural network layers), we must first understand the \textbf{structures} in which data "lives." We begin with the most abstract building blocks and build up to the essential concept of a \textbf{vector space}.

\section{Basic Algebraic Structures}
These structures define sets equipped with operations obeying specific axioms.

\subsection{Groups}
A group formalizes symmetry and reversible transformations.

\begin{mydef}{Group}{theoexample}   
\begin{definition}[Group]
    A \textbf{group} $(G, \ast)$ is a set $G$ with a binary operation $\ast: G \times G \to G$ such that:
    \begin{enumerate}
        \item \textbf{Closure:} $\forall a,b \in G,\ a \ast b \in G$.
        \item \textbf{Associativity:} $\forall a,b,c \in G,\ (a \ast b) \ast c = a \ast (b \ast c)$.
        \item \textbf{Identity:} $\exists e \in G$ such that $\forall a \in G,\ a \ast e = e \ast a = a$.
        \item \textbf{Inverse:} $\forall a \in G,\ \exists b \in G$ such that $a \ast b = b \ast a = e$. (We write $b = a^{-1}$).
    \end{enumerate}
    If $\forall a,b, a\ast b = b\ast a$, the group is \textbf{abelian} (commutative).
\end{definition}
\end{mydef}

\begin{example}
    $(\mathbb{Z}, +)$ is an abelian group (identity 0, inverse $-n$). $(\mathbb{R}\setminus\{0\}, \times)$ is an abelian group (identity 1, inverse $1/x$).
\end{example}
\textbf{ML Context:} The set of all permutations of data features forms a (non-abelian) group.

\subsection{Rings}
Rings have two operations, often thought of as addition and multiplication.
\begin{mydef}{Ring}{theoexample}   
\begin{definition}[Ring]
    A \textbf{ring} $(R, +, \cdot)$ is a set $R$ with two operations such that:
    \begin{enumerate}
        \item $(R, +)$ is an \textbf{abelian group} (identity denoted 0).
        \item \textbf{Multiplication Associativity:} $\forall a,b,c \in R,\ (a \cdot b) \cdot c = a \cdot (b \cdot c)$.
        \item \textbf{Distributivity:} $a \cdot (b + c) = a \cdot b + a \cdot c$ and $(a + b) \cdot c = a \cdot c + b \cdot c$.
    \end{enumerate}
    If multiplication is commutative ($a\cdot b = b\cdot a$), it's a \textbf{commutative ring}. If there is a multiplicative identity (denoted 1), it's a \textbf{ring with unity}.
\end{definition}
\end{mydef}

\begin{example}
    $\mathbb{Z}$ (integers), $\mathbb{R}^{n\times n}$ (square matrices) are rings. Matrices show a non-commutative ring.
\end{example}
\textbf{ML Context:} The arithmetic of weights and inputs in a model often occurs in a ring.

\subsection{Fields}
Fields are rings where division (except by zero) is possible.
\begin{mydef}{Fields}{theoexample} 
\begin{definition}[Field]
    A \textbf{field} $\mathbb{F}$ is a commutative ring with unity where every non-zero element has a multiplicative inverse. Formally, $(\mathbb{F}\setminus\{0\}, \cdot)$ is an abelian group.
\end{definition}
\begin{example}
    $\mathbb{Q}$ (rationals), $\mathbb{R}$ (reals), $\mathbb{C}$ (complex numbers) are fields. $\mathbb{Z}$ is \textbf{not} a field (no inverse for 2 in $\mathbb{Z}$).
\end{example}
\textbf{ML Context:} Almost all numerical ML uses real numbers $\mathbb{R}$ or sometimes complex numbers $\mathbb{C}$ as the underlying scalar field.
\end{mydef}

\section{Vector Spaces: Where Data Lives}
A vector space combines a field of scalars with an abelian group of vectors.

\begin{mydef}{Vector Space}{theoexample} 
\begin{definition}[Vector Space]
    Let $\mathbb{F}$ be a field. A \textbf{vector space} $V$ over $\mathbb{F}$ is a set $V$ equipped with:
    \begin{itemize}
        \item \textbf{Vector Addition:} $+: V \times V \to V$, making $(V, +)$ an \textbf{abelian group}.
        \item \textbf{Scalar Multiplication:} $\cdot: \mathbb{F} \times V \to V$.
    \end{itemize}
    These operations must satisfy $\forall \alpha, \beta \in \mathbb{F}, \ \mathbf{u}, \mathbf{v} \in V$:
    \begin{enumerate}
        \item $\alpha(\mathbf{u}+\mathbf{v}) = \alpha\mathbf{u} + \alpha\mathbf{v}$
        \item $(\alpha+\beta)\mathbf{u} = \alpha\mathbf{u} + \beta\mathbf{u}$
        \item $(\alpha\beta)\mathbf{u} = \alpha(\beta\mathbf{u})$
        \item $1 \cdot \mathbf{u} = \mathbf{u}$ (where 1 is the multiplicative identity in $\mathbb{F}$)
    \end{enumerate}
\end{definition}
\begin{example}
    $\mathbb{R}^n$ over $\mathbb{R}$ is the canonical example. The set of all $m\times n$ matrices over $\mathbb{R}$ is a vector space. Functions $f: \mathbb{R} \to \mathbb{R}$ also form a vector space.
\end{example}
\textbf{ML Context:} A single data point with $n$ features is a vector in $\mathbb{R}^n$. A dataset of $m$ points is a set (or matrix) of vectors.

\section{Essential Vector Space Concepts}
\subsection{Linear Combination, Span, and Subspaces}
\begin{definition}[Linear Combination]
    Given vectors $\mathbf{v}_1, \ldots, \mathbf{v}_k \in V$ and scalars $\alpha_1, \ldots, \alpha_k \in \mathbb{F}$, the vector $\alpha_1\mathbf{v}_1 + \ldots + \alpha_k\mathbf{v}_k$ is a \textbf{linear combination}.
\end{definition}
\end{mydef}

\begin{definition}[Span]
    The \textbf{span} of a set $S = \{\mathbf{v}_1, \ldots, \mathbf{v}_k\}$ is the set of \textit{all} linear combinations of those vectors:
    \[
    \text{span}(S) = \left\{ \sum_{i=1}^k \alpha_i \mathbf{v}_i : \alpha_i \in \mathbb{F} \right\}.
    \]
    This is always a \textbf{subspace} (a vector space contained within $V$).
\end{definition}
\textbf{ML Context:} The span represents all possible points that can be constructed (e.g., modeled) using a given set of feature vectors. A model's hypothesis space is often a subspace.

\subsection{Linear Independence and Basis}
\begin{definition}[Linear Independence]
    A set $S = \{\mathbf{v}_1, \ldots, \mathbf{v}_k\}$ is \textbf{linearly independent} if the equation
    \[
    \alpha_1\mathbf{v}_1 + \ldots + \alpha_k\mathbf{v}_k = \mathbf{0}
    \]
    has \textbf{only} the \textit{trivial solution} $\alpha_1 = \ldots = \alpha_k = 0$. Otherwise, the set is \textbf{linearly dependent}.
\end{definition}
\textbf{Interpretation:} Independence means no vector in $S$ is redundant; it cannot be written as a combination of the others.

\begin{definition}[Basis and Dimension]
    A \textbf{basis} $\mathcal{B}$ for a vector space $V$ is a set of vectors that is:
    \begin{enumerate}
        \item \textbf{Linearly Independent}
        \item \textbf{Spans} $V$ (i.e., $\text{span}(\mathcal{B}) = V$)
    \end{enumerate}
    The \textbf{dimension} $\text{dim}(V)$ is the number of vectors in any basis for $V$. Every vector $\mathbf{v} \in V$ can be expressed \textbf{uniquely} as a linear combination of basis vectors.
\end{definition}

\begin{example}
    The \textbf{standard basis} for $\mathbb{R}^3$: $\mathcal{B} = \{(1,0,0), (0,1,0), (0,0,1)\}$. Any vector $(x,y,z) = x(1,0,0) + y(0,1,0) + z(0,0,1)$.
\end{example}

\subsection{Role in Feature Representation}
This is the core connection to ML:
\begin{itemize}
    \item \textbf{Feature Vector:} A data point $\mathbf{x} = [x_1, \ldots, x_n]^T \in \mathbb{R}^n$ is a vector.
    \item \textbf{Feature Space:} The vector space $\mathbb{R}^n$ is the \textbf{feature space}.
    \item \textbf{Basis as Feature Directions:} Each basis vector $\mathbf{e}_i$ can represent a fundamental, independent \textbf{direction} or \textbf{concept} in the feature space (e.g., ``pixel intensity at location i,'' ``word count for word i'').
    \item \textbf{Coefficients as Representations:} The coordinates $(x_1, \ldots, x_n)$ of $\mathbf{x}$ relative to the standard basis \textit{are} the feature values. Changing the basis is like changing the \textbf{perspective} or \textbf{coordinate system} for viewing the data.
    \item \textbf{Dimensionality Reduction:} If your data points are linearly dependent, the true ``intrinsic'' dimension is less than $n$. Finding a smaller basis that \textit{approximately} spans the data (e.g., via PCA) is the goal of dimensionality reduction.
\end{itemize}

\section*{Summary}
We built a hierarchy: \textbf{Group} $\to$ \textbf{Ring} $\to$ \textbf{Field} $\to$ \textbf{Vector Space}.
\begin{itemize}
    \item A vector space over a field is the primary stage for numerical data.
    \item Concepts of \textbf{span, linear independence, and basis} allow us to discuss representation, dimensionality, and transformations of data.
\end{itemize}

\section{Metric Spaces}
\begin{mydef}{Metric Space}{theoexample} 
\begin{definition}[Metric Space]
A \textbf{metric space} is an ordered pair $(X, d)$ consisting of:
\begin{itemize}
    \item A set $X$ (whose elements are called "points")
    \item A function $d: X \times X \rightarrow \mathbb R$ (called a \textbf{metric} or \textbf{distance function})
\end{itemize}
satisfying the following axioms for all $x, y, z \in X$:
\begin{enumerate}
    \item {Non-negativity}: $d(x, y) \geq 0$
    
    \item {Identity of indiscernibles}: $d(x, y) = 0 \iff x = y$
    
    \item {Symmetry}: $d(x, y) = d(y, x)$
    
    \item {Triangle inequality}: $d(x, z) \leq d(x, y) + d(y, z)$
\end{enumerate}
\end{definition}
\end{mydef}

\section{Types of Distances (Metrics)}

\subsection{Standard Metrics on $\mathbb R^n$}

\begin{definition}[$\ell^p$ Metrics]
For $p \geq 1$, the $\ell^p$ metric on $\mathbb R^n$ is:
\[
d_p(x, y) = \left(\sum_{i=1}^n |x_i - y_i|^p\right)^{1/p}
\]
where $x = (x_1, \dots, x_n)$, $y = (y_1, \dots, y_n) \in \mathbb R^n$.
\end{definition}

\begin{example}[Important Special Cases]
\begin{enumerate}
    \item \textbf{Euclidean distance} ($\ell^2$ metric):
    \[
    d_2(x, y) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}
    \]
    
    \item \textbf{Manhattan distance} ($\ell^1$ metric):
    \[
    d_1(x, y) = \sum_{i=1}^n |x_i - y_i|
    \]
    
    \item \textbf{Chebyshev distance} ($\ell^\infty$ metric):
    \[
    d_\infty(x, y) = \max_{1 \leq i \leq n} |x_i - y_i|
    \]
\end{enumerate}
\end{example}



\subsection{Discrete Metric}

\begin{definition}[Discrete Metric]
For any set $X$, the \textbf{discrete metric} is:
\[
d(x, y) = 
\begin{cases}
0 & \text{if } x = y \\
1 & \text{if } x \neq y
\end{cases}
\]
\end{definition}

\begin{example}
On $X = \{a, b, c\}$:
\[
d(a,b) = 1, \quad d(b,c) = 1, \quad d(a,c) = 1, \quad d(a,a) = 0
\]
All distinct points are exactly "1 unit" apart.
\end{example}

\subsection{Metrics on Function Spaces}

\begin{definition}[Uniform Metric]
On $C[a,b]$ (continuous functions on $[a,b]$):
\[
d_\infty(f, g) = \sup_{x \in [a,b]} |f(x) - g(x)|
\]
Also called the \textbf{supremum metric} or \textbf{Chebyshev metric}.
\end{definition}

\begin{definition}[$L^p$ Metrics]
On appropriate function spaces:
\[
d_p(f, g) = \left(\int_a^b |f(x) - g(x)|^p dx\right)^{1/p}
\]
Special cases:
\begin{itemize}
    \item $p=1$: $d_1(f,g) = \int_a^b |f(x)-g(x)| dx$ (total area between curves)
    \item $p=2$: $d_2(f,g) = \sqrt{\int_a^b |f(x)-g(x)|^2 dx}$ (root mean square distance)
\end{itemize}
\end{definition}

\subsection{Metrics on Sequence Spaces}

\begin{definition}[$\ell^p$ Sequence Spaces]
For sequences $(a_n), (b_n)$:
\[
d_p((a_n), (b_n)) = \left(\sum_{n=1}^\infty |a_n - b_n|^p\right)^{1/p}
\]
provided the sum converges.
\end{definition}

\subsection{Specialized Metrics}

\begin{definition}[Hamming Distance]
For strings of equal length $x, y \in \{0,1\}^n$ (or any alphabet):
\[
d_H(x, y) = \text{number of positions where } x_i \neq y_i
\]
\end{definition}

\begin{definition}[Cosine Distance]
For vectors $x, y \in \mathbb R^n$:
\[
d_{\text{cos}}(x, y) = 1 - \frac{\langle x, y \rangle}{\norm{x}\norm{y}}
\]
where $\langle x, y \rangle$ is the dot product.
\end{definition}

\begin{definition}[Mahalanobis Distance]
For vectors $x, y \in \mathbb R^n$ with covariance matrix $\Sigma$:
\[
d_M(x, y) = \sqrt{(x-y)^T \Sigma^{-1} (x-y)}
\]
\end{definition}

\begin{definition}[Normed Space]
A \textbf{normed space} (or normed vector space) is a pair $(V, \|\cdot\|)$ where $V$ is a vector space over a field $\mathbb{F}$ (usually $\mathbb{R}$ or $\mathbb{C}$) and $\|\cdot\|: V \to [0,\infty)$ is a function called a \textbf{norm} that satisfies the following properties for all $x, y \in V$ and all scalars $\alpha \in \mathbb{F}$:

\begin{enumerate}
    \item \textbf{Positivity:} $\|x\| \geq 0$, and $\|x\| = 0$ if and only if $x = 0$
    \item \textbf{Homogeneity:} $\|\alpha x\| = |\alpha| \cdot \|x\|$
    \item \textbf{Triangle Inequality:} $\|x + y\| \leq \|x\| + \|y\|$
\end{enumerate}
\end{definition}

\section{Important Examples}

\begin{example}[Euclidean Space $\mathbb{R}^n$]
The vector space $\mathbb{R}^n$ with the Euclidean norm (or 2-norm) defined by
\[
\|x\|_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2} = \sqrt{\sum_{i=1}^n x_i^2}
\]
for $x = (x_1, x_2, \ldots, x_n) \in \mathbb{R}^n$ is a normed space.
\end{example}

\begin{example}[$p$-norms on $\mathbb{R}^n$]
For $1 \leq p < \infty$, the $p$-norm on $\mathbb{R}^n$ is defined by
\[
\|x\|_p = \left(\sum_{i=1}^n |x_i|^p\right)^{1/p}
\]
Special cases:
\begin{itemize}
    \item $p = 1$: $\|x\|_1 = |x_1| + |x_2| + \cdots + |x_n|$ (Manhattan norm)
    \item $p = 2$: $\|x\|_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}$ (Euclidean norm)
    \item $p = \infty$: $\|x\|_\infty = \max_{1 \leq i \leq n} |x_i|$ (supremum norm)
\end{itemize}
\end{example}

\begin{example}[Space of Continuous Functions]
Let $C[a,b]$ be the vector space of continuous real-valued functions on the closed interval $[a,b]$. The supremum norm (or uniform norm) is defined by
\[
\|f\|_\infty = \sup_{x \in [a,b]} |f(x)| = \max_{x \in [a,b]} |f(x)|
\]
This makes $(C[a,b], \|\cdot\|_\infty)$ a normed space.
\end{example}

\begin{example}[$L^p$ Spaces]
For $1 \leq p < \infty$, the space $L^p[a,b]$ consists of measurable functions $f: [a,b] \to \mathbb{R}$ such that
\[
\|f\|_p = \left(\int_a^b |f(x)|^p \, dx\right)^{1/p} < \infty
\]
This defines a norm on $L^p[a,b]$ (technically on equivalence classes of functions that differ only on sets of measure zero).
\end{example}

\begin{example}[Sequence Spaces $\ell^p$]
For $1 \leq p < \infty$, the space $\ell^p$ consists of all sequences $(x_n)_{n=1}^\infty$ of real (or complex) numbers such that
\[
\|x\|_p = \left(\sum_{n=1}^\infty |x_n|^p\right)^{1/p} < \infty
\]
For $p = \infty$, the space $\ell^\infty$ consists of bounded sequences with norm
\[
\|x\|_\infty = \sup_{n \in \mathbb{N}} |x_n|
\]
\end{example}

\begin{example}[Matrix Norms]
The space $\mathbb{R}^{m \times n}$ of $m \times n$ real matrices can be equipped with various norms, such as the Frobenius norm:
\[
\|A\|_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2}
\]
or the operator norm (induced norm):
\[
\|A\|_{op} = \sup_{\|x\|=1} \|Ax\|
\]
\end{example}

\section{Properties and Remarks}

\begin{theorem}[Equivalence of Norms in Finite Dimensions]
In a finite-dimensional vector space, all norms are equivalent. That is, if $\|\cdot\|_\alpha$ and $\|\cdot\|_\beta$ are two norms on a finite-dimensional space $V$, then there exist constants $c, C > 0$ such that
\[
c\|x\|_\alpha \leq \|x\|_\beta \leq C\|x\|_\alpha
\]
for all $x \in V$.
\end{theorem}

\textbf{Remark:} Every normed space is a metric space with the metric induced by the norm:
\[
d(x,y) = \|x - y\|
\]

\textbf{Remark:} A normed space that is complete with respect to the metric induced by its norm is called a \textbf{Banach space}.

\begin{definition}[Inner Product Space]
An \textbf{inner product space} is a vector space $V$ over a field $\mathbb{F}$ (where $\mathbb{F} = \mathbb{R}$ or $\mathbb{F} = \mathbb{C}$) equipped with an \textbf{inner product}, which is a function $\langle \cdot, \cdot \rangle: V \times V \to \mathbb{F}$ satisfying the following properties for all $x, y, z \in V$ and all scalars $\alpha, \beta \in \mathbb{F}$:

\begin{enumerate}
    \item \textbf{Conjugate Symmetry:} $\langle x, y \rangle = \overline{\langle y, x \rangle}$
    
    (For real spaces: $\langle x, y \rangle = \langle y, x \rangle$)
    
    \item \textbf{Linearity in the First Argument:}
    \[
    \langle \alpha x + \beta y, z \rangle = \alpha \langle x, z \rangle + \beta \langle y, z \rangle
    \]
    
    \item \textbf{Positive Definiteness:} $\langle x, x \rangle \geq 0$, and $\langle x, x \rangle = 0$ if and only if $x = 0$
\end{enumerate}

\textbf{Note:} From properties (1) and (2), the inner product is conjugate-linear (antilinear) in the second argument:
\[
\langle x, \alpha y + \beta z \rangle = \overline{\alpha} \langle x, y \rangle + \overline{\beta} \langle x, z \rangle
\]
\end{definition}

\begin{definition}[Hilbert Space]
A \textbf{Hilbert space} is an inner product space that is complete with respect to the norm induced by the inner product.
\end{definition}

\section{Induced Norm}

Every inner product induces a norm on the vector space defined by:
\[
\|x\| = \sqrt{\langle x, x \rangle}
\]

This norm satisfies all the axioms of a normed space and additionally satisfies the \textbf{parallelogram law}:
\[
\|x + y\|^2 + \|x - y\|^2 = 2(\|x\|^2 + \|y\|^2)
\]

\section{Important Examples}

\begin{example}[Euclidean Space $\mathbb{R}^n$]
The vector space $\mathbb{R}^n$ with the standard inner product (dot product):
\[
\langle x, y \rangle = x \cdot y = \sum_{i=1}^n x_i y_i = x_1 y_1 + x_2 y_2 + \cdots + x_n y_n
\]
for $x = (x_1, \ldots, x_n)$ and $y = (y_1, \ldots, y_n)$.

The induced norm is the Euclidean norm: $\|x\| = \sqrt{\sum_{i=1}^n x_i^2}$
\end{example}

\begin{example}[Complex Space $\mathbb{C}^n$]
The vector space $\mathbb{C}^n$ with the standard Hermitian inner product:
\[
\langle x, y \rangle = \sum_{i=1}^n x_i \overline{y_i}
\]
where $\overline{y_i}$ denotes the complex conjugate of $y_i$.

Note the conjugate is on the second argument in this convention.
\end{example}

\begin{example}[Weighted Inner Product on $\mathbb{R}^n$]
For a vector of positive weights $w = (w_1, w_2, \ldots, w_n)$ with $w_i > 0$, we can define:
\[
\langle x, y \rangle_w = \sum_{i=1}^n w_i x_i y_i
\]
This is an inner product on $\mathbb{R}^n$ different from the standard one.
\end{example}

\begin{example}[Space of Continuous Functions]
Let $C[a,b]$ be the space of continuous real-valued functions on $[a,b]$. The $L^2$ inner product is defined by:
\[
\langle f, g \rangle = \int_a^b f(x) g(x) \, dx
\]

The induced norm is: $\|f\| = \sqrt{\int_a^b f(x)^2 \, dx}$

\textbf{Note:} This space is not complete under this norm; its completion is $L^2[a,b]$.
\end{example}

\begin{example}[Weighted $L^2$ Space]
With a positive weight function $w(x) > 0$, we can define:
\[
\langle f, g \rangle_w = \int_a^b f(x) g(x) w(x) \, dx
\]
This is useful in orthogonal polynomial theory and approximation theory.
\end{example}

\begin{example}[$L^2(\mathbb{R})$ Space]
The space $L^2(\mathbb{R})$ of square-integrable functions with inner product:
\[
\langle f, g \rangle = \int_{-\infty}^{\infty} f(x) \overline{g(x)} \, dx
\]
This is a Hilbert space (complete inner product space) fundamental in quantum mechanics and Fourier analysis.
\end{example}

\begin{example}[Sequence Space $\ell^2$]
The space $\ell^2$ of square-summable sequences $(x_n)_{n=1}^\infty$ with inner product:
\[
\langle x, y \rangle = \sum_{n=1}^\infty x_n \overline{y_n}
\]
where the series converges absolutely.

This is a Hilbert space with induced norm: $\|x\| = \sqrt{\sum_{n=1}^\infty |x_n|^2}$
\end{example}

\begin{example}[Matrix Inner Product (Frobenius)]
For $m \times n$ matrices $A, B$ over $\mathbb{R}$ or $\mathbb{C}$:
\[
\langle A, B \rangle = \text{tr}(B^* A) = \sum_{i=1}^m \sum_{j=1}^n A_{ij} \overline{B_{ij}}
\]
where $B^*$ is the conjugate transpose of $B$ and $\text{tr}$ denotes the trace.

For real matrices: $\langle A, B \rangle = \text{tr}(B^T A) = \sum_{i,j} A_{ij} B_{ij}$
\end{example}

\begin{example}[Polynomial Space with Integration Inner Product]
Let $P_n[a,b]$ be the space of polynomials of degree at most $n$ on $[a,b]$. Define:
\[
\langle p, q \rangle = \int_a^b p(x) q(x) \, dx
\]
This makes $P_n[a,b]$ an inner product space.
\end{example}

\section{Important Properties}

\begin{theorem}[Cauchy-Schwarz Inequality]
For any inner product space and any $x, y \in V$:
\[
|\langle x, y \rangle| \leq \|x\| \cdot \|y\|
\]
with equality if and only if $x$ and $y$ are linearly dependent.
\end{theorem}

\begin{definition}[Orthogonality]
Two vectors $x, y \in V$ are \textbf{orthogonal} (written $x \perp y$) if $\langle x, y \rangle = 0$.
\end{definition}

\begin{theorem}[Pythagorean Theorem]
If $x \perp y$, then:
\[
\|x + y\|^2 = \|x\|^2 + \|y\|^2
\]
\end{theorem}

\begin{proposition}[Parallelogram Law]
In any inner product space:
\[
\|x + y\|^2 + \|x - y\|^2 = 2(\|x\|^2 + \|y\|^2)
\]
Moreover, a norm comes from an inner product if and only if it satisfies the parallelogram law.
\end{proposition}

\section{Orthonormal Bases}

\begin{definition}[Orthonormal Set]
A set of vectors $\{e_\alpha\}_{\alpha \in I}$ is \textbf{orthonormal} if:
\[
\langle e_\alpha, e_\beta \rangle = \delta_{\alpha\beta} = \begin{cases}
1 & \text{if } \alpha = \beta \\
0 & \text{if } \alpha \neq \beta
\end{cases}
\]
\end{definition}

\textbf{Remark:} Every inner product space has an orthonormal basis. If $\{e_1, e_2, \ldots, e_n\}$ is an orthonormal basis for a finite-dimensional inner product space, then any vector $x$ can be written as:
\[
x = \sum_{i=1}^n \langle x, e_i \rangle e_i
\]


\title{Hilbert Spaces: Definitions and Examples}

\section{Hilbert Spaces}

\begin{definition}[Cauchy Sequence]
A sequence $(x_n)$ in an inner product space $V$ is called a \textbf{Cauchy sequence} if for every $\epsilon > 0$, there exists $N \in \mathbb{N}$ such that:
\[
\|x_n - x_m\| < \epsilon \quad \text{for all } n, m \geq N
\]
\end{definition}

\begin{definition}[Complete Metric Space]
A metric space is \textbf{complete} if every Cauchy sequence in the space converges to a limit within the space.
\end{definition}

\begin{definition}[Hilbert Space]
A \textbf{Hilbert space} $\mathcal{H}$ is an inner product space that is complete with respect to the norm induced by the inner product:
\[
\|x\| = \sqrt{\langle x, x \rangle}
\]

In other words, a Hilbert space is a complete inner product space.
\end{definition}


Every finite-dimensional inner product space is automatically a Hilbert space, since all finite-dimensional normed spaces are complete. The interesting examples are infinite-dimensional.


\section{Examples of Hilbert Spaces}

\begin{example}[Euclidean Space $\mathbb{R}^n$]
The space $\mathbb{R}^n$ with the standard inner product:
\[
\langle x, y \rangle = \sum_{i=1}^n x_i y_i
\]
is a Hilbert space. The induced norm is $\|x\| = \sqrt{\sum_{i=1}^n x_i^2}$.

This is a finite-dimensional Hilbert space of dimension $n$.
\end{example}

\begin{example}[Complex Euclidean Space $\mathbb{C}^n$]
The space $\mathbb{C}^n$ with the Hermitian inner product:
\[
\langle x, y \rangle = \sum_{i=1}^n x_i \overline{y_i}
\]
is a Hilbert space. This is fundamental in quantum mechanics, where quantum states are represented as vectors in $\mathbb{C}^n$ (for finite-dimensional systems).
\end{example}

\begin{example}[Sequence Space $\ell^2$]
The space $\ell^2$ consists of all infinite sequences $x = (x_1, x_2, x_3, \ldots)$ of real or complex numbers such that:
\[
\sum_{n=1}^\infty |x_n|^2 < \infty
\]

The inner product is defined by:
\[
\langle x, y \rangle = \sum_{n=1}^\infty x_n \overline{y_n}
\]

The induced norm is: $\|x\|_2 = \sqrt{\sum_{n=1}^\infty |x_n|^2}$

This is a separable, infinite-dimensional Hilbert space. It serves as the canonical model for separable Hilbert spaces.
\end{example}

\begin{example}[ Space Square Integrable Functions]
The space $L^2[a,b]$ consists of (equivalence classes of) measurable functions $f: [a,b] \to \mathbb{C}$ such that:
\[
\int_a^b |f(x)|^2 \, dx < \infty
\]

The inner product is:
\[
\langle f, g \rangle = \int_a^b f(x) \overline{g(x)} \, dx
\]

The induced norm is: $\|f\|_2 = \sqrt{\int_a^b |f(x)|^2 \, dx}$

This is a separable, infinite-dimensional Hilbert space. Functions that differ only on a set of measure zero are identified.
\end{example}

\begin{example}[$L^2(\mathbb{R})$ Space]
The space $L^2(\mathbb{R})$ consists of measurable functions $f: \mathbb{R} \to \mathbb{C}$ such that:
\[
\int_{-\infty}^{\infty} |f(x)|^2 \, dx < \infty
\]

With inner product:
\[
\langle f, g \rangle = \int_{-\infty}^{\infty} f(x) \overline{g(x)} \, dx
\]

This is the fundamental Hilbert space in quantum mechanics (for particles in one dimension) and Fourier analysis. The Fourier transform is a unitary operator on this space.
\end{example}

\begin{example}[$L^2(\mathbb{R}^n)$ Space]
The natural generalization to $n$ dimensions: measurable functions $f: \mathbb{R}^n \to \mathbb{C}$ with:
\[
\int_{\mathbb{R}^n} |f(x)|^2 \, dx < \infty
\]

Inner product:
\[
\langle f, g \rangle = \int_{\mathbb{R}^n} f(x) \overline{g(x)} \, dx
\]

This space is fundamental in partial differential equations and quantum mechanics for multi-particle systems.
\end{example}

\begin{example}[Sobolev Space $H^1(\Omega)$]
For an open set $\Omega \subset \mathbb{R}^n$, the Sobolev space $H^1(\Omega)$ consists of functions $f \in L^2(\Omega)$ whose weak derivatives also belong to $L^2(\Omega)$:
\[
H^1(\Omega) = \{f \in L^2(\Omega) : \partial_i f \in L^2(\Omega) \text{ for } i = 1, \ldots, n\}
\]

The inner product is:
\[
\langle f, g \rangle_{H^1} = \int_\Omega f(x) \overline{g(x)} \, dx + \sum_{i=1}^n \int_\Omega \partial_i f(x) \overline{\partial_i g(x)} \, dx
\]

Sobolev spaces are crucial in the theory of partial differential equations.
\end{example}

\begin{example}[Hardy Space $H^2(\mathbb{D})$]
The Hardy space $H^2(\mathbb{D})$ consists of holomorphic functions $f$ on the unit disk $\mathbb{D} = \{z \in \mathbb{C} : |z| < 1\}$ such that:
\[
\sup_{0 < r < 1} \int_0^{2\pi} |f(re^{i\theta})|^2 \, d\theta < \infty
\]

The inner product is:
\[
\langle f, g \rangle = \frac{1}{2\pi} \int_0^{2\pi} f(e^{i\theta}) \overline{g(e^{i\theta})} \, d\theta
\]

Hardy spaces are important in complex analysis and operator theory.
\end{example}

\begin{example}[Bergman Space $A^2(\mathbb{D})$]
The Bergman space $A^2(\mathbb{D})$ consists of holomorphic functions on the unit disk that are square-integrable with respect to area measure:
\[
A^2(\mathbb{D}) = \left\{f : \mathbb{D} \to \mathbb{C} \text{ holomorphic} : \int_{\mathbb{D}} |f(z)|^2 \, dA(z) < \infty\right\}
\]

Inner product:
\[
\langle f, g \rangle = \frac{1}{\pi} \int_{\mathbb{D}} f(z) \overline{g(z)} \, dA(z)
\]

where $dA(z) = dx \, dy$ is the area element.
\end{example}

\section{Non-Examples}

\begin{example}[The Space C[a,b] is NOT Complete]
Consider $C[a,b]$ with the $L^2$ inner product:
\[
\langle f, g \rangle = \int_a^b f(x) g(x) \, dx
\]

This is an inner product space but NOT a Hilbert space because it is not complete. There exist Cauchy sequences of continuous functions whose limits (in the $L^2$ norm) are not continuous.

For instance, on $[-1,1]$, the sequence:
\[
f_n(x) = \begin{cases}
-1 & x \in [-1, -1/n) \\
nx & x \in [-1/n, 1/n] \\
1 & x \in (1/n, 1]
\end{cases}
\]
converges in $L^2$ norm to the discontinuous sign function.
\end{example}

\section{Key Properties of Hilbert Spaces}

\begin{theorem}[Riesz Representation Theorem]
Let $\mathcal{H}$ be a Hilbert space and let $\phi: \mathcal{H} \to \mathbb{F}$ be a bounded linear functional. Then there exists a unique $y \in \mathcal{H}$ such that:
\[
\phi(x) = \langle x, y \rangle \quad \text{for all } x \in \mathcal{H}
\]
Moreover, $\|\phi\| = \|y\|$.
\end{theorem}

\begin{theorem}[Projection Theorem]
Let $\mathcal{H}$ be a Hilbert space and let $M$ be a closed subspace of $\mathcal{H}$. Then every $x \in \mathcal{H}$ can be uniquely written as:
\[
x = y + z
\]
where $y \in M$ and $z \in M^\perp$ (the orthogonal complement of $M$). Moreover, $y$ is the unique element of $M$ closest to $x$.
\end{theorem}

\begin{definition}[Orthonormal Basis]
A set $\{e_\alpha\}_{\alpha \in I}$ in a Hilbert space $\mathcal{H}$ is an \textbf{orthonormal basis} if:
\begin{enumerate}
    \item $\langle e_\alpha, e_\beta \rangle = \delta_{\alpha\beta}$ (orthonormality)
    \item The set $\{e_\alpha\}_{\alpha \in I}$ is maximal, i.e., the only vector orthogonal to all $e_\alpha$ is the zero vector
\end{enumerate}
\end{definition}

\begin{theorem}[Parseval's Identity]
If $\{e_n\}_{n=1}^\infty$ is an orthonormal basis for a separable Hilbert space $\mathcal{H}$, then for any $x \in \mathcal{H}$:
\[
\|x\|^2 = \sum_{n=1}^\infty |\langle x, e_n \rangle|^2
\]
and
\[
x = \sum_{n=1}^\infty \langle x, e_n \rangle e_n
\]
\end{theorem}

\begin{corollary}[Bessel's Inequality]
For any orthonormal set $\{e_n\}_{n=1}^N$ (not necessarily a basis) and any $x \in \mathcal{H}$:
\[
\sum_{n=1}^N |\langle x, e_n \rangle|^2 \leq \|x\|^2
\]
\end{corollary}

\section{Separability}

\begin{definition}[Separable Hilbert Space]
A Hilbert space $\mathcal{H}$ is \textbf{separable} if it contains a countable dense subset.
\end{definition}

\begin{theorem}
A Hilbert space is separable if and only if it has a countable orthonormal basis.
\end{theorem}


All the concrete examples listed above ($\ell^2$, $L^2[a,b]$, $L^2(\mathbb{R}^n)$, Hardy spaces, etc.) are separable Hilbert spaces. Every separable infinite-dimensional Hilbert space is isometrically isomorphic to $\ell^2$.



\begin{definition}[Dual Space]
Let $V$ be a vector space over a field $\mathbb{F}$. The \textbf{dual space} of $V$, denoted $V^*$ (or $V'$), is the vector space of all linear functionals on $V$:
\[
V^* = \{\varphi: V \to \mathbb{F} \mid \varphi \text{ is linear}\}
\]
That is, $V^*$ consists of all linear maps from $V$ to the scalar field $\mathbb{F}$.
\end{definition}

\section*{Properties}

\subsection*{Linearity Condition}
A functional $\varphi \in V^*$ satisfies:
\begin{align*}
\varphi(v_1 + v_2) &= \varphi(v_1) + \varphi(v_2) \quad \forall v_1, v_2 \in V \\
\varphi(\alpha v) &= \alpha \varphi(v) \quad \forall \alpha \in \mathbb{F}, v \in V
\end{align*}

\subsection*{Vector Space Structure on $V^*$}
The dual space $V^*$ is itself a vector space with operations:
\begin{itemize}
\item \textbf{Addition}: For $\varphi, \psi \in V^*$,
\[
(\varphi + \psi)(v) = \varphi(v) + \psi(v) \quad \forall v \in V
\]

\item \textbf{Scalar multiplication}: For $\alpha \in \mathbb{F}$ and $\varphi \in V^*$,
\[
(\alpha \varphi)(v) = \alpha \cdot \varphi(v) \quad \forall v \in V
\]
\end{itemize}

\begin{theorem}[Dimension of Dual Space]
If $V$ is finite-dimensional with $\dim(V) = n$, then:
\[
\dim(V^*) = \dim(V) = n
\]
In particular, $V^* \cong V$ as vector spaces (though not canonically).
\end{theorem}

\subsection*{Dual Basis}

\begin{definition}[Dual Basis]
Let $V$ be finite-dimensional with basis $\mathcal{B} = \{v_1, v_2, \ldots, v_n\}$. The \textbf{dual basis} $\mathcal{B}^* = \{v_1^*, v_2^*, \ldots, v_n^*\}$ for $V^*$ is defined by:
\[
v_i^*(v_j) = \delta_{ij} = \begin{cases}
1 & \text{if } i = j \\
0 & \text{if } i \neq j
\end{cases}
\]
where $\delta_{ij}$ is the Kronecker delta.
\end{definition}

\section*{Examples}

\begin{example}[$(\mathbb{R}^n)^*$]
The dual space of $\mathbb{R}^n$ can be identified with the space of row vectors. For the standard basis:
\[
\mathbf{e}_1 = \begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix}, \quad \mathbf{e}_2 = \begin{pmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{pmatrix}, \ldots, \mathbf{e}_n = \begin{pmatrix} 0 \\ 0 \\ \vdots \\ 1 \end{pmatrix}
\]

The dual basis elements are:
\[
\mathbf{e}_i^*(\mathbf{x}) = x_i \quad \text{(the $i$-th coordinate functional)}
\]

In matrix notation, if $\mathbf{x} = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}$, then:
\[
\mathbf{e}_i^*(\mathbf{x}) = \begin{pmatrix} 0 & \cdots & 1 & \cdots & 0 \end{pmatrix} \begin{pmatrix} x_1 \\ \vdots \\ x_i \\ \vdots \\ x_n \end{pmatrix} = x_i
\]
\end{example}

\begin{example}[Dual of Polynomial Space]
Let $V = \mathbb{P}_2(\mathbb{R})$ be the space of polynomials of degree at most 2. Examples of linear functionals in $V^*$ include:
\begin{itemize}
\item \textbf{Evaluation functional}: $\varphi_a(p) = p(a)$ for fixed $a \in \mathbb{R}$
\item \textbf{Derivative at a point}: $\psi_a(p) = p'(a)$
\item \textbf{Integration}: $\mu(p) = \int_0^1 p(x)\,dx$
\end{itemize}

For the basis $\{1, x, x^2\}$ of $\mathbb{P}_2(\mathbb{R})$, the dual basis $\{\varphi_1^*, \varphi_2^*, \varphi_3^*\}$ satisfies:
\[
\varphi_i^*(x^{j-1}) = \delta_{ij}
\]
\end{example}

\section*{Double Dual}

\begin{definition}[Double Dual]
The \textbf{double dual} (or \textbf{bidual}) of $V$ is:
\[
V^{**} = (V^*)^*
\]
the dual space of the dual space.
\end{definition}

\begin{theorem}[Canonical Isomorphism]
For finite-dimensional $V$, there exists a canonical isomorphism:
\[
\Phi: V \to V^{**}
\]
defined by $\Phi(v)(\varphi) = \varphi(v)$ for all $v \in V$ and $\varphi \in V^*$.

This means every vector $v \in V$ can be viewed as a linear functional on $V^*$.
\end{theorem}

\section*{Important Note}
While $V \cong V^*$ for finite-dimensional spaces, this isomorphism is \textbf{not canonical} (it depends on choosing a basis). However, the isomorphism $V \cong V^{**}$ \textbf{is canonical} (basis-independent).



\section{Tensor Representation Theorem}

\begin{theorem}[Tensor Representation Theorem / Universal Property of Tensor Products]
Let $V$ and $W$ be vector spaces over a field $\mathbb{F}$. There exists a vector space $V \otimes W$ (the \textbf{tensor product}) and a bilinear map:
\[
\varphi: V \times W \to V \otimes W
\]
with the following \textbf{universal property}:

For any vector space $U$ and any bilinear map $\beta: V \times W \to U$, there exists a \textbf{unique} linear map $T: V \otimes W \to U$ such that:
\[
\beta = T \circ \varphi
\]

Equivalently, the following diagram commutes:
\[
\begin{tikzcd}
V \times W \arrow[rr, "\beta"] \arrow[dr, "\varphi"'] & & U \\
& V \otimes W \arrow[ur, "\exists! T"'] &
\end{tikzcd}
\]
\end{theorem}

\section*{Key Properties}

\begin{enumerate}
\item \textbf{Bilinearity of $\varphi$}: The map $\varphi(v, w) = v \otimes w$ satisfies:
\begin{align*}
\varphi(v_1 + v_2, w) &= \varphi(v_1, w) + \varphi(v_2, w) \\
\varphi(v, w_1 + w_2) &= \varphi(v, w_1) + \varphi(v, w_2) \\
\varphi(\alpha v, w) &= \varphi(v, \alpha w) = \alpha \cdot \varphi(v, w) \quad \text{for } \alpha \in \mathbb{F}
\end{align*}

\item \textbf{Universal Property}: Any bilinear map from $V \times W$ factors uniquely through $V \otimes W$ via a linear map.

\item \textbf{Uniqueness}: The tensor product is unique up to isomorphism.

\item \textbf{Dimension}: If $V$ and $W$ are finite-dimensional with $\dim(V) = n$ and $\dim(W) = m$, then:
\[
\dim(V \otimes W) = n \cdot m
\]

\item \textbf{Basis}: If $\{v_1, \ldots, v_n\}$ is a basis for $V$ and $\{w_1, \ldots, w_m\}$ is a basis for $W$, then:
\[
\{v_i \otimes w_j \mid 1 \leq i \leq n, 1 \leq j \leq m\}
\]
forms a basis for $V \otimes W$.
\end{enumerate}

\textbf{Interpretation:} The theorem says that $V \otimes W$ is the ``freest'' way to turn bilinear maps into linear maps.

\section*{Example: Tensor Product of $\mathbb{R}^2$ and $\mathbb{R}^3$}

\subsection*{Setup}
Let $V = \mathbb{R}^2$ and $W = \mathbb{R}^3$ be vector spaces over $\mathbb{R}$.

\subsection*{Standard Bases}
\begin{align*}
\text{Basis for } V: \quad & \mathbf{e}_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix}, \quad \mathbf{e}_2 = \begin{pmatrix} 0 \\ 1 \end{pmatrix} \\[10pt]
\text{Basis for } W: \quad & \mathbf{f}_1 = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}, \quad \mathbf{f}_2 = \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}, \quad \mathbf{f}_3 = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}
\end{align*}

\subsection*{Tensor Product Space}
The tensor product $V \otimes W = \mathbb{R}^2 \otimes \mathbb{R}^3$ has dimension:
\[
\dim(V \otimes W) = \dim(V) \cdot \dim(W) = 2 \cdot 3 = 6
\]

\subsection*{Basis for $V \otimes W$}
A basis for $\mathbb{R}^2 \otimes \mathbb{R}^3$ consists of all tensor products of basis vectors:
\[
\{\mathbf{e}_i \otimes \mathbf{f}_j \mid i \in \{1,2\}, \, j \in \{1,2,3\}\}
\]

Explicitly:
\begin{align*}
\mathbf{e}_1 \otimes \mathbf{f}_1, \quad \mathbf{e}_1 \otimes \mathbf{f}_2, \quad \mathbf{e}_1 \otimes \mathbf{f}_3, \\
\mathbf{e}_2 \otimes \mathbf{f}_1, \quad \mathbf{e}_2 \otimes \mathbf{f}_2, \quad \mathbf{e}_2 \otimes \mathbf{f}_3
\end{align*}

\subsection*{Example Tensor}
Consider vectors:
\[
\mathbf{v} = \begin{pmatrix} 2 \\ 3 \end{pmatrix} = 2\mathbf{e}_1 + 3\mathbf{e}_2, \quad 
\mathbf{w} = \begin{pmatrix} 1 \\ -1 \\ 2 \end{pmatrix} = \mathbf{f}_1 - \mathbf{f}_2 + 2\mathbf{f}_3
\]

Their tensor product is:
\begin{align*}
\mathbf{v} \otimes \mathbf{w} &= (2\mathbf{e}_1 + 3\mathbf{e}_2) \otimes (\mathbf{f}_1 - \mathbf{f}_2 + 2\mathbf{f}_3) \\
&= 2\mathbf{e}_1 \otimes \mathbf{f}_1 - 2\mathbf{e}_1 \otimes \mathbf{f}_2 + 4\mathbf{e}_1 \otimes \mathbf{f}_3 \\
&\quad + 3\mathbf{e}_2 \otimes \mathbf{f}_1 - 3\mathbf{e}_2 \otimes \mathbf{f}_2 + 6\mathbf{e}_2 \otimes \mathbf{f}_3
\end{align*}

\subsection*{Matrix Representation}
We can represent $\mathbf{v} \otimes \mathbf{w}$ as the outer product (Kronecker product):
\[
\mathbf{v} \otimes \mathbf{w} \leftrightarrow \begin{pmatrix} 2 \\ 3 \end{pmatrix} \otimes \begin{pmatrix} 1 \\ -1 \\ 2 \end{pmatrix} = \begin{pmatrix}
2 \cdot 1 \\ 2 \cdot (-1) \\ 2 \cdot 2 \\ 3 \cdot 1 \\ 3 \cdot (-1) \\ 3 \cdot 2
\end{pmatrix} = \begin{pmatrix}
2 \\ -2 \\ 4 \\ 3 \\ -3 \\ 6
\end{pmatrix} \in \mathbb{R}^6
\]

Or as a $2 \times 3$ matrix:
\[
\mathbf{v} \mathbf{w}^T = \begin{pmatrix} 2 \\ 3 \end{pmatrix} \begin{pmatrix} 1 & -1 & 2 \end{pmatrix} = \begin{pmatrix}
2 & -2 & 4 \\
3 & -3 & 6
\end{pmatrix}
\]

\subsection*{Key Properties Demonstrated}
\begin{enumerate}
\item \textbf{Bilinearity}: 
\[
(a\mathbf{v}) \otimes \mathbf{w} = \mathbf{v} \otimes (a\mathbf{w}) = a(\mathbf{v} \otimes \mathbf{w})
\]

\item \textbf{Not all elements are simple tensors}: A general element of $\mathbb{R}^2 \otimes \mathbb{R}^3$ is a linear combination of basis tensors:
\[
\sum_{i=1}^{2} \sum_{j=1}^{3} c_{ij} (\mathbf{e}_i \otimes \mathbf{f}_j)
\]
Not all such elements can be written as $\mathbf{v} \otimes \mathbf{w}$ for some $\mathbf{v} \in \mathbb{R}^2, \mathbf{w} \in \mathbb{R}^3$.

\item \textbf{Isomorphism}: $\mathbb{R}^2 \otimes \mathbb{R}^3 \cong \mathbb{R}^6$ as vector spaces, but the tensor product structure preserves additional information about the factorization.
\end{enumerate}

\begin{theorem}[Tensor Product is a Vector Space]
Let $V$ and $W$ be vector spaces over a field $\mathbb{F}$. The tensor product $V \otimes W$ is itself a vector space over $\mathbb{F}$.
\end{theorem}

\section*{Vector Space Structure on $V \otimes W$}

The tensor product $V \otimes W$ inherits vector space operations that make it a vector space over $\mathbb{F}$.

\subsection*{1. Addition}
For tensors $\tau_1, \tau_2 \in V \otimes W$, their sum is defined element-wise. If:
\begin{align*}
\tau_1 &= \sum_{i} v_i \otimes w_i \\
\tau_2 &= \sum_{j} v_j' \otimes w_j'
\end{align*}
then:
\[
\tau_1 + \tau_2 = \sum_{i} v_i \otimes w_i + \sum_{j} v_j' \otimes w_j'
\]

\textbf{Example}: In $\mathbb{R}^2 \otimes \mathbb{R}^2$,
\begin{align*}
(\mathbf{e}_1 \otimes \mathbf{e}_1) + (\mathbf{e}_2 \otimes \mathbf{e}_2) &= \mathbf{e}_1 \otimes \mathbf{e}_1 + \mathbf{e}_2 \otimes \mathbf{e}_2
\end{align*}
This is a tensor that cannot be written as a simple tensor $\mathbf{v} \otimes \mathbf{w}$.

\subsection*{2. Scalar Multiplication}
For $\alpha \in \mathbb{F}$ and $\tau \in V \otimes W$, where $\tau = \sum_{i} v_i \otimes w_i$:
\[
\alpha \tau = \sum_{i} \alpha(v_i \otimes w_i) = \sum_{i} (\alpha v_i) \otimes w_i = \sum_{i} v_i \otimes (\alpha w_i)
\]

\textbf{Note}: By bilinearity, $\alpha(v \otimes w) = (\alpha v) \otimes w = v \otimes (\alpha w)$.

\subsection*{3. Zero Element}
The zero vector in $V \otimes W$ is:
\[
\mathbf{0}_{V \otimes W} = 0_V \otimes w = v \otimes 0_W \quad \text{for any } v \in V, w \in W
\]

More generally, the zero element can be written as the empty sum or:
\[
\mathbf{0}_{V \otimes W} = \sum_{i} 0 \cdot (v_i \otimes w_i)
\]

\section*{Verification of Vector Space Axioms}

Let $\tau_1, \tau_2, \tau_3 \in V \otimes W$ and $\alpha, \beta \in \mathbb{F}$.

\begin{enumerate}
\item \textbf{Commutativity of addition}:
\[
\tau_1 + \tau_2 = \tau_2 + \tau_1
\]

\item \textbf{Associativity of addition}:
\[
(\tau_1 + \tau_2) + \tau_3 = \tau_1 + (\tau_2 + \tau_3)
\]

\item \textbf{Additive identity}:
\[
\tau + \mathbf{0}_{V \otimes W} = \tau
\]

\item \textbf{Additive inverse}: For each $\tau = \sum_i v_i \otimes w_i$, there exists $-\tau = \sum_i (-v_i) \otimes w_i$ such that:
\[
\tau + (-\tau) = \mathbf{0}_{V \otimes W}
\]

\item \textbf{Scalar multiplication identity}:
\[
1 \cdot \tau = \tau
\]

\item \textbf{Scalar multiplication associativity}:
\[
(\alpha \beta) \tau = \alpha (\beta \tau)
\]

\item \textbf{Distributivity over vector addition}:
\[
\alpha(\tau_1 + \tau_2) = \alpha \tau_1 + \alpha \tau_2
\]

\item \textbf{Distributivity over scalar addition}:
\[
(\alpha + \beta)\tau = \alpha \tau + \beta \tau
\]
\end{enumerate}


\section*{Dimension and Basis}

\begin{proposition}
If $V$ and $W$ are finite-dimensional with bases $\{v_1, \ldots, v_n\}$ and $\{w_1, \ldots, w_m\}$ respectively, then $V \otimes W$ has:
\begin{itemize}
\item \textbf{Dimension}: $\dim(V \otimes W) = nm$
\item \textbf{Basis}: $\{v_i \otimes w_j \mid 1 \leq i \leq n, 1 \leq j \leq m\}$
\end{itemize}
\end{proposition}

This means every element $\tau \in V \otimes W$ can be uniquely written as:
\[
\tau = \sum_{i=1}^{n} \sum_{j=1}^{m} c_{ij} (v_i \otimes w_j)
\]
where $c_{ij} \in \mathbb{F}$.

\section*{Simple vs. General Tensors}

\subsection*{Important Distinction}

\begin{itemize}
\item \textbf{Simple (decomposable) tensors}: Elements of the form $v \otimes w$ for some $v \in V, w \in W$

\item \textbf{General tensors}: Arbitrary linear combinations $\sum_i c_i (v_i \otimes w_i)$
\end{itemize}

\textbf{Key Fact}: Not every element of $V \otimes W$ is a simple tensor!

\subsection*{Example: $\mathbb{R}^2 \otimes \mathbb{R}^2$}

Consider:
\[
\tau = \mathbf{e}_1 \otimes \mathbf{e}_1 + \mathbf{e}_2 \otimes \mathbf{e}_2 \in \mathbb{R}^2 \otimes \mathbb{R}^2
\]

\textbf{Claim}: $\tau$ cannot be written as $\mathbf{v} \otimes \mathbf{w}$ for any $\mathbf{v}, \mathbf{w} \in \mathbb{R}^2$.

\textbf{Proof}: Suppose $\tau = \mathbf{v} \otimes \mathbf{w}$ where:
\[
\mathbf{v} = \begin{pmatrix} a \\ b \end{pmatrix}, \quad \mathbf{w} = \begin{pmatrix} c \\ d \end{pmatrix}
\]

Then as a matrix (outer product):
\[
\mathbf{v} \otimes \mathbf{w} = \begin{pmatrix} ac & ad \\ bc & bd \end{pmatrix}
\]

But our tensor corresponds to the identity matrix:
\[
\tau \leftrightarrow \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}
\]

For this to equal $\begin{pmatrix} ac & ad \\ bc & bd \end{pmatrix}$, we need:
\begin{align*}
ac &= 1 \\
ad &= 0 \\
bc &= 0 \\
bd &= 1
\end{align*}

From $ad = 0$: either $a = 0$ or $d = 0$.
\begin{itemize}
\item If $a = 0$, then $ac = 0 \neq 1$. Contradiction.
\item If $d = 0$, then $bd = 0 \neq 1$. Contradiction.
\end{itemize}

Therefore, $\tau$ is not a simple tensor.

\section*{Summary}

The tensor product $V \otimes W$ is a vector space because:
\begin{enumerate}
\item It is constructed as a quotient of a free vector space
\item It inherits well-defined addition and scalar multiplication
\item These operations satisfy all vector space axioms
\item It has a basis (in finite dimensions) and dimension $\dim(V) \cdot \dim(W)$
\end{enumerate}

The elements of $V \otimes W$ include simple tensors $v \otimes w$ and more general linear combinations, all forming a rich vector space structure.


\section*{Tensors Taking Vectors and Dual Vectors from Multiple Spaces}

\begin{definition}[Symmetric Multi-Space Tensor]
Let $V_1, V_2, \ldots, V_N$ be vector spaces over a field $\mathbb{F}$ with corresponding dual spaces $V_1^*, V_2^*, \ldots, V_N^*$.

A \textbf{multi-space tensor} is a multilinear map:
\[
T: V_1 \times V_2 \times \cdots \times V_N \times V_1^* \times V_2^* \times \cdots \times V_N^* \to \mathbb{F}
\]

That is, $T$ takes as input:
\begin{itemize}
\item One vector from each space: $v_1 \in V_1, v_2 \in V_2, \ldots, v_N \in V_N$
\item One dual vector from each dual space: $\omega_1 \in V_1^*, \omega_2 \in V_2^*, \ldots, \omega_N \in V_N^*$
\end{itemize}

And outputs a scalar:
\[
T(v_1, v_2, \ldots, v_N, \omega_1, \omega_2, \ldots, \omega_N) \in \mathbb{F}
\]
\end{definition}

\subsection*{Multilinearity Condition}

The tensor $T$ must be linear in each of its $2N$ arguments. For example:

\textbf{Linearity in $v_i$}:
\begin{align*}
T(v_1, \ldots, v_i + v_i', \ldots, v_N, \omega_1, \ldots, \omega_N) &= T(v_1, \ldots, v_i, \ldots, v_N, \omega_1, \ldots, \omega_N) \\
&\quad + T(v_1, \ldots, v_i', \ldots, v_N, \omega_1, \ldots, \omega_N)
\end{align*}
\[
T(v_1, \ldots, \alpha v_i, \ldots, v_N, \omega_1, \ldots, \omega_N) = \alpha \cdot T(v_1, \ldots, v_i, \ldots, v_N, \omega_1, \ldots, \omega_N)
\]

\textbf{Linearity in $\omega_j$}:
\begin{align*}
T(v_1, \ldots, v_N, \omega_1, \ldots, \omega_j + \omega_j', \ldots, \omega_N) &= T(v_1, \ldots, v_N, \omega_1, \ldots, \omega_j, \ldots, \omega_N) \\
&\quad + T(v_1, \ldots, v_N, \omega_1, \ldots, \omega_j', \ldots, \omega_N)
\end{align*}
\[
T(v_1, \ldots, v_N, \omega_1, \ldots, \alpha \omega_j, \ldots, \omega_N) = \alpha \cdot T(v_1, \ldots, v_N, \omega_1, \ldots, \omega_j, \ldots, \omega_N)
\]

\section*{Tensor Product Representation}

\begin{theorem}[Tensor Product Space]
The space of all such multilinear maps is isomorphic to the tensor product:
\[
\mathcal{T} = V_1^* \otimes V_2^* \otimes \cdots \otimes V_N^* \otimes V_1 \otimes V_2 \otimes \cdots \otimes V_N
\]

Equivalently:
\[
\mathcal{T} = \left(\bigotimes_{i=1}^{N} V_i^*\right) \otimes \left(\bigotimes_{i=1}^{N} V_i\right)
\]
\end{theorem}

\textbf{Why this correspondence?} 
\begin{itemize}
\item A vector $v_i \in V_i$ in the \emph{input} corresponds to $V_i^*$ in the \emph{tensor product}
\item A dual vector $\omega_i \in V_i^*$ in the \emph{input} corresponds to $V_i$ in the \emph{tensor product}
\end{itemize}

This is because $(V_i^*)^* \cong V_i$ (double dual isomorphism).

\section*{Component Representation}

\begin{definition}[Components]
Given bases:
\begin{itemize}
\item $\{e_{(i),a_i}\}_{a_i=1}^{n_i}$ for $V_i$ with $\dim(V_i) = n_i$
\item Dual bases $\{e^{(i),a_i}\}_{a_i=1}^{n_i}$ for $V_i^*$ satisfying $e^{(i),a_i}(e_{(i),b_i}) = \delta^{a_i}_{b_i}$
\end{itemize}

The tensor components are:
\[
T_{a_1 a_2 \cdots a_N}^{b_1 b_2 \cdots b_N} = T(e_{(1),b_1}, e_{(2),b_2}, \ldots, e_{(N),b_N}, e^{(1),a_1}, e^{(2),a_2}, \ldots, e^{(N),a_N})
\]

where:
\begin{itemize}
\item $a_i \in \{1, 2, \ldots, n_i\}$ (indices for dual vectors from $V_i^*$)
\item $b_i \in \{1, 2, \ldots, n_i\}$ (indices for vectors from $V_i$)
\end{itemize}
\end{definition}

\subsection*{Full Tensor Expansion}

The tensor can be written as:
\[
T = \sum_{a_1, \ldots, a_N} \sum_{b_1, \ldots, b_N} T_{a_1 \cdots a_N}^{b_1 \cdots b_N} \, e^{(1),a_1} \otimes \cdots \otimes e^{(N),a_N} \otimes e_{(1),b_1} \otimes \cdots \otimes e_{(N),b_N}
\]

\subsection*{Evaluation Formula}

For general inputs:
\[
v_i = \sum_{b_i=1}^{n_i} (v_i)^{b_i} e_{(i),b_i}, \quad \omega_j = \sum_{a_j=1}^{n_j} (\omega_j)_{a_j} e^{(j),a_j}
\]

The tensor evaluation is:
\[
T(v_1, \ldots, v_N, \omega_1, \ldots, \omega_N) = \sum_{\text{all indices}} T_{a_1 \cdots a_N}^{b_1 \cdots b_N} \, (v_1)^{b_1} \cdots (v_N)^{b_N} \, (\omega_1)_{a_1} \cdots (\omega_N)_{a_N}
\]

Using Einstein summation convention (sum over repeated indices):
\[
T(v_1, \ldots, v_N, \omega_1, \ldots, \omega_N) = T_{a_1 \cdots a_N}^{b_1 \cdots b_N} \, (v_1)^{b_1} \cdots (v_N)^{b_N} \, (\omega_1)_{a_1} \cdots (\omega_N)_{a_N}
\]

\section*{Dimension}

\begin{proposition}[Dimension Formula]
If $\dim(V_i) = n_i$ for each $i = 1, 2, \ldots, N$, then:
\[
\dim(\mathcal{T}) = \prod_{i=1}^{N} n_i^2 = n_1^2 \cdot n_2^2 \cdots n_N^2
\]
\end{proposition}

\textbf{Proof}: Each space contributes one contravariant and one covariant index, giving $n_i \times n_i = n_i^2$ components per space.

\begin{example}[Dimension Calculation]
For $V_1 = \mathbb{R}^2$, $V_2 = \mathbb{R}^3$, $V_3 = \mathbb{R}^4$:
\[
\dim(\mathcal{T}) = 2^2 \cdot 3^2 \cdot 4^2 = 4 \cdot 9 \cdot 16 = 576
\]
\end{example}

\section*{Simple Examples}

\begin{example}[Two Spaces: $\mathbb{R}^2$ and $\mathbb{R}^3$]
Let $V_1 = \mathbb{R}^2$ and $V_2 = \mathbb{R}^3$. Consider:
\[
T: \mathbb{R}^2 \times \mathbb{R}^3 \times (\mathbb{R}^2)^* \times (\mathbb{R}^3)^* \to \mathbb{R}
\]

\textbf{Construction}: Define $T$ by:
\[
T(v_1, v_2, \omega_1, \omega_2) = \omega_1(v_1) \cdot \omega_2(v_2)
\]

This is the product of the natural pairings.

\textbf{Example evaluation}:
\[
v_1 = \begin{pmatrix} 1 \\ 2 \end{pmatrix}, \quad v_2 = \begin{pmatrix} 1 \\ 0 \\ -1 \end{pmatrix}, \quad \omega_1 = (2, -1), \quad \omega_2 = (1, 1, 1)
\]

\[
\omega_1(v_1) = 2(1) + (-1)(2) = 0
\]
\[
\omega_2(v_2) = 1(1) + 1(0) + 1(-1) = 0
\]
\[
T(v_1, v_2, \omega_1, \omega_2) = 0 \cdot 0 = 0
\]

\textbf{Components}: With standard bases,
\[
T_{ab}^{ij} = \delta^i_a \delta^j_b
\]

where $i,a \in \{1,2\}$ (for $\mathbb{R}^2$) and $j,b \in \{1,2,3\}$ (for $\mathbb{R}^3$).

In this case:
\[
T(v_1, v_2, \omega_1, \omega_2) = T_{ab}^{ij} \, (v_1)^i (v_2)^j (\omega_1)_a (\omega_2)_b = \delta^i_a \delta^j_b \, (v_1)^i (v_2)^j (\omega_1)_a (\omega_2)_b = (v_1)^a (v_2)^b (\omega_1)_a (\omega_2)_b
\]
\end{example}

\begin{example}[Three Spaces with Mixed Components]
Let $V_1 = \mathbb{R}^2$, $V_2 = \mathbb{R}^2$, $V_3 = \mathbb{R}^2$. Define:
\[
T: V_1 \times V_2 \times V_3 \times V_1^* \times V_2^* \times V_3^* \to \mathbb{R}
\]

with components:
\[
T_{abc}^{ijk} = \epsilon^{ij} \epsilon^{ab} \delta^k_c
\]

where $\epsilon^{ij}$ is the Levi-Civita symbol in 2D: $\epsilon^{12} = -\epsilon^{21} = 1$, $\epsilon^{11} = \epsilon^{22} = 0$.

\textbf{Evaluation}:
\[
v_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix}, \quad v_2 = \begin{pmatrix} 0 \\ 1 \end{pmatrix}, \quad v_3 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}
\]
\[
\omega_1 = (1, 0), \quad \omega_2 = (0, 1), \quad \omega_3 = (2, 0)
\]

\[
T(v_1, v_2, v_3, \omega_1, \omega_2, \omega_3) = T_{abc}^{ijk} (v_1)^i (v_2)^j (v_3)^k (\omega_1)_a (\omega_2)_b (\omega_3)_c
\]

Computing:
\begin{align*}
&= \epsilon^{ij} \epsilon^{ab} \delta^k_c \, (v_1)^i (v_2)^j (v_3)^k (\omega_1)_a (\omega_2)_b (\omega_3)_c \\
&= \epsilon^{ij} (v_1)^i (v_2)^j \cdot \epsilon^{ab} (\omega_1)_a (\omega_2)_b \cdot (v_3)^c (\omega_3)_c
\end{align*}

First term: $\epsilon^{ij} (v_1)^i (v_2)^j = \epsilon^{12}(1)(1) = 1$ (determinant of $[v_1, v_2]$)

Second term: $\epsilon^{ab} (\omega_1)_a (\omega_2)_b = \epsilon^{12}(1)(1) = 1$

Third term: $(v_3)^c (\omega_3)_c = (1)(2) + (1)(0) = 2$

Result: $T = 1 \cdot 1 \cdot 2 = 2$
\end{example}

\section*{Special Case: Pairing Tensor}

\begin{definition}[Natural Pairing Tensor]
The \textbf{natural pairing tensor} is defined by:
\[
P(v_1, \ldots, v_N, \omega_1, \ldots, \omega_N) = \prod_{i=1}^{N} \omega_i(v_i)
\]

This is the product of all the natural duality pairings.
\end{definition}

\textbf{Components}:
\[
P_{a_1 \cdots a_N}^{b_1 \cdots b_N} = \prod_{i=1}^{N} \delta^{b_i}_{a_i} = \delta^{b_1}_{a_1} \delta^{b_2}_{a_2} \cdots \delta^{b_N}_{a_N}
\]

\begin{example}[Pairing Tensor for Two Spaces]
For $V_1 = \mathbb{R}^2$ and $V_2 = \mathbb{R}^3$:
\[
P(v_1, v_2, \omega_1, \omega_2) = \omega_1(v_1) \cdot \omega_2(v_2)
\]

Components: $P_{ab}^{ij} = \delta^i_a \delta^j_b$

\textbf{Matrix representation}: We can think of this as a $4$-index tensor. For fixed $v_2$ and $\omega_2$, we get a $2 \times 2$ matrix acting on $(v_1, \omega_1)$.
\end{example}

\section*{Contraction Operations}

\begin{definition}[Contraction on Space $V_i$]
Given a tensor $T$ as above, we can \textbf{contract} on space $V_i$ by summing over matching indices:
\[
S_{a_1 \cdots \widehat{a_i} \cdots a_N}^{b_1 \cdots \widehat{b_i} \cdots b_N} = \sum_{c=1}^{n_i} T_{a_1 \cdots a_{i-1} \, c \, a_{i+1} \cdots a_N}^{b_1 \cdots b_{i-1} \, c \, b_{i+1} \cdots b_N}
\]

where $\widehat{\phantom{a}}$ indicates omitted indices.

This produces a new tensor on the remaining $N-1$ spaces.
\end{definition}

\begin{example}[Contraction Example]
For $T: V_1 \times V_2 \times V_1^* \times V_2^* \to \mathbb{R}$ with components $T_{ab}^{ij}$, contracting on $V_1$ gives:
\[
S_b^j = T_{cb}^{cj} = \sum_{c=1}^{n_1} T_{cb}^{cj}
\]

This is now a tensor $S: V_2 \times V_2^* \to \mathbb{R}$.

If $T_{ab}^{ij} = \delta^i_a \delta^j_b$ (natural pairing), then:
\[
S_b^j = \delta^c_c \delta^j_b = n_1 \delta^j_b
\]

So $S(v_2, \omega_2) = n_1 \, \omega_2(v_2)$.
\end{example}

\section*{Applications}

\begin{example}[Quantum Entanglement]
In quantum mechanics, consider $N$ quantum systems with Hilbert spaces $\mathcal{H}_1, \ldots, \mathcal{H}_N$.

An observable on the composite system can be represented as:
\[
O: \mathcal{H}_1 \times \cdots \times \mathcal{H}_N \times \mathcal{H}_1^* \times \cdots \times \mathcal{H}_N^* \to \mathbb{C}
\]

For a separable observable (product of individual observables):
\[
O(\psi_1, \ldots, \psi_N, \phi_1, \ldots, \phi_N) = \prod_{i=1}^{N} \langle \phi_i, \psi_i \rangle_i
\]

Entangled observables involve more complex tensor structures.
\end{example}

\begin{example}[Multi-Body Interactions in Physics]
Consider $N$ particles with:
\begin{itemize}
\item Position spaces $V_i = \mathbb{R}^3$
\item Momentum spaces $V_i^* = (\mathbb{R}^3)^*$
\end{itemize}

An $N$-body interaction potential can be represented as:
\[
U: \mathbb{R}^3 \times \cdots \times \mathbb{R}^3 \times (\mathbb{R}^3)^* \times \cdots \times (\mathbb{R}^3)^* \to \mathbb{R}
\]

For example, electrostatic interactions involve tensors coupling positions from different particles.
\end{example}

\begin{example}[Network Flow Problems]
Consider $N$ nodes in a network, each with:
\begin{itemize}
\item Input space $V_i$ (incoming flows)
\item Output space $V_i^*$ (outgoing flows)
\end{itemize}

A cost functional for the entire network can be:
\[
C: V_1 \times \cdots \times V_N \times V_1^* \times \cdots \times V_N^* \to \mathbb{R}
\]

This captures how inputs and outputs at all nodes jointly contribute to overall network cost.
\end{example}

\section*{Symmetry Properties}


The tensor $T$ may have various symmetry properties:

\begin{enumerate}
\item \textbf{Symmetric in vector arguments}: $T(\ldots, v_i, \ldots, v_j, \ldots) = T(\ldots, v_j, \ldots, v_i, \ldots)$

\item \textbf{Symmetric in dual arguments}: $T(\ldots, \omega_i, \ldots, \omega_j, \ldots) = T(\ldots, \omega_j, \ldots, \omega_i, \ldots)$

\item \textbf{Paired symmetry}: $T(v_1, \ldots, v_N, \omega_{\sigma(1)}, \ldots, \omega_{\sigma(N)})$ depends on permutation $\sigma$
\end{enumerate}

These reduce the number of independent components.






\section*{Appendix 1. Modules}

A \textbf{module} is a fundamental algebraic structure that generalizes the concept of a vector space. While vector spaces are defined over fields, modules are defined over rings. This generalization makes modules more flexible but also more complex.

\subsection{The Need for Modules}

\begin{itemize}
    \item \textbf{Vector spaces}: Require a \textbf{field} as the scalar set
    \item \textbf{Modules}: Allow a \textbf{ring} as the scalar set
    \item \textbf{Key difference}: In a ring, not all elements have multiplicative inverses, making modules more general but with more subtle structure
\end{itemize}

\subsection{Formal Definition}

\begin{definition}[Module over a Ring]
Let $R$ be a \textbf{ring} (with unity $1_R$). A \textbf{left $R$-module} is an abelian group $(M, +)$ together with a scalar multiplication operation:
\[
\cdot: R \times M \to M
\]
denoted $(r, m) \mapsto r \cdot m$ or simply $rm$, satisfying for all $r, s \in R$ and $m, n \in M$:

\begin{enumerate}[label=(\roman*)]
    \item \textbf{Distributivity over module addition}:
    \[
    r(m + n) = rm + rn
    \]
    
    \item \textbf{Distributivity over ring addition}:
    \[
    (r + s)m = rm + sm
    \]
    
    \item \textbf{Compatibility with ring multiplication}:
    \[
    (rs)m = r(sm)
    \]
    
    \item \textbf{Identity action}:
    \[
    1_R \cdot m = m
    \]
\end{enumerate}

We denote this structure as ${}_R M$ (left module) or simply $M$ when the ring is clear.
\end{definition}


\section*{Appendix 2. Rotations in 2-D and 3-D}
Rotations in Euclidean space provide excellent examples of mathematical groups. While 2D rotations form a simple abelian group, 3D rotations exhibit richer, non-abelian structure. Both are crucial in physics, computer graphics, and machine learning.


\subsection{Rotations in Two Dimensions}
A rotation in the plane by angle $\theta$ about the origin can be represented in multiple equivalent ways:

\begin{enumerate}
    \item \textbf{As an angle}: $\theta \in [0, 2\pi)$ or $\mathbb{R} \mod 2\pi$
    \item \textbf{As a complex number}: $e^{i\theta} = \cos\theta + i\sin\theta$ on the unit circle
    \item \textbf{As a 2×2 matrix}:
    \[
    R(\theta) = \begin{pmatrix}
    \cos\theta & -\sin\theta \\
    \sin\theta & \cos\theta
    \end{pmatrix}
    \]
\end{enumerate}

\begin{center}
\begin{tikzpicture}[scale=1.2]
    % Axes
    \draw[->] (-2,0) -- (2,0) node[right] {$x$};
    \draw[->] (0,-2) -- (0,2) node[above] {$y$};
    
    % Unit circle
    \draw[dashed] (0,0) circle (1.5);
    
    % Initial vector
    \draw[thick, red, ->] (0,0) -- (1.5,0) node[midway, above] {$\mathbf{v}$};
    
    % Rotated vector
    \draw[thick, blue, ->] (0,0) -- (0.75,1.299) node[midway, above left] {$R(\theta)\mathbf{v}$};
    
    % Angle arc
    \draw (0.5,0) arc (0:60:0.5);
    \node at (0.7,0.2) {$\theta$};
    
    % Dotted line for projection
    \draw[dotted] (0.75,1.299) -- (0.75,0);
    \draw[dotted] (0.75,1.299) -- (0,1.299);
\end{tikzpicture}
\end{center}

\subsection{2D Rotations Form a Group}
Let $G = \{R(\theta) : \theta \in \mathbb{R} \mod 2\pi\}$ with matrix multiplication as the group operation.

\begin{proof}[Verification of Group Axioms]
\begin{enumerate}
    \item \textbf{Closure}: 
    \[
    R(\theta_1)R(\theta_2) = 
    \begin{pmatrix}
    \cos\theta_1 & -\sin\theta_1 \\
    \sin\theta_1 & \cos\theta_1
    \end{pmatrix}
    \begin{pmatrix}
    \cos\theta_2 & -\sin\theta_2 \\
    \sin\theta_2 & \cos\theta_2
    \end{pmatrix}
    = R(\theta_1 + \theta_2) \in G
    \]
    
    \item \textbf{Associativity}: Matrix multiplication is associative.
    
    \item \textbf{Identity}: 
    \[
    R(0) = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} = I_2
    \]
    
    \item \textbf{Inverse}: 
    \[
    R(\theta)^{-1} = R(-\theta) = \begin{pmatrix}
    \cos\theta & \sin\theta \\
    -\sin\theta & \cos\theta
    \end{pmatrix}
    \]
    since $R(\theta)R(-\theta) = R(0) = I_2$.
\end{enumerate}
\end{proof}

\subsection{Properties of the 2D Rotation Group}
\begin{itemize}
    \item \textbf{Abelian (commutative)}: 
    \[
    R(\theta_1)R(\theta_2) = R(\theta_1 + \theta_2) = R(\theta_2 + \theta_1) = R(\theta_2)R(\theta_1)
    \]
    
    \item \textbf{Isomorphic to the circle group}: 
    \[
    \text{SO}(2) \cong U(1) \cong S^1
    \]
    where SO(2) = Special Orthogonal group in 2D, $U(1)$ = unitary complex numbers, $S^1$ = unit circle.
    
    \item \textbf{One-dimensional}: Parameterized by a single parameter $\theta$.
\end{itemize}

\section{Rotations in Three Dimensions}

\subsection{Representation of 3D Rotations}
3D rotations are more complex due to non-commutativity. They can be represented as:

\begin{enumerate}
    \item \textbf{Axis-angle representation}: $(\hat{\mathbf{n}}, \theta)$ where $\hat{\mathbf{n}}$ is a unit vector (axis) and $\theta$ is the angle.
    \item \textbf{3×3 rotation matrices}: Elements of SO(3), satisfying:
    \[
    R^T R = I_3 \quad \text{and} \quad \det(R) = 1
    \]
    \item \textbf{Quaternions}: $q = \cos(\theta/2) + \sin(\theta/2)(n_x i + n_y j + n_z k)$
\end{enumerate}

\begin{center}
\begin{tikzpicture}[scale=1.2]
    % Axes
    \draw[->] (0,0,0) -- (2,0,0) node[right] {$x$};
    \draw[->] (0,0,0) -- (0,2,0) node[above] {$y$};
    \draw[->] (0,0,0) -- (0,0,2) node[below left] {$z$};
    
    % Rotation axis
    \draw[thick, purple, ->] (0,0,0) -- (1.5,1.5,1.5) node[midway, above left] {$\hat{\mathbf{n}}$};
    
    % Vector being rotated
    \draw[thick, red, ->] (0,0,0) -- (1,1,0) node[midway, below right] {$\mathbf{v}$};
    
    % Rotated vector
    \draw[thick, blue, ->] (0,0,0) -- (0,1,1) node[midway, above right] {$R\mathbf{v}$};
    
    % Rotation path (simplified)
    \draw[dashed, gray] (1,1,0) arc (45:90:1.414 and 1);
\end{tikzpicture}
\end{center}

\subsection{SO(3): The 3D Rotation Group}
Let SO(3) = $\{R \in \mathbb{R}^{3\times 3} : R^T R = I_3, \ \det(R) = 1\}$ with matrix multiplication.

\begin{proof}[Verification of Group Axioms]
\begin{enumerate}
    \item \textbf{Closure}: If $R_1, R_2 \in \text{SO}(3)$, then:
    \[
    (R_1 R_2)^T (R_1 R_2) = R_2^T R_1^T R_1 R_2 = R_2^T I_3 R_2 = I_3
    \]
    and $\det(R_1 R_2) = \det(R_1)\det(R_2) = 1 \cdot 1 = 1$.
    
    \item \textbf{Associativity}: Matrix multiplication is associative.
    
    \item \textbf{Identity}: $I_3 \in \text{SO}(3)$ since $I_3^T I_3 = I_3$ and $\det(I_3) = 1$.
    
    \item \textbf{Inverse}: For $R \in \text{SO}(3)$, $R^{-1} = R^T \in \text{SO}(3)$ because:
    \[
    (R^T)^T R^T = R R^T = I_3 \quad \text{and} \quad \det(R^T) = \det(R) = 1
    \]
\end{enumerate}
\end{proof}

\subsection{Key Differences from 2D Case}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Property} & \textbf{2D Rotations (SO(2))} & \textbf{3D Rotations (SO(3))} \\
\hline
Commutative & Yes (Abelian) & No (Non-abelian) \\
Dimension & 1 & 3 \\
Parameterization & Single angle $\theta$ & 3 parameters (Euler angles: $\alpha,\beta,\gamma$ \\
& & or axis-angle: $\hat{\mathbf{n}},\theta$) \\
Manifold structure & Circle $S^1$ & 3-sphere with antipodes identified \\
& & (Real projective space $\mathbb{RP}^3$) \\
\hline
\end{tabular}
\end{table}

\subsection{Non-Commutativity Example in 3D}
Rotations in 3D generally do not commute. Consider:
\begin{align*}
R_x(90^\circ) &= \text{Rotation by } 90^\circ \text{ about x-axis} \\
R_y(90^\circ) &= \text{Rotation by } 90^\circ \text{ about y-axis}
\end{align*}

Then:
\[
R_x(90^\circ) R_y(90^\circ) \neq R_y(90^\circ) R_x(90^\circ)
\]

\begin{center}
\begin{tikzpicture}[scale=0.8]
\node at (0,0) {$\begin{pmatrix} 1 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & 1 & 0 \end{pmatrix}
\begin{pmatrix} 0 & 0 & 1 \\ 0 & 1 & 0 \\ -1 & 0 & 0 \end{pmatrix}
=
\begin{pmatrix} 0 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \end{pmatrix}$};
\node at (0,-2.5) {$\begin{pmatrix} 0 & 0 & 1 \\ 0 & 1 & 0 \\ -1 & 0 & 0 \end{pmatrix}
\begin{pmatrix} 1 & 0 & 0 \\ 0 & 0 & -1 \\ 0 & 1 & 0 \end{pmatrix}
=
\begin{pmatrix} 0 & 1 & 0 \\ 0 & 0 & -1 \\ -1 & 0 & 0 \end{pmatrix}$};
\end{tikzpicture}
\end{center}

These are different matrices, demonstrating non-commutativity.

\section{Applications in Machine Learning}

\subsection{Data Augmentation}
Rotation groups provide natural symmetries for data augmentation:
\begin{itemize}
    \item Image rotation (SO(2) for 2D images)
    \item 3D object rotation (SO(3) for point clouds, meshes)
\end{itemize}

\subsection{Equivariant Neural Networks}
Networks designed to respect rotational symmetry:
\begin{itemize}
    \item \textbf{SO(2)-equivariant CNNs}: For 2D images
    \item \textbf{SO(3)-equivariant networks}: For 3D molecular data, point clouds
    \item Use group representation theory to design filters
\end{itemize}

\subsection{Geometric Deep Learning}
\begin{itemize}
    \item SE(3) networks (combining SO(3) with translations)
    \item Spherical CNNs for data on spheres
    \item Steerable filters
\end{itemize}

\section*{Summary}
\begin{itemize}
    \item \textbf{2D rotations} form the abelian group SO(2) $\cong$ U(1) $\cong$ S$^1$
    \item \textbf{3D rotations} form the non-abelian group SO(3)
    \item Both are \textbf{Lie groups} (groups that are also smooth manifolds)
    \item SO(3) has dimension 3, while SO(2) has dimension 1
    \item The non-commutativity of 3D rotations has profound mathematical and physical consequences
    \item These groups are fundamental in machine learning for handling rotational symmetry
\end{itemize}

\begin{theorem}
Both SO(2) and SO(3) are compact connected Lie groups. SO(2) is abelian, while SO(3) is simple and non-abelian.
\end{theorem}


\section{Appendix 3: $SO(2)$, $SO(3)$}

The \textbf{Special Orthogonal groups} SO(2) and SO(3) are fundamental mathematical structures that describe rotations in two and three dimensions, respectively. They are essential in physics, computer graphics, robotics, and machine learning.


\begin{definition}[Orthogonal Matrix]
An $n \times n$ real matrix $R$ is \textbf{orthogonal} if:
\[
R^T R = R R^T = I_n
\]
where $R^T$ is the transpose of $R$ and $I_n$ is the $n \times n$ identity matrix.
\end{definition}

\begin{definition}[Determinant of Orthogonal Matrices]
For any orthogonal matrix $R$, $\det(R) = \pm 1$.
\end{definition}

\section{SO(2): Rotations in Two Dimensions}

\begin{definition}[SO(2)]
The \textbf{Special Orthogonal group in 2 dimensions}, denoted SO(2), is the set of all $2 \times 2$ orthogonal matrices with determinant 1:
\[
\text{SO}(2) = \{R \in \mathbb{R}^{2\times2} : R^T R = I_2, \ \det(R) = 1\}
\]
\end{definition}

\subsection{Matrix Representation}

Every element of SO(2) can be written as:
\[
R(\theta) = \begin{pmatrix}
\cos\theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{pmatrix}
\quad \text{for some } \theta \in \mathbb{R}
\]

\subsection{Properties of SO(2)}

\begin{theorem}[Group Structure of SO(2)]
SO(2) forms a group under matrix multiplication:
\begin{enumerate}
    \item \textbf{Closure}: $R(\theta_1)R(\theta_2) = R(\theta_1 + \theta_2) \in \text{SO}(2)$
    \item \textbf{Associativity}: Matrix multiplication is associative
    \item \textbf{Identity}: $R(0) = I_2$
    \item \textbf{Inverse}: $R(\theta)^{-1} = R(-\theta) = R(\theta)^T$
\end{enumerate}
\end{theorem}

\begin{theorem}[SO(2) is Abelian]
SO(2) is commutative:
\[
R(\theta_1)R(\theta_2) = R(\theta_1 + \theta_2) = R(\theta_2 + \theta_1) = R(\theta_2)R(\theta_1)
\]
\end{theorem}


\subsection{Algebraic Properties}

\begin{theorem}[Isomorphisms of SO(2)]
SO(2) is isomorphic to:
\begin{enumerate}
    \item The \textbf{circle group} $U(1) = \{e^{i\theta} : \theta \in \mathbb{R}\}$
    \item The \textbf{1-sphere} $S^1 = \{(x,y) \in \mathbb{R}^2 : x^2 + y^2 = 1\}$
    \item The \textbf{real numbers modulo $2\pi$}: $\mathbb{R}/2\pi\mathbb{Z}$
\end{enumerate}
\end{theorem}


\section{SO(3): Rotations in Three Dimensions}

\begin{definition}[SO(3)]
The \textbf{Special Orthogonal group in 3 dimensions}, denoted SO(3), is the set of all $3 \times 3$ orthogonal matrices with determinant 1:
\[
\text{SO}(3) = \{R \in \mathbb{R}^{3\times3} : R^T R = I_3, \ \det(R) = 1\}
\]
\end{definition}

\subsection{Multiple Representations}

\subsection{1. Axis-Angle Representation}
Every rotation in 3D can be described by an axis $\hat{\bm{n}} \in S^2$ (unit vector) and angle $\theta \in \mathbb{R}$:
\[
R(\hat{\bm{n}}, \theta) = \exp(\theta[\hat{\bm{n}}]_\times)
\]
where $[\hat{\bm{n}}]_\times$ is the cross-product matrix:
\[
[\hat{\bm{n}}]_\times = \begin{pmatrix}
0 & -n_z & n_y \\
n_z & 0 & -n_x \\
-n_y & n_x & 0
\end{pmatrix}
\]

\subsection{2. Euler Angles}
Three successive rotations about coordinate axes (e.g., ZYX convention):
\[
R(\alpha,\beta,\gamma) = R_z(\alpha)R_y(\beta)R_x(\gamma)
\]

\subsection{3. Quaternions}
Unit quaternions $q = (w, \bm{v})$ with $w^2 + \|\bm{v}\|^2 = 1$:
\[
q = \cos(\theta/2) + \sin(\theta/2)(n_x i + n_y j + n_z k)
\]

\subsection{Properties of SO(3)}

\begin{theorem}[Group Structure of SO(3)]
SO(3) forms a group under matrix multiplication:
\begin{enumerate}
    \item \textbf{Closure}: Product of rotations is a rotation
    \item \textbf{Associativity}: Matrix multiplication is associative
    \item \textbf{Identity}: $I_3 \in \text{SO}(3)$
    \item \textbf{Inverse}: $R^{-1} = R^T \in \text{SO}(3)$
\end{enumerate}
\end{theorem}

\begin{theorem}[SO(3) is Non-Abelian]
SO(3) is \textbf{not} commutative. For rotations about different axes:
\[
R_x(\alpha)R_y(\beta) \neq R_y(\beta)R_x(\alpha)
\]
\end{theorem}



\section{Applications}

\subsection{Physics}

\subsection{Computer Graphics and Robotics}
\begin{itemize}
    \item \textbf{SO(2)}: 2D image rotation, sprite animation
    \item \textbf{SO(3)}: 3D object orientation, robot arm kinematics
\end{itemize}

\subsection{Machine Learning}
\begin{itemize}
    \item \textbf{SO(2)-equivariant networks}: For 2D images with rotational symmetry
    \item \textbf{SO(3)-equivariant networks}: For 3D point clouds, molecular data
    \item \textbf{Spherical CNNs}: Operating on $S^2$ using SO(3) representations
\end{itemize}

\subsection{Computer Vision}
\begin{itemize}
    \item Camera calibration and pose estimation
    \item Structure from motion
    \item Image registration
\end{itemize}

\section{Matrix Examples}

\subsection{SO(2) Example}
Rotation by $\theta = \pi/3$ (60°):
\[
R(\pi/3) = \begin{pmatrix}
\cos(\pi/3) & -\sin(\pi/3) \\
\sin(\pi/3) & \cos(\pi/3)
\end{pmatrix} = 
\begin{pmatrix}
\frac{1}{2} & -\frac{\sqrt{3}}{2} \\
\frac{\sqrt{3}}{2} & \frac{1}{2}
\end{pmatrix}
\]

\subsection{SO(3) Example}
Rotation by $\pi/2$ about x-axis:
\[
R_x(\pi/2) = \begin{pmatrix}
1 & 0 & 0 \\
0 & 0 & -1 \\
0 & 1 & 0
\end{pmatrix}
\]

Rotation by $\pi/2$ about y-axis:
\[
R_y(\pi/2) = \begin{pmatrix}
0 & 0 & 1 \\
0 & 1 & 0 \\
-1 & 0 & 0
\end{pmatrix}
\]

Their non-commutativity:
\[
R_x(\pi/2)R_y(\pi/2) = \begin{pmatrix}
0 & 0 & 1 \\
1 & 0 & 0 \\
0 & 1 & 0
\end{pmatrix} \neq
R_y(\pi/2)R_x(\pi/2) = \begin{pmatrix}
0 & 1 & 0 \\
0 & 0 & -1 \\
-1 & 0 & 0
\end{pmatrix}
\]



\section{Summary}

\begin{itemize}
    \item \textbf{SO(2)} represents all rotations in the plane. It's a 1-dimensional abelian group isomorphic to the circle.
    
    \item \textbf{SO(3)} represents all rotations in 3D space. It's a 3-dimensional non-abelian group with rich structure.
    
    \item The key difference is \textbf{commutativity}: SO(2) is abelian, while SO(3) is not.
    
    \item These groups are fundamental in physics and engineering for describing rotational symmetry.
\end{itemize}



\section{Appendix 4: Symmetries of a Pentagon}

We construct a group with 10 elements: the \textbf{dihedral group $D_5$}, which represents symmetries of a regular pentagon. The "triangle" operation $\triangle$ will represent function composition of these symmetries.

Let $G = \{e, r, r^2, r^3, r^4, s, sr, sr^2, sr^3, sr^4\}$ where:
\begin{itemize}
    \item $e$ = identity (do nothing)
    \item $r$ = rotation by $72^\circ$ ($2\pi/5$ radians) clockwise
    \item $r^k$ = rotation by $72k^\circ$ (composition of $k$ rotations)
    \item $s$ = reflection across vertical axis (fixing vertex at top)
    \item $sr^k$ = reflection followed by rotation
\end{itemize}

\begin{center}
\begin{tikzpicture}[scale=1.2]
    % Pentagon
    \draw[thick] (0,0) circle (1.5);
    \draw[thick, fill=blue!10] (90:1.5) -- (90-72:1.5) -- (90-144:1.5) -- (90-216:1.5) -- (90-288:1.5) -- cycle;
    
    % Vertices with labels
    \foreach \i in {0,...,4} {
        \node at ({90-72*\i}:1.7) {$v_{\i}$};
        \fill ({90-72*\i}:1.5) circle (2pt);
    }
    
    % Rotation arrow
    \draw[->, thick, red] (0.8,0) arc (0:-72:0.8);
    \node at (0.4,-0.3) {$r$};
    
    % Reflection line
    \draw[dashed, thick, green!70!black] (0,1.8) -- (0,-1.8);
    \node at (0.3,1.5) {$s$};
\end{tikzpicture}
\end{center}

\section{The "Triangle" Operation}

Define $\triangle: G \times G \to G$ as function composition:
\[
a \triangle b = b \circ a \quad \text{(apply $a$ first, then $b$)}
\]
or equivalently in our notation: $a \triangle b$ means "do symmetry $a$, then symmetry $b$".

\section{Group Axioms Verification}

\begin{enumerate}
    \item \textbf{Closure}: Composition of any two symmetries yields another symmetry of the pentagon.
    
    \item \textbf{Associativity}: Function composition is always associative.
    
    \item \textbf{Identity}: $e$ is the identity: $e \triangle a = a \triangle e = a$ for all $a \in G$.
    
    \item \textbf{Inverses}:
    \begin{itemize}
        \item $r^k$ has inverse $r^{5-k}$ (since $r^5 = e$)
        \item $s$ is its own inverse: $s \triangle s = e$
        \item $(sr^k)^{-1} = sr^{5-k}$
    \end{itemize}
\end{enumerate}

\section{Group Table for $D_5$}

The following table defines $a \triangle b$ (read $a$ from left column, $b$ from top row):

{\centering
\newcommand{\mc}[1]{\multicolumn{1}{|c|}{#1}}
\begin{tabular}{|c|cccccccccc|}
\hline
$\triangle$ & $e$ & $r$ & $r^2$ & $r^3$ & $r^4$ & $s$ & $sr$ & $sr^2$ & $sr^3$ & $sr^4$ \\
\hline
$e$ & $e$ & $r$ & $r^2$ & $r^3$ & $r^4$ & $s$ & $sr$ & $sr^2$ & $sr^3$ & $sr^4$ \\
$r$ & $r$ & $r^2$ & $r^3$ & $r^4$ & $e$ & $sr^4$ & $s$ & $sr$ & $sr^2$ & $sr^3$ \\
$r^2$ & $r^2$ & $r^3$ & $r^4$ & $e$ & $r$ & $sr^3$ & $sr^4$ & $s$ & $sr$ & $sr^2$ \\
$r^3$ & $r^3$ & $r^4$ & $e$ & $r$ & $r^2$ & $sr^2$ & $sr^3$ & $sr^4$ & $s$ & $sr$ \\
$r^4$ & $r^4$ & $e$ & $r$ & $r^2$ & $r^3$ & $sr$ & $sr^2$ & $sr^3$ & $sr^4$ & $s$ \\
$s$ & $s$ & $sr$ & $sr^2$ & $sr^3$ & $sr^4$ & $e$ & $r$ & $r^2$ & $r^3$ & $r^4$ \\
$sr$ & $sr$ & $sr^2$ & $sr^3$ & $sr^4$ & $s$ & $r^4$ & $e$ & $r$ & $r^2$ & $r^3$ \\
$sr^2$ & $sr^2$ & $sr^3$ & $sr^4$ & $s$ & $sr$ & $r^3$ & $r^4$ & $e$ & $r$ & $r^2$ \\
$sr^3$ & $sr^3$ & $sr^4$ & $s$ & $sr$ & $sr^2$ & $r^2$ & $r^3$ & $r^4$ & $e$ & $r$ \\
$sr^4$ & $sr^4$ & $s$ & $sr$ & $sr^2$ & $sr^3$ & $r$ & $r^2$ & $r^3$ & $r^4$ & $e$ \\
\hline
\end{tabular}
\par}

\section{Key Properties}

\subsection{Non-Abelian}
The group is \textbf{non-abelian} (not commutative). For example:
\[
r \triangle s = sr^4 \quad \text{but} \quad s \triangle r = sr
\]
Since $sr^4 \neq sr$, the operation doesn't commute.

\subsection{Subgroup Structure}
\begin{itemize}
    \item $\{e, r, r^2, r^3, r^4\}$ forms a \textbf{cyclic subgroup} of order 5 (rotations only)
    \item $\{e, s\}$ forms a subgroup of order 2
    \item $\{e, sr^k\}$ for any fixed $k$ forms a subgroup of order 2
\end{itemize}

\subsection{Visual Example of Operation}
Consider $r \triangle s = sr^4$:
\begin{enumerate}
    \item Start with pentagon in original position
    \item Apply $s$: Reflect across vertical axis
    \item Apply $r$: Rotate resulting figure by $72^\circ$
    \item The net effect is equivalent to $sr^4$
\end{enumerate}

\begin{center}
\begin{tikzpicture}[scale=0.7]
% Sequence of operations
\node at (0,0) {\textbf{Original}};
\draw (0,0) circle (0.8);
\draw[thick] (90:0.8) -- (90-72:0.8) -- (90-144:0.8) -- (90-216:0.8) -- (90-288:0.8) -- cycle;
\node at (90:1) {$v_0$};

\node at (3,0) {$s$};
\draw[->, thick] (1.2,0) -- (1.8,0);

\node at (6,0) {\textbf{After $s$}};
\draw (6,0) circle (0.8);
\draw[thick] (90:0.8) -- (90+72:0.8) -- (90+144:0.8) -- (90+216:0.8) -- (90+288:0.8) -- cycle;
\node at (6,1) {$v_0$};

\node at (9,0) {$r$};
\draw[->, thick] (7.2,0) -- (7.8,0);

\node at (12,0) {\textbf{After $r \triangle s$}};
\draw (12,0) circle (0.8);
\draw[thick] (90+72:0.8) -- (90+144:0.8) -- (90+216:0.8) -- (90+288:0.8) -- (90:0.8) -- cycle;
\node at (12,1) {$v_1$};
\end{tikzpicture}
\end{center}

\section{Alternative Interpretation: Cyclic Group $\mathbb{Z}_{10}$}

If we want an \textbf{abelian} group of order 10 with triangle as addition modulo 10:

Let $G = \{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\}$ with operation:
\[
a \triangle b = (a + b) \mod 10
\]

\begin{center}
\begin{tabular}{|c|cccccccccc|}
\hline
$\triangle$ & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
\hline
0 & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
1 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 0 \\
2 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 0 & 1 \\
3 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 0 & 1 & 2 \\
4 & 4 & 5 & 6 & 7 & 8 & 9 & 0 & 1 & 2 & 3 \\
5 & 5 & 6 & 7 & 8 & 9 & 0 & 1 & 2 & 3 & 4 \\
6 & 6 & 7 & 8 & 9 & 0 & 1 & 2 & 3 & 4 & 5 \\
7 & 7 & 8 & 9 & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\
8 & 8 & 9 & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
9 & 9 & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\hline
\end{tabular}
\end{center}

This is the \textbf{cyclic group $\mathbb{Z}_{10}$}, which is abelian. (See Appendix 3 for a definition of cyclic group.)

\section{Comparison}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Property} & \textbf{Dihedral $D_5$} & \textbf{Cyclic $\mathbb{Z}_{10}$} \\
\hline
Order & 10 & 10 \\
Abelian? & No & Yes \\
Operation & Function composition & Addition mod 10 \\
Structure & Pentagonal symmetries & Integers modulo 10 \\
Subgroups & More complex & Simple (all cyclic) \\
\hline
\end{tabular}
\end{table}

\section{Conclusion}
\begin{itemize}
    \item The dihedral group $D_5$ provides a natural example of a 10-element group with geometric meaning.
    \item The "triangle" operation represents composition of symmetries.
    \item $D_5$ is non-abelian, illustrating that not all groups are commutative.
    \item Alternatively, $\mathbb{Z}_{10}$ gives an abelian group of order 10.
    \item Both are valid groups, demonstrating the diversity of group structures.
\end{itemize}



\section{Appendix 3:  Cyclic Groups}

\subsection{Intuitive Understanding}
A \textbf{cyclic group} is the mathematical abstraction of "going around in circles" or "clock arithmetic." It represents the simplest possible group structure where every element can be generated from a single starting element.

\begin{center}
\begin{tikzpicture}[scale=1.2]
    % Clock analogy
    \draw (0,0) circle (1.5);
    \foreach \angle/\hour in {90/12, 60/1, 30/2, 0/3, -30/4, -60/5, -90/6, -120/7, -150/8, -180/9, -210/10, -240/11}
    {
        \draw (\angle:1.3) -- (\angle:1.5);
        \node at (\angle:1.8) {\hour};
    }
    \draw[->, thick] (0,0) -- (0,1) node[midway, left] {$g$};
    \node at (0,-2.5) {$\mathbb{Z}_{12}$: Adding 1 hour repeatedly generates all hours};
\end{tikzpicture}
\end{center}

\section{Formal Definition and Properties}

\begin{definition}[Cyclic Group]
A group $(G, \ast)$ is \textbf{cyclic} if there exists an element $g \in G$ such that:
\[
G = \langle g \rangle = \{g^n : n \in \mathbb{Z}\}
\]
where:
\begin{itemize}
    \item $g^0 = e$ (the identity element)
    \item $g^n = \underbrace{g \ast g \ast \cdots \ast g}_{n \text{ times}}$ for $n > 0$
    \item $g^{-n} = (g^{-1})^n = \underbrace{g^{-1} \ast g^{-1} \ast \cdots \ast g^{-1}}_{n \text{ times}}$ for $n > 0$
\end{itemize}
The element $g$ is called a \textbf{generator} of $G$. We write $G = \langle g \rangle$.
\end{definition}

\subsection{Types of Cyclic Groups}
\begin{enumerate}
    \item \textbf{Finite Cyclic Groups}: Have a finite number of elements. Denoted $\mathbb{Z}_n$ or $C_n$.
    \item \textbf{Infinite Cyclic Groups}: Have infinitely many elements. The only example is $\mathbb{Z}$ (integers under addition).
\end{enumerate}

\section{Examples of Cyclic Groups}

\subsection{Example 1: Integers Modulo n ($\mathbb{Z}_n$)}
For any positive integer $n$, the set $\mathbb{Z}_n = \{0, 1, 2, \dots, n-1\}$ with addition modulo $n$ forms a cyclic group.

\begin{example}[$\mathbb{Z}_6$]
Let $G = \{0, 1, 2, 3, 4, 5\}$ with addition modulo 6.
\begin{itemize}
    \item Generator: $1$ generates all elements:
    \begin{align*}
        1 &\rightarrow 1 \\
        1+1 &= 2 \\
        1+1+1 &= 3 \\
        1+1+1+1 &= 4 \\
        1+1+1+1+1 &= 5 \\
        1+1+1+1+1+1 &= 0 \ (\text{mod } 6)
    \end{align*}
    \item Also generator: $5$ (since $5 \equiv -1 \mod 6$)
    \item Not generator: $2$ only generates $\{0, 2, 4\}$ (subgroup of order 3)
    \item Order: $|G| = 6$
\end{itemize}
\end{example}


\end{document}



\section{Probability Theory and Random Phenomena}

\newthought{Probability theory} is a branch of mathematics that studies the behavior of {\em random phenomena } through a formal system of axioms. It is built upon and closely related to \textit{measure theory}, a broader mathematical framework that generalizes the notions of length, area, and volume in $\mathbb{R}^n$ by assigning a measure to subsets of a given space.

Measure theory serves as the foundational framework for {\em  integrating functions} over general spaces and plays a pivotal role in probability theory by enabling the rigorous definition of probability measures. Among the classical measures developed within this framework are the Jordan, Lebesgue, and Borel measures, which extend intuitive notions of length, area, and volume. More specialized constructs—such as complex measures, Haar measures (on locally compact groups), and probability measures—are designed to capture diverse properties of sets and functions across functional analysis, topology, and stochastic modeling.


\subsection{Random Phenomena}
Probability theory is useful for modeling \textit{random phenomena} because it provides a mathematically rigorous way to describe uncertainty, quantify risk, and predict patterns in systems where outcomes are not deterministic.
\begin{mydef}{Random Phenomenon}{theoexample}
A phenomenon or procedure for generating data is \textit{random} if
\begin{itemize}
\item the outcome is not predictable in advance;
\item there is a predictable  long-term pattern that can be described by the distribution
of outcomes over very many observations.
\end{itemize}

%  \ref{th:theoexample} and is given on page \pageref{th:theoexample}.
\end{mydef}

\begin{mydef}{Random Outcome}{theoexample}
A \textit{random outcome} is the result of a random phenomenon or procedure.
\end{mydef}
The outcome of an individual random experiment cannot be predicted with certainty, but
the set of all \textit{possible} outcomes is known in advance. 

\begin{mybox}{Rolling a Dice}{theoexample}
Imagine I roll a fair die privately, and I tell you if the resulting throw is odd or even.
\begin{itemize}
\item The possible outcomes are integers from one to six.
\item The information available to you is whether the roll is odd or even.
\item Probabilities are computed on the basis that each outcome is equally likely, so we have $\frac{1}{2}$ chance of obtaining odd/even.
\end{itemize}
\end{mybox}
 \begin{marginfigure}
\includegraphics{fig/code1.png}
\caption{R code implementation of the random experiment described in Example 1. Outcomes have equal probability (i.e., the dice is fair). }
\end{marginfigure}

\subsection{Random Phenomena Examples}

Outcomes in Health Sciences that are unpredictable can be modeled, in principle, as random phenomena. A principled modeling of these events
can have a significant impact on patient care, research, and decision-making. The following are examples of random phenomena.

%\begin{mybox}{Random Phenomena in Health Sciences}{theoexample}
\begin{enumerate}
\item \textit{Patient outcomes}: The response of individual patients to medical treatments or interventions can vary due to random factors. For instance, some patients may recover quickly from surgery while others with similar conditions may experience complications, and these differences can be influenced by factors that are not fully understood.

\item \textit{Disease spread}: The spread of infectious diseases can exhibit random patterns. Factors such as the movement of infected individuals, contact patterns, and the effectiveness of preventive measures can all contribute to the randomness of disease transmission.

\item \textit{Clinical trials}: Randomized controlled trials (RCTs) are widely used in healthcare research to evaluate the efficacy of new treatments or interventions. The random assignment of participants to treatment and control groups helps reduce bias and accounts for random variations in outcomes.

\item \textit{Emergency Room traffic}: The number of patients arriving at an emergency room can vary significantly from day to day or even hour to hour. Random factors like accidents, weather conditions, and disease outbreaks can influence the patient flow.

\item \textit{Medication response}: Some patients may respond differently to the same medication due to genetic variations, environmental factors, or random fluctuations in their physiology. This can complicate medication management and dosing.

\item \textit{Diagnostic Testing}: The results of diagnostic tests, such as blood tests or imaging scans, may show variability due to random measurement error, equipment calibration, or sample handling procedures.

\item \textit{Disease Outbreaks}: The occurrence and severity of disease outbreaks, such as flu epidemics or COVID-19 surges, can exhibit random patterns influenced by factors like population density, vaccination rates, and individual behavior.

\item \textit{Healthcare resource allocation}: The allocation of healthcare resources, such as hospital beds and ventilators during a public health crisis, can be influenced by random factors like the sudden surge in cases or unexpected logistical challenges.

\item \textit{Healthcare costs}: The cost of healthcare services can vary randomly due to factors such as fluctuations in the prices of medical supplies, changes in insurance coverage, and unexpected healthcare demands.

\item  \textit{Healthcare workforce availability}: The availability of healthcare professionals, including doctors, nurses, and support staff, can be influenced by random factors like staff illnesses or scheduling conflicts.
\end{enumerate}
%\end{mybox}



\subsection{Random Experiments}

Probability theory is also useful for modeling  \textit{random experiments}.

\begin{mydef}{Random Experiment}{theoexample}
A \textit{random experiment} is understood as any procedure such that when it is repeated under the same initial conditions, the result obtained is not always the same. It is characterized by three main features.
\begin{itemize}
\item The possible outcomes of the experiment.
\item The events we can observe, i.e. the information that is revealed at the end of the experiment.
\item The probabilities assigned to each event.
\end{itemize}
\end{mydef}

In practical applications, a random experiment can, in principle, be repeated numerous times under the same
conditions. The outcomes of individual experiments must be independent, and must in no
way be affected by any previous outcome.





\section{Random Phenomena, Random Experiments, Measure Theory and  Probability Theories}



\newthought{Measure theory} is concerned with the problem of how to assign a size to certain sets, enabling a principle definition of a probability measure, which is the principal tool of probability theory.
In daily life, assigning a size  to sets is easy to do:
\begin{itemize}
\item count: $\{a,b,c,\dots,x,y,x\}$ has 26 letters;
\item take measurements: length (in one dimension), area (in two dimensions), volume (in three dimensions), or time;
\item calculate the odds of winning the lottery.
\end{itemize}
In each case, we compare and express the result with respect to some base unit (Figure 4-1).
\begin{marginfigure}
\includegraphics{fig/Lebesgue.png}
\caption{Henri Léon Lebesgue (French, 1875-1941) was a French mathematician known for his theory of integration, which was a generalization of the 17th-century concept of integration—summing the area between an axis and the curve of a function defined for that axis. His theory was published originally in his dissertation Intégrale, longueur, aire ("Integral, length, area") at the University of Nancy in 1902.}
\end{marginfigure}


\begin{figure}
\includegraphics{fig/meas.png}
\caption{Measure theory is concerned with the problem of how to assign a size to certain sets. 
In daily life this is easy to do: we can count, take measurements, or calculate rates. In each case, we compare
and express the result with respect to some base unit.  

The area of the circle in 5 can be computed as follows.
 $$\textsf{area}(\bigcirc) = lim_{n\longrightarrow \infty} 2^n \times \textsf{area}( \bigtriangleup\, \textsf{ at step}\,n).$$
Source: R. L. SchillingMeasures, Integrals, and Martingales, 2nd edition, Cambridge University Press, 2017.
}
\end{figure}


\subsection{Probability Measure Key Features}

Notice that triangles are more flexible than rectangles since we can represent
every rectangle, and actually any odd-shaped quadrangle, as the `sum'
of two non-overlapping triangles Fig. 4-2. In doing so we have assumed a few things:
\begin{itemize}
\item In Fig. 4-3 we have chosen a \textit{particular} baseline and the corresponding height 
arbitrarily. But the concept of \textit{area} should not depend on such choice and the
calculation this choice entails. 
\item The independence of the area from the way we calculate it is called well-definedness.
Plainly, we have the choices shown in Fig 4-3. Notice that Fig 4-3 allows us to pick the most convenient method
to work out the area. 
\item In Fig. 4-2 we actually use two facts:
\begin{itemize}
\item the area of non-overlapping (disjoint) sets can be added, i.e.
\begin{equation*}
\textsf{area}(A) = \alpha,\, \textsf{area}(B) = \beta\,, A\cap B = \emptyset \rightarrow, \textsf{area}(A\cup B) = \alpha + \beta.
\end{equation*}
\end{itemize}
\end{itemize}
\begin{marginfigure}
\centering
\includegraphics[width=4cm]{fig/dice.png}
\caption{Measures over finite and countable sets have the properties described above. For instance, given a fair dice, $$\mu(\{\epsdice{2},\epsdice{4},\epsdice{6}\})= \mu(\{\epsdice{2}\})+ \mu(\{\epsdice{4}\}) +\mu(\{\epsdice{6}\}) = \frac{1}{2}.$$
}
\end{marginfigure}
This shows that the least we should expect from a reasonable measure $\mu$ is that it is
\begin{itemize}
\item well-defined, takes values in $[0,\infty]$, and $\mu(\emptyset)=0$;
\item additive, i.e., $\mu(A \cap B) = \mu(A) + \mu(B)$ whenever $A\cap B = \emptyset$; 
\item is invariant under congruences and translations, which is a characteristic property of length, area, and volume (the Lebesgue measure in $\mathbb R^n$).
\end{itemize}

 


Measures defined on the set of outcomes/events of a random phenomena/experiments enable a principled definition definition
of probability.

\begin{figure}
\includegraphics{fig/overview.png}
\caption{Kolmogorov axions principal concepts.
}
\end{figure}



\subsection{Kolmogorov's Axioms}




\newthought{Kolmogorov's axioms} are the foundations of probability theory. These axioms were introduced by Andrey Kolmogorov in 1933. 
These axioms remain central and have direct contributions to real-world probability cases. 

These axioms are expressed using the fundamental concept of \textit{probability space}  $(\Omega,\mathscr F, P)$, where $\Omega$ is the sample space, $\mathscr F$ is a $\sigma$-algebra, and $P$ is a probability measure. The elements of $\Omega$ are often called outcomes or  \textit{elementary events}, and the elements of $\mathscr F$, which are subsets of $\Omega$, are called \textit{events}.

Informally speaking a probability space is like a special playground where we play with chance and randomness. It's made up of three important parts:

\begin{itemize}
\item \textit{Sample space:} This is like a list of all the possible things that can happen when we're dealing with something random. For example, if we're rolling a die, the sample space includes all the possible outcomes: 1, 2, 3, 4, 5, and 6.

\begin{marginfigure}
\includegraphics{fig/Kolmogorov.png}
\caption{Andrey Nikolaevich Kolmogorov (Russian, 1903-1987) was a Soviet mathematician who contributed to the mathematics of probability theory, topology, intuitionistic logic, turbulence, classical mechanics, algorithmic information theory, and computational complexity. His contributions to probability theory
provided a principled approach to the study of random phenomena. }
\end{marginfigure}


\item \textit{Events}: These are like the games we play in our probability playground. Events are just groups of outcomes from the sample space. For example, the event of "getting an even number" when rolling a die includes outcomes 2, 4, and 6 (see Box 1).

\item \textit{Probability measure}: This is like a rule that tells us how likely each event is. It's like saying, "In our playground, the chance of this game happening is this much." For example, the probability of getting an even number when rolling a fair six-sided die is 1/2 because there are three even outcomes out of six possible outcomes.
\end{itemize}



\subsection{Probability Space}
\newthought{The probability space concept} sets a principled frame to model random phenomena and random experiments. 
Here we provide an interpretation of each of the elements of a probability space: sample space, $\sigma$-algebra, and probability measure.
\begin{figure}
\includegraphics{fig/dm.png}
\caption{Probability space for throwing a die twice in succession: The sample space 
$\Omega$  consists of all 36 possible elementary outcomes; three different events (colored polygons) are shown with their respective probabilities (assuming that the dice are fair). Source: https://en.wikipedia.org}
\end{figure}
\begin{mydef}{Probability Space}{theoexample}     
A probability space consists of an ordered triplet $(\Omega, \mathscr F, P)$, where $\Omega$ is an arbitrary \textit{set} called \textit{sample space},  $\mathscr F$ is a $\sigma$-algebra of subsets of $\Omega$, and $P$ is a probability measure defined on $\mathscr F$.
\end{mydef}



\begin{marginfigure}
\centering
\includegraphics[width=4cm]{fig/ss.png}
\caption{Sample space $\Omega$ containing three possible outcomes $\omega_1$, $\omega_2$, and $\omega_3$.
}
\end{marginfigure}

\subsection{Sample Space $\Omega$}

Loosely speaking,  sample space in probability is like a list of all the things that could possibly happen when you're dealing with something random, like flipping a coin or rolling a die. Imagine you're flipping a coin. The sample space for this is just a list of the two things that can happen: "heads" and "tails." This is the sample space associated with flipping a coin.  So, a sample space is like a simple list of all the different outcomes you might get when you do something random. It helps you see what could happen and helps you figure out the chances or probabilities of each outcome.

\begin{mydef}{Sample Space}{theoexample}     
A \textit{sample space} is a set $\Omega$ containing all possible outcomes of a random phenomenon or procedure.
An \textit{outcome} $\omega$  is an element in $\Omega$, i.e., $\omega \in \Omega$ (which we may or may not observed).
\end{mydef}

\begin{itemize}
\item Rolling a die: $\Omega=\{1,2,3,4,5,6\}$.
\item Flipping two coins: $\Omega=\{\mathsf{HH,HT,TH,TT}\}$.
\item Type in “random.rand()” on a Python shell: $\Omega=[0,1]$.
\item The temperature in my room  from today to time $T$: $\Omega= \{f(t): f(t) \geq \mathsf{-273.1}^o \text{C}\in \mathsf{ [0,T]}\}$, where  $f(t)$ is a continuous  function.
\end{itemize}

\begin{mydef}{Event}{theoexample} 
An \textit{event} $A$ is represented by a subset of $\Omega$. After the realization of a random experiment we say that ``$A$ happens" if  $\omega \in A$.
\end{mydef}


\begin{itemize}

\item Getting a die even roll: $A = \{\mathsf{2,4,6}\}$.
\item Getting the same outcome in two coin flips: $A = \{\mathsf{HH, TT}\}$.
\item “random.rand()” gives a number larger than 0.5: $A = \mathsf{(0.5, 1]}$.
\item The temperature in my room of at time  $T$ is above $\mathsf {20}^o C$ : $A = \{T > \mathsf{20}^o C\}$.
\end{itemize}

\subsection{Sample Space Types}

Sample spaces can be classified according to their cardinality, i.e., the number of elements, $\# \Omega$, forming the set.
\begin{mydef}{Types of Sample Spaces}{theoexample}
\begin{itemize}
\item A sample space consisting of a finite or a \textit{countably infinite} number of elements is called a
\textit{discrete sample space}.
\item When the sample space includes all the numbers in an interval of the real line, it is called a
\textit{continuous sample space}. 
\end{itemize} 
\end{mydef}


The following are examples of sample spaces.



%\begin{mybox}{Sample Spaces from Health Sciences}{theoexample}  
\begin{enumerate}

\item \textit{Coin toss}: When you flip a fair coin, the sample space consists of two possible outcomes: heads (H) or tails (T).
$\Omega = \{\textsf{ H, T}\}.$

\item \textit{ Rolling a six-sided die}: When you roll a standard six-sided die, the sample space includes the numbers 1 through 6.
$\Omega = \{\textsf{ 1, 2, 3, 4, 5, 6}\}.$

\item \textit{Flipping two coins}: If you flip two coins simultaneously, the sample space includes all possible combinations of outcomes for each coin.
$\Omega = \{\textsf{ \{H,H\}, \{H,T\}, \{T,H\}, \{T,T\}}\}.$

\item \textit{Rolling two dice}: Rolling two six-sided dice results in a sample space that includes all possible combinations of the dice values.
$\Omega = \{\textsf{ \{1,1\}, \{1,2\}, \dots \{6,6\}}\}.$ (Figure 4).

\item \textit{Rolling a six-sided die and flipping a coin}: If you roll a die and flip a coin, the combined sample space includes all possible pairs of outcomes.
$$\Omega = \{\textsf{ \{1, H\}, \{1, T\}, \{2, H\}, \{2, T\}, ..., \{6, H\}, \{6, T\} }.\}$$

\item \textit{Diagnostic test}: In a diagnostic test for a disease, the sample space could consist of four outcomes $\omega_1 = \sf TP$, $\omega_2 = \sf  FP$, $\omega_3 = \sf TN$ and $\omega_4 = \sf FN$, so $\Omega = \{\textsf{TP, FP, TN, FN}\}$. Key: True Positive, \textsf{TP}; False Positive, \textsf{FP}; True Negative, \textsf{TN}; and  False Negative \textsf{FN}.

\item \textit{Medication dosage}: Imagine a situation where a doctor is deciding on the dosage of a particular medication for a patient. The sample space would consist of all possible dosages that the doctor can prescribe, ranging from the lowest possible dose to the highest.
$\Omega = \{\textsf{Low, Medium, High}\}$.

\item \textit{Hospital stay length}: When a patient is admitted to a hospital, the length of their stay can vary. The sample space for the length of stay could include various possibilities, such as short stays, average stays, and long stays.  $\Omega = \{\textsf{Short, Average, Long}\}$.

\item \textit{Patient discharge status}: After receiving treatment, a patient may be discharged under different conditions. The sample space for discharge status might include being discharged in good health, with ongoing treatment needs, or with a referral to a specialist. $\Omega = \{\textsf{Discharged, Ongoing treatment, Referred}\}$.
%\end{enumerate}
%\end{mybox}

%\begin{mybox}{More Sample Spaces from Health Sciences}{theoexample}  
%\begin{enumerate}
\item \textit{Surgical outcomes}: In the case of a surgical procedure, the sample space could include different possible outcomes, such as successful surgery, or unsuccessful surgery. $\Omega = \{\textsf{Successful, Unsuccessful}\}$.


\item \textit{Disease progression}: For patients with chronic diseases, the progression of the disease can vary. The sample space for disease progression could include different stages of the disease, such as mild, moderate, or severe. $\Omega = \{\textsf{Mild, Moderate, Severe}\}$.

\item \textit{Emergency room triage}: In an emergency room, patients are triaged based on the severity of their condition. The sample space for triage levels might include various levels of urgency, such as critical, urgent, or non-urgent. $\Omega = \{\textsf{Critical, Urgent, Non-urgent}\}$.

\item \textit{Appointment scheduling}: When scheduling patient appointments, there can be various time slots available. The sample space for appointment scheduling would consist of all the possible time slots. $$\Omega = \{\textsf{8:00 AM, 10:00 AM, 12:00 PM, 2:00 PM, 4:00 PM}\}.$$

\item \textit{Psychological testing:} In psychological assessments, the sample space may consist of all possible test scores or responses. For example, when conducting an IQ test, the sample space includes all potential scores that individuals might achieve.

\item \textit{Survey responses:} When conducting surveys or questionnaires in psychology, the sample space represents all possible responses to each question. For instance, in a survey about people's feelings, the sample space could include options like "happy," "neutral," "sad," and so on.

\item \textit{Behavioral observations:} When observing and recording behaviors in psychological studies, the sample space could encompass various behavioral categories. For example, in a study on child behavior, the sample space might include categories like "playing," "crying," "listening," and "talking."

\item \textit{Emotional responses:} In studies of emotional responses, the sample space can describe the full range of possible emotions that individuals might experience, such as "joy," "anger," "fear," "disgust," "surprise," and "sadness."

%\end{enumerate}
%\end{mybox}
%\begin{mybox}{More Sample Spaces}{theoexample}
%\begin{enumerate}
\item \textit{Blood pressure measurement}: When measuring blood pressure, both systolic and diastolic pressures are continuous variables, and their sample spaces consist of real numbers within specified ranges. For example, systolic pressure might fall within the range of $90$ mmHg to $180$ mmHg, while diastolic pressure might range from $60$ mmHg to $120$ mmHg.  Therefore 
$$\Omega = \sf  [90\,mmHg, 180\,mmHg],$$  and  $$\Omega = \sf  [60\,mmHg, 120\,mmHg],$$ for systolic and diastolic pressures, respectively.

\item \textit{Weather forecast}: A weather forecast might have different categories like sunny, cloudy, rainy, or snowy.
$\Omega = \{\textsf{ Sunny, Cloudy, Rainy, Snowy}\}.$

\item \textit{Temperature measurement}: When measuring temperature using a thermometer, the sample space includes all possible real numbers within a specified temperature range. For instance, the temperature could be any real number within a range of $-100 ^oC$ to $100 ^oC$.
$\Omega = [-100 ^oC, 100 ^oC]$.

\item \textit{Inventory management:} The sample space could represent the different levels of inventory for a product. For example, $$\Omega = \{\textsf{ High Inventory, Moderate Inventory, Low Inventory}\}.$$

\item \textit{Market Research:}
 When conducting market research, the sample space may represent customer preferences. For instance, 
  $$\Omega = \{\textsf{ Product A Preferred, Product B Preferred, No Preference}\}.$$


\item \textit{Financial Investments:}
In the context of investment decisions, the sample space might represent investment outcomes like
 $$\Omega = \{\textsf{ Profit, Break-even, Loss}\}.$$
 
\item \textit{Project Management:}
For project management, the sample space could depict project outcomes such as
 $$\Omega = \{\textsf{On Time and On Budget, Delayed but On Budget, Delayed and Over Budget}\}.$$
 
\item \textit{Product Launch Success:}
When launching a new product, the sample space may represent the possible outcomes for success, like
 $$\Omega = \{\textsf{ High Sales, Moderate Sales, Low Sales}\}.$$

\item \textit{Employee Performance Evaluation:}
In performance evaluations, the sample space might represent performance levels, such as
 $$\Omega = \{\textsf{ High Inventory, Moderate Inventory, Low Inventory}\}.$$

\item \textit{Customer Satisfaction:}
Customer satisfaction surveys often use sample spaces like 
 $$\Omega = \{\textsf{Very Satisfied, Satisfied, Neutral, Dissatisfied, Very Dissatisfied}\}.$$

\item \textit{Risk Assessment:}
In risk assessment, the sample space may include potential risks or events, like 
 $$\Omega = \{\textsf{Market Downturn, Regulatory Changes, Supplier Issues}\}.$$

\item \textit{Marketing Campaign Effectiveness:}
For assessing the effectiveness of a marketing campaign, the sample space could represent the different customer responses, such as  $$\Omega = \{\textsf{Conversion, Click-Through, No Response}\}.$$
 
\item \textit{Production Quality Control:}
In quality control, the sample space might represent the quality of products, like
 $$\Omega = \{\textsf{Defective, Passes Quality Control, Exceptional Quality}\}.$$

\item \textit{Sample Space of Continuous Functions $C([a, b]$}: The sample space of continuous functions often denoted as $C([a, b])$, represents the set of all functions f(x) defined on the closed interval [a, b] that are continuous over that interval. In mathematical notation:
$$\Omega = \{f(x) | f(x) \textsf{is continuous on $[a, b]$}\}.$$
%\end{enumerate}
%\end{mybox}
%The nature of the sample space of natural language (Box 2)  can be appreciated by understanding how deep learning architectures for Large Language Models (LLMs), such as Transformers, process text to conduct natural language processing tasks. Transformers use embeddings and positional encoding to convert text sequences into number sequences.
%\begin{mybox}{The Sample Space of Natural Language Texts}{theoexample}
%\begin{enumerate}
\item \textit{The sample space of texts:} In the context of natural language and textual data, represents the set of all possible text strings that can be generated within a specific language or character encoding. The sample space of texts is essentially the universe of all potential textual documents, messages, or sequences of characters. Here are some key points to consider regarding the sample space of texts:

\begin{itemize}

\item \textit{Character set}: The sample space of texts depends on the character set used. For example, in English, it would be the set of all combinations of English letters (both uppercase and lowercase), digits, punctuation marks, and special characters.

\item \textit{Infinite nature}: The sample space of texts is typically considered infinite, as there is no upper limit on the length of text strings that can be generated. Texts can be of varying lengths, from a single character to entire books or larger documents.

\item \textit{Variability}: The sample space encompasses a wide variety of texts, ranging from common words and phrases to unique combinations and rare or gibberish sequences of characters. It includes valid and meaningful texts as well as invalid, meaningless, or nonsensical ones.

\item \textit{Natural language}: The sample space is most relevant in the context of natural language, where it represents the potential for human communication in written form. It includes text in languages other than English, each with its own set of characters and rules.

\item \textit{Encoding and formatting}: The sample space is influenced by text encoding standards and formatting rules. For instance, plain text documents, HTML, JSON, XML, or any other text-based data format contribute to the diversity within the sample space.

\item \textit{Applications}: The sample space of texts is fundamental in natural language processing (NLP), text analysis, information retrieval, and various data science applications that involve textual data.

\item \textit{Size and complexity}: Due to the infinite nature of the sample space, it is practically impossible to exhaustively list or describe all possible text strings. The complexity increases with the size of the character set and the length of the text.

\item \textit{Machine earning}: In machine learning and NLP tasks, understanding the sample space of texts is important for tasks like text generation, text classification, and text mining. Models and algorithms are trained on data from this sample space to perform specific tasks.
\end{itemize}
The nature of the sample space of natural language can be appreciated by understanding how deep learning architectures for Large Language Models (LLMs), conduct natural language processing tasks.  Models such as transformers, often use \textit{embeddings} and \textit{positional encoding} to convert text sequences into number sequences.
\begin{itemize}
\item \textit{Word embeddings}: Transformers use word embeddings to convert input tokens (e.g., words) into continuous vector representations. These embeddings capture semantic information, allowing the model to understand the meaning of words. Word embeddings help transformers handle a large vocabulary efficiently and generalize better because they learn to represent words with similar meanings in similar vector spaces.
\item \textit{Positional encoding}: Transformers don't have a built-in notion of word order or position, which is essential for understanding sequences (e.g., sentences or documents). Positional encoding provides this information. Positional encoding is added to the word embeddings to convey the position of words in a sequence. It allows the model to distinguish between the same word in different positions and learn the sequential relationships between words. In summary, embeddings and positional encoding in transformers play a critical role in enabling these models to handle sequences of data effectively, whether it's natural language text or other sequential data. Embeddings capture the meaning of words or tokens, while positional encoding imparts information about the order of tokens in the sequence. This combination enables transformers to excel in tasks like machine translation, text generation, and more.
\end{itemize}
\end{enumerate}

\begin{mybox}{Application Example (Injection System Faliure?) }{theoexample}
 A spacecraft’s motor receives fuel from an injection system, denoted by $F$. If this injection system fails, a master alarm ($M$) is triggered on the commander’s main dashboard. One possible cause of failure in the injection system is the occurrence of extreme vibrations, represented by $V$, which the spacecraft may experience during operation. However, it is important to note that extreme vibrations can erroneously trigger the master alarm even if the injection system itself is functioning correctly.
\begin{itemize}
 \item Write down the sample space for this example.
 \item Write down the outcomes related to the events: 
 \item $\mathsf A$ = "\textsf{The injection system failed.}"
\item $\mathsf B$ = "\textsf {The injection system failed and  the master alarm is activated.}"
\item $\mathsf C$ = "\textsf{The injection system failed,  the master alarm is activated, and the spacecraft experiences extreme vibrations.}"
\item Solution.
\end{itemize} 

\begin{enumerate}
\item Sample space and outcome probabilities:
   \begin{center}
  \begin{tabular}{ |l|c|}
    \hline
     $\Omega$'s oucomes & $P(\omega)$ \\\hline
    $\omega_1 = \mathsf{V^cFA}$ &  $\alpha_1$\\\hline
    $\omega_2 =\mathsf{V^cFA^c}$    &  $\alpha_2$\\\hline
    $\omega_3 =\mathsf{V^cF^cA}$    &  $\alpha_3$\\\hline
    $\omega_4 =\mathsf{V^cF^cA^c}$        &  $\alpha_4$\\\hline
    $\omega_5 =\mathsf{VFA}$     & $\alpha_5$\\\hline
    $\omega_6 =\mathsf{VFA^c}$        & $\alpha_6$\\\hline     
    $\omega_7 =\mathsf{VF^cA}$        & $\alpha_7$\\\hline
    $\omega_8 =\mathsf{VF^cA^c}$           & $\alpha_8$\\\hline         
  \end{tabular}
  \end{center}
\item  Events:
  \vspace{0.3cm}  
  \bmp{0.5\tw}  
  \vspace{0.3cm}
  \begin{center}
  \begin{tabular}{ |l|c|}
    \hline
     $\mathsf A$'s outcomes & $P(\omega)$ \\\hline
    $\omega_{5}= \mathsf{VFA}$       & $\alpha_{5}$\\\hline
    $\omega_{6}=\mathsf{VFA^c}$       & $\alpha_{6}$\\\hline
    $\omega_{7}=\mathsf{VF^cA}$           & $\alpha_{7}$\\ \hline    
    $\omega_{8}=\mathsf{VF^cA^c}$       & $\alpha_{8}$\\\hline
  \end{tabular}    
  \end{center}
  \emp  

   \bmp{0.5\tw} 
   % \vspace{0.2cm}   
   \begin{center}
   \begin{tabular}{ |l|c|}
    \hline
    $\mathsf B$ 's outcomes & $P(\omega)$ \\\hline    
    $\omega_{5}= \mathsf{VFA}$       & $\alpha_{5}$\\\hline
    $\omega_{6}=\mathsf{VFA^c}$       & $\alpha_{6}$\\\hline
  \end{tabular}   
  \end{center}
  \emp 
      \bmp{0.5\tw} 
    \vspace{0.3cm}   
   \begin{center}
   \begin{tabular}{ |l|c|}
    \hline
    $\mathsf C$'s outcomes& $P(\omega)$ \\\hline    
    $\omega_{5}= \mathsf{VFA}$       & $\alpha_{5}$\\\hline
  \end{tabular}   
  \end{center}
  \emp

\end{enumerate}  

\end{mybox}


\begin{mybox}{Sample Space Exercise }{theoexample}
\small
\begin{itemize}
\item Mr. Holmes now lives in Los Angeles. One morning when Homes leaves his house, he realizes that his grass is wet ($H$). Is it due to rain ($\mathsf R$), or has he forgotten to turn off the sprinkler ($ S$)? Next, he notices that the grass of his neighbor, Dr. Watson, is also wet ($ W$). 
\begin{enumerate}
 \item Write down the sample space for this example.
 \item Write down the outcomes related to the events: 
\begin{itemize}
\item $\mathsf A$ = "\textsf{Holmes grass is wet.}"
\item $\mathsf B$ = "\textsf{Holmes forgot to turn the sprinkler off.}"
\item $\mathsf C$ = "\textsf{Holmes forgot to turn the sprinkler off and Watson's grass is wet.}"
\end{itemize} 
\end{enumerate}
\item Solution.
\begin{enumerate}
\item Sample space and outcome probabilities:
   \begin{center}
  \begin{tabular}{ |l|c|}
    \hline
     $\Omega$'s oucomes & $P(\omega)$ \\\hline
    $\omega_1 = \mathsf{H^cW^cS^cR^c}$ &  $\alpha_1$\\\hline
    $\omega_2 =\mathsf{H^cW^cS^cR}$    &  $\alpha_2$\\\hline
    $\omega_3 =\mathsf{H^cW^cSR^c}$    &  $\alpha_3$\\\hline
    $\omega_4 =\mathsf{H^cW^cSR}$        &  $\alpha_4$\\\hline
    $\omega_5 =\mathsf{H^cWS^cR^c}$     & $\alpha_5$\\\hline
    $\omega_6 =\mathsf{H^cWS^cR}$        & $\alpha_6$\\\hline     
    $\omega_7 =\mathsf{H^cWSR^c}$        & $\alpha_7$\\\hline
    $\omega_8 =\mathsf{H^cWSR}$           & $\alpha_8$\\\hline
    $\omega_9 =\mathsf{HW^cS^cR^c}$    & $\alpha_9$\\ \hline    
    $\omega_{10} =\mathsf{HW^cS^cR}$       & $\alpha_{10}$\\\hline
    $\omega_{11} =\mathsf{HW^cSR^c}$       & $\alpha_{11}$\\\hline
    $\omega_{12} =\mathsf{HW^cSR}$           & $\alpha_{12}$\\ \hline    
    $\omega_{13} =\mathsf{HWS^cR^c}$       & $\alpha_{13}$\\\hline
    $\omega_{14} =\mathsf{HWS^cR}$          & $\alpha_{14}$\\\hline
    $\omega_{15} =\mathsf{HWSR^c}$          & $\alpha_{15}$\\\hline     
    $\omega_{16} =\mathsf{HWSR}$             & $\alpha_{16}$\\                  
     \hline
  \end{tabular}
  \end{center}
\item  Events:
  \vspace{0.3cm}  
  \bmp{0.5\tw}  
  \vspace{0.3cm}
  \begin{center}
  \begin{tabular}{ |l|c|}
    \hline
     $\mathsf A$'s outcomes & $P(\omega)$ \\\hline
     $\omega_{9}$ = \textcolor{black}{$\mathsf{HW^cS^cR^c}$ }   & $\alpha_9$\\ \hline    
    $\omega_{10}= \mathsf{HW^cS^cR}$       & $\alpha_{10}$\\\hline
    $\omega_{11}=\mathsf{HW^cSR^c}$       & $\alpha_{11}$\\\hline
    $\omega_{12}=\mathsf{HW^cSR}$           & $\alpha_{12}$\\ \hline    
    $\omega_{13}=\mathsf{HWS^cR^c}$       & $\alpha_{13}$\\\hline
    $\omega_{14}=\mathsf{HWS^cR}$          & $\alpha_{14}$\\\hline
    $\omega_{15}=\mathsf{HWSR^c}$          & $\alpha_{15}$\\\hline     
    $\omega_{16}=\mathsf{HWSR}$             & $\alpha_{16}$\\                  
     \hline
  \end{tabular}    
  \end{center}
  \emp  

   \bmp{0.5\tw} 
   % \vspace{0.2cm}   
   \begin{center}
   \begin{tabular}{ |l|c|}
    \hline
    $\mathsf B$ 's outcomes & $P(\omega)$ \\\hline    
    $\omega_{3} = \mathsf{H^cW^cSR^c}$    &  $\alpha_3$\\\hline
    $\omega_{4} = \mathsf{H^cW^cSR}$        &  $\alpha_4$\\\hline   
    $\omega_{7} = \mathsf{H^cWSR^c}$        & $\alpha_7$\\\hline
    $\omega_{8} = \mathsf{H^cWSR}$           & $\alpha_8$\\\hline
    $\omega_{11} = \mathsf{HW^cSR^c}$       & $\alpha_{11}$\\\hline
    $\omega_{12} = \mathsf{HW^cSR}$           & $\alpha_{12}$\\ \hline   
    $\omega_{15} = \mathsf{HWSR^c}$          & $\alpha_{15}$\\\hline     
    $\omega_{16} = \mathsf{HWSR}$             & $\alpha_{16}$\\                 
     \hline
  \end{tabular}   
  \end{center}
  \emp 
      \bmp{0.5\tw} 
    \vspace{0.3cm}   
   \begin{center}
   \begin{tabular}{ |l|c|}
    \hline
    $\mathsf C$'s outcomes& $P(\omega)$ \\\hline    
     $\omega_{7}= \mathsf{H^cWSR^c}$        & $\alpha_7$\\\hline
     $\omega_{8}=\mathsf{H^cWSR}$           & $\alpha_8$\\\hline   
     $\omega_{15}=\mathsf{HWSR^c}$          & $\alpha_{15}$\\\hline     
     $\omega_{16}=\mathsf{HWSR}$             & $\alpha_{16}$\\                  
     \hline
  \end{tabular}   
  \end{center}
  \emp

\end{enumerate}  

\end{itemize}

\end{mybox}

%\end{enumerate}
%\end{mybox}

\begin{mybox}{Application Example (Was it the burglar?) }{theoexample}
 Mary lives in San Francisco City. One afternoon, she is driving back home and receives a phone call from 
 her neighbor Jane. She told her that her house alarm was set off ($A$). While driving, she also heard on the radio
 ($ R$) that a small earthquake ($ E$) hit the city.  Small earthquakes sometimes activate the alarm, and perhaps
 this is the reason why the alarm was sounding.
\begin{itemize}
 \item Write down the sample space for this example.
 \item Write down the outcomes related to the events: 
\item $\mathsf A$ = "\textsf{A burglar ($B$) broke into the house.}"
\item $\mathsf  B$ = "\textsf{A burglar broke into the house and the alarm was set off.}"
\item $\mathsf  C$ = "\textsf{A burglar broke into the house and the alarm was set off and a small earthquake hit the city.}"
\end{itemize} 
\end{mybox}




\subsection{$\sigma$-algebras}

To gain some intuition of what a $\sigma$-algebra is, imagine you have a collection of things, like a number of different events or sets of outcomes in probability. A sigma algebra is like a special container or group that holds these things together in an organized way (Figure 19). 

\begin{mydef}{$\sigma$-algebra }{theoexample}  
A $\sigma$-algebra  $\mathscr F$ on a set $\Omega$ is a family of subsets of $\Omega$ such that:
\begin{itemize}
\item $\Omega \in \mathscr F$ 
\item $A \in \mathscr F \Longrightarrow A^c \in \mathscr F$ 
\item $(A_n)_{n \in \mathbb N} \subset \mathscr F \Longrightarrow  \bigcup A_n \in \mathscr F$
\end{itemize}
The set  $A \in \mathscr F$  is said to be \textit{measurable} or $\mathscr F$-\textit{measurable}.
\end{mydef}


\begin{marginfigure}
\centering
\includegraphics[width=4cm]{fig/es.png}
\caption{The event space is a $\sigma$-algebra on the sample space $\Omega$. It is a subset of the power set,
$\mathcal P(\Omega)$,  
 whose elements, called events, satisfy some regularity conditions. When 
 is finite and discrete, the power set is a valid $\sigma$-algebra that can be used as the event space.
The  $\sigma$-algebra satisfies $\Omega 	\in  \mathcal F$; the sample space is an event called the \textit{sure} event.
$A\mathcal \in  F \Longrightarrow A^c \in \mathcal F$, i.e., $\mathcal F$ is \textit{closed} under complementation.
 $A_1, A_2, \dots$, $ \bigcup A_n \in \mathscr F$, so  $F$ is closed under \textit{countable unions}.
 Source: http://bit.ly/3s11bq5.  
}
\end{marginfigure}


To be a $\sigma$-algebra, this container has to follow a few rules:

\begin{enumerate}

\item It must always contain the "whole thing." In other words, it includes everything you're interested in. For example, if you're thinking about all possible outcomes when rolling a die, the $\sigma$-algebra would definitely include "rolling a 1," "rolling a 2," and so on, all the way up to "rolling a 6."

\item It should also include the "opposites" of things. Let's say you toss a coin and have "getting a head" as one of your events. The sigma $\sigma$-algebra should also have "not getting a head" (which means getting a tail) inside it.

\item When you look inside the $\sigma$-algebra, you should find all the combinations and possibilities of the things you're interested in. So, if you have "rolling an even number" and "rolling a prime number," you want the sigma-algebra to include things like "rolling an even number AND rolling a prime number."
\end{enumerate}


In summary,  a sigma-algebra is like a special container that holds all the possible outcomes and their opposites in an  organized way, helping you study and understand different events and probabilities in a systematic manner. The following are examples of $\sigma$-algebras.

\begin{enumerate} 
\item Let X be a set.
\item $\{\emptyset,X\}$ is a $\sigma$-algebra (the minimal $\sigma$-algebra in $X$).
\item $\mathscr P(X)$, the power set of $X$  (the maximal $\sigma$-algebra in $X$).
\item $\{\emptyset,X, B, B^c\}$, $B\subset X$ is a $\sigma$-algebra.
\item $X=\{\mathsf{HH,HT,TH,TT}\} = \{\omega_1, \omega_2, \omega_3, \omega_4\}$.
\begin{align*}
\mathscr A_{\min} &= \{\emptyset,{\{\omega_1,\omega_2,\omega_3,\omega_4\}\}}.\\
\mathscr A_{\max} &= \{\emptyset,\{\omega_1\}, \{ \omega_2\},  \{\omega_3\}, \{\omega_4\}, \{\omega_1,\omega_2\},\{\omega_1,\omega_3\}. \{\omega_1,\omega_4\},\\
			     &\;\;\;\;\;\;  {\{\omega_2,\omega_3\},\{\omega_2,\omega_4\}, \{\omega_3,\omega_4\}}, {\{\omega_1,\omega_2,\omega_3\}}, \\
                              &\;\;\;\;\;\;   {\{\omega_1,\omega_2,\omega_4\}}, {\{\omega_1,\omega_3,\omega_4\}}, {\{ \omega_2,\omega_3,\omega_4\}},  X\}.\\
\mathscr A_1 &= \{\emptyset, \{\omega_1,\omega_2\}, \{\omega_1,\omega_2\}^c, X\}, \{\omega_1,\omega_2\}^c = \{\omega_3,\omega_4\}. \\
\end{align*}
\item Is $\omega_1\neq \{\omega_1\}$?.

\end{enumerate}


%\begin{mybox}{ $\sigma$-algebra Examples }{theoexample} 

\subsection{Exercises}
\begin{enumerate} 
\item Construct a $\sigma$-algebra for the set $\Omega = \{\epsdice{1},\epsdice{2}\}.$
$$\mathscr F = \{\Omega, \emptyset,  \{\epsdice{1}\},\{\epsdice{2}\}\}$$
\item Consider the set $\Omega = \{\epsdice{1},\epsdice{2},\epsdice{3}\}.$ 
\begin{itemize} 
\item Construct the \textit{minimal } $\sigma$-algebra for the set $\Omega$.
$$\mathscr F = \{\Omega, \emptyset\}.$$
\item Construct the \textit{maximal}  $\sigma$-algebra for the set $\Omega$ (i.e., $\Omega$'s power set, $\mathscr P(\Omega)$).
$$\mathscr F = \{ \emptyset,  \{\epsdice{1}\} ,\{\epsdice{2}\}, \{\epsdice{3}\}, \{\epsdice{1},\epsdice{2}\}, \{\epsdice{1},\epsdice{3}\}, \{\epsdice{2},\epsdice{3}\}, \Omega\}.$$
\item Construct a  $\sigma$-algebra for the set $\Omega$ that is neither maximal or minimal.
 $$\mathscr F = \{\Omega, \emptyset, B, B^c \}, B\subset \Omega$$
 \item \textit{Remark:} no set is an element of itself. For example,  $$\epsdice{1} \neq \{\epsdice{1}\}. $$
\end{itemize}

\item \textit{Emotional responses:} In studies of emotional responses, the sample space can describe the full range of possible emotions that individuals might experience, such as "joy," "anger," "fear," "disgust," "surprise," and "sadness." Write down the sample space $\Omega$  associated with the given outcomes. Let $$\mathscr F = \{\Omega, \emptyset, B, B^c \},$$ where $B=\{\textsf{joy}\}$. 
 Is $\mathscr F$ is a $\sigma$-algebra?  

\item \textit{Surgical outcomes}: In the case of a surgical procedure, the sample space could include different possible outcomes, such as successful surgery, complications, or the need for additional surgeries.
$$\Omega= \{\textsf{Successful surgery, Complications, Additional surgery}\}$$
Let $\mathscr F = \{\Omega, \emptyset, A, A^c \}$ where $A= \{\textsf{Successful surgery}\}$.  Show that $\mathscr F$ is a $\sigma$-algebra?
\item \textit{Product sales:} When launching a new product or analyzing sales data, the sample space represents all possible sales outcomes, including the range of sales volumes and revenue generated.
$$\Omega = \{\textsf{Low sales, Moderate sales, High sales}\}$$
Construct a $\sigma$-algebra for  $\Omega$.

 \item \textit{Emergency room triage}: In an emergency room, patients are triaged based on the severity of their condition. The sample space for triage levels might include various levels of urgency, such as critical, urgent, or non-urgent.
$$\Omega = \{\textsf{Critical, Urgent, Non-urgent}\}.$$
 What is the maximal $\sigma$-algebra for $\Omega$?
\end{enumerate}
%\end{mybox}

\begin{mybox}{Intuitions behind a $\sigma$-algebra }{theoexample}  
\begin{itemize}
\item Represents the information that will be revealed to us after the realization of a random outcome.
\item Contains all events that can be verified if they happened or not after $\omega$ has been realized.
\end{itemize}
\end{mybox}

\begin{itemize}  
\item  Roll a die: $\Omega= \mathsf{\{1,2,3,4,5,6\}}$.
\begin{align*}
\mathscr F_1 &= \{\emptyset, \Omega, \mathsf{\{1,2,3\}, \{4,5,6\}\}}.\\
\mathscr F_{\max} &= \mathscr P(\Omega) = \textsf{the set of all subsets of $\Omega$}.
\end{align*}
are both $\sigma$-algebras. $\mathscr F_1$ contains information on whether the roll is strictly less than four or not, and $\mathscr F_2$ contains information on the exact outcome. (Why?)
\item $\Omega=\{\mathsf{HH,HT,TH,TT}\}$.
\begin{align*}
\mathscr F_1 &= \{\emptyset, \Omega, \mathsf{\{HH,HT\}, \{TH,TT\}\}}.\\
\end{align*}
\item  $\mathscr F_1$ is a $\sigma$-algebra containing information on the outcome of the first flip. (Why?)
\end{itemize}



\subsection{ $\sigma$-algebra Generators$^*$ }

Given a set of events or outcomes, a $\sigma$-algebra generator refers to the smallest $\sigma$-algebra that contains these events. In other words, it's the collection of all possible events that can be formed by combining the original set of events through set operations like unions, intersections, and complements.

\begin{mydef}{$\sigma$-algebra Generators$^*$ }{theoexample}  
$\sigma$-algebra generators are defined as follows:
\begin{itemize}
\item For every system of sets $\mathscr G \in \mathscr P(\Omega)$ there exist a smallest $\sigma$-algebra containing $\mathscr G$. This is the $\sigma$-algebra generated by $\mathscr G$, denoted by $\sigma(\mathscr G)$, and $\mathscr G$ is called the generator.
\item Notice that the intersection  $\bigcap_{i \in I} \mathscr A_i$ of arbitrarily many $\sigma$-algebras in $\Omega$ is again a $\sigma$-algebra in $\Omega$.
\item Except in a few simple examples, \underline{it is hard} to write down explicitly a generated $\sigma$-algebra.
\end{itemize}
\end{mydef}
Box 3 shows an example of a $\sigma$-algebra generator and the corresponding $\sigma$-algebra.
\begin{mybox}{ $\sigma$-algebra Generators Example$^*$ }{theoexample} 
\begin{itemize}
\item  Let $A,B \in \Omega$, $A\bigcap B = \emptyset$. Define $\mathscr G = \{A,B\}$.
\begin{equation*}
\sigma(\mathscr G) = \left\{\emptyset, A,B,A\cup B, A^c,B^c, (A\cup B)^c,\Omega\right\}.
\end{equation*}
Prove this statement.
\end{itemize}
\end{mybox}

In an experiment of rolling a die, suppose we are interested in knowing whether the outcome belongs to a low-range $\mathsf (1, 2)$, mid-range $\mathsf{(3, 4)}$, or high-range $\mathsf{(5, 6)}$. What is the minimal information required?  In this example, the required minimal information is ``encoded''  by the $\sigma$-algebra generated by $\mathscr G =\{\mathsf{\{1,2\},\{3,4\},\{5,6\}}\}:$
$$\sigma(\mathscr G) = \{\emptyset, \mathsf{\{1,2\}, \{3,4\}, \{5,6 \}, \{3,4,5,6\},\{1,2,5,6\},\{1,2,3,4\}}, \Omega\}.$$

If we are interested in the exact outcome of the die, then take $\mathscr G = \{\{1\},\{2\},\{3\},\{4\},\{5\},\{6\}\}$ and $\sigma(\mathscr G) = \mathscr P(\Omega)$.

\begin{mybox}{Application Example (Injection System Faliure?) }{theoexample}
 A spacecraft’s motor receives fuel from an injection system, denoted by $F$. If this injection system fails, a master alarm ($M$) is triggered on the commander’s main dashboard. One possible cause of failure in the injection system is the occurrence of extreme vibrations, represented by $V$, which the spacecraft may experience during operation. However, it is important to note that extreme vibrations can erroneously trigger the master alarm even if the injection system itself is functioning correctly.
\begin{itemize}
 \item Write down the sample space for this example.
 \item Write down the sigma algebra generated by  event  $A$  and provide an intuitive interpretation.
 \item $\mathsf A$ = "\textsf{The injection system failed.}"
\item Solution.
\end{itemize} 

\begin{enumerate}
\item Sample space and outcome probabilities:
   \begin{center}
  \begin{tabular}{ |l|c|}
    \hline
     $\Omega$'s oucomes & $P(\omega)$ \\\hline
    $\omega_1 = \mathsf{V^cFA}$ &  $\alpha_1$\\\hline
    $\omega_2 =\mathsf{V^cFA^c}$    &  $\alpha_2$\\\hline
    $\omega_3 =\mathsf{V^cF^cA}$    &  $\alpha_3$\\\hline
    $\omega_4 =\mathsf{V^cF^cA^c}$        &  $\alpha_4$\\\hline
    $\omega_5 =\mathsf{VFA}$     & $\alpha_5$\\\hline
    $\omega_6 =\mathsf{VFA^c}$        & $\alpha_6$\\\hline     
    $\omega_7 =\mathsf{VF^cA}$        & $\alpha_7$\\\hline
    $\omega_8 =\mathsf{VF^cA^c}$           & $\alpha_8$\\\hline         
  \end{tabular}
  \end{center}
\item  Events:
  \vspace{0.3cm}  
  \bmp{0.5\tw}  
  \vspace{0.3cm}
  \begin{center}
  \begin{tabular}{ |l|c|}
    \hline
     $\mathsf A$'s outcomes & $P(\omega)$ \\\hline
    $\omega_{5}= \mathsf{VFA}$       & $\alpha_{5}$\\\hline
    $\omega_{6}=\mathsf{VFA^c}$       & $\alpha_{6}$\\\hline
    $\omega_{7}=\mathsf{VF^cA}$           & $\alpha_{7}$\\ \hline    
    $\omega_{8}=\mathsf{VF^cA^c}$       & $\alpha_{8}$\\\hline
  \end{tabular}    
  \end{center}
  \emp  
\end{enumerate}  

\end{mybox}

\begin{mybox}{Sample Space Exercise }{theoexample}
\small
\begin{itemize}
\item Mr. Holmes now lives in Los Angeles. One morning when Homes leaves his house, he realizes that his grass is wet ($H$). Is it due to rain ($\mathsf R$), or has he forgotten to turn off the sprinkler ($ S$)? Next, he notices that the grass of his neighbor, Dr. Watson, is also wet ($ W$). 
\begin{enumerate}
 \item Write down the sample space for this example.
 \item Write down the sigma algebra generated by  event  $C$  and provide an intuitive interpretation.
\begin{itemize}
\item $\mathsf C$ = "\textsf{Holmes forgot to turn the sprinkler off and Watson's grass is wet.}"
\end{itemize} 
\end{enumerate}
\item Solution.
\begin{enumerate}
\item Sample space and outcome probabilities:
   \begin{center}
  \begin{tabular}{ |l|c|}
    \hline
     $\Omega$'s oucomes & $P(\omega)$ \\\hline
    $\omega_1 = \mathsf{H^cW^cS^cR^c}$ &  $\alpha_1$\\\hline
    $\omega_2 =\mathsf{H^cW^cS^cR}$    &  $\alpha_2$\\\hline
    $\omega_3 =\mathsf{H^cW^cSR^c}$    &  $\alpha_3$\\\hline
    $\omega_4 =\mathsf{H^cW^cSR}$        &  $\alpha_4$\\\hline
    $\omega_5 =\mathsf{H^cWS^cR^c}$     & $\alpha_5$\\\hline
    $\omega_6 =\mathsf{H^cWS^cR}$        & $\alpha_6$\\\hline     
    $\omega_7 =\mathsf{H^cWSR^c}$        & $\alpha_7$\\\hline
    $\omega_8 =\mathsf{H^cWSR}$           & $\alpha_8$\\\hline
    $\omega_9 =\mathsf{HW^cS^cR^c}$    & $\alpha_9$\\ \hline    
    $\omega_{10} =\mathsf{HW^cS^cR}$       & $\alpha_{10}$\\\hline
    $\omega_{11} =\mathsf{HW^cSR^c}$       & $\alpha_{11}$\\\hline
    $\omega_{12} =\mathsf{HW^cSR}$           & $\alpha_{12}$\\ \hline    
    $\omega_{13} =\mathsf{HWS^cR^c}$       & $\alpha_{13}$\\\hline
    $\omega_{14} =\mathsf{HWS^cR}$          & $\alpha_{14}$\\\hline
    $\omega_{15} =\mathsf{HWSR^c}$          & $\alpha_{15}$\\\hline     
    $\omega_{16} =\mathsf{HWSR}$             & $\alpha_{16}$\\                  
     \hline
  \end{tabular}
  \end{center}
\item  Event:
  \vspace{0.3cm}  
      \bmp{0.5\tw} 
    \vspace{0.3cm}   
   \begin{center}
   \begin{tabular}{ |l|c|}
    \hline
    $\mathsf C$'s outcomes& $P(\omega)$ \\\hline    
     $\omega_{7}= \mathsf{H^cWSR^c}$        & $\alpha_7$\\\hline
     $\omega_{8}=\mathsf{H^cWSR}$           & $\alpha_8$\\\hline   
     $\omega_{15}=\mathsf{HWSR^c}$          & $\alpha_{15}$\\\hline     
     $\omega_{16}=\mathsf{HWSR}$             & $\alpha_{16}$\\                  
     \hline
  \end{tabular}   
  \end{center}
  \emp

\end{enumerate}  

\end{itemize}

\end{mybox}




\section*{Constructing a $\sigma$-Algebra from Overlapping Generator Sets}

Let the sample space be all 3-bit binary strings:


\[
\Omega = \{000, 001, 010, 011, 100, 101, 110, 111\}
\]



Define the generator $\mathcal{G} = \{G_1, G_2, G_3\}$ as follows:
\begin{itemize}
    \item $G_1 = \{x \in \Omega : \text{first bit is } 1\}$
    \item $G_2 = \{x \in \Omega : \text{number of 1s is even}\}$
    \item $G_3 = \{x \in \Omega : \text{last bit is } 1\}$
\end{itemize}

The membership table is:
\[
\begin{array}{|c|c|c|c|c|}
\hline
x & x \in G_1 & x \in G_2 & x \in G_3 & \text{Membership Vector} \\
\hline
000 & 0 & 1 & 0 & (0,1,0) \\
001 & 0 & 0 & 1 & (0,0,1) \\
010 & 0 & 0 & 0 & (0,0,0) \\
011 & 0 & 1 & 1 & (0,1,1) \\
100 & 1 & 0 & 0 & (1,0,0) \\
101 & 1 & 1 & 1 & (1,1,1) \\
110 & 1 & 1 & 0 & (1,1,0) \\
111 & 1 & 0 & 1 & (1,0,1) \\
\hline
\end{array}
\]




Each unique membership vector defines an atom:
\begin{align*}
A_{(0,1,0)} &= \{000\} \\
A_{(0,0,1)} &= \{001\} \\
A_{(0,0,0)} &= \{010\} \\
A_{(0,1,1)} &= \{011\} \\
A_{(1,0,0)} &= \{100\} \\
A_{(1,1,1)} &= \{101\} \\
A_{(1,1,0)} &= \{110\} \\
A_{(1,0,1)} &= \{111\}
\end{align*}

These atoms form a partition of $\Omega$, and the $\sigma$-algebra generated by $\mathcal{G}$ is:


\[
\sigma(\mathcal{G}) = \left\{ \bigcup_{i \in I} A_i : I \subseteq \{1,\dots,8\} \right\}
\]








\subsection*{Excercise}
%\begin{mybox}{Sample Space Exercise }{theoexample}
\small
Let the sample space be the set of all 4-bit binary strings:
$
\Omega = \{0000, 0001, 0010, \dots, 1111\}
$. That is, 
$$\Omega = \{x \in \{0,1\}^4\},$$ with $|\Omega| = 16$.
Define the generator:
\[
\mathcal{G} = \left\{ A_1, A_2 \right\}, \quad \text{where}
\]
\begin{itemize}
    \item $A_1 = \{x \in \Omega : x \text{ has even parity} \}$
    \item $A_2 = \{x \in \Omega : x \text{ starts with } 1 \}$
\end{itemize}

\begin{itemize} 
\item Construct the the smallest $\sigma$-algebra on $\Omega$ that contains both $A_1$ and $A_2$.
\item Explicitly list the elements of $A_1$ and $A_2$.
\item Determine all atoms (minimal non-empty elements) of $\mathcal{F}$.
\item List all subsets in $\mathcal{F}$.
\item Compare $\mathcal{F}$ with the power set $\mathcal{P}(\Omega)$.
\item Discuss how this $\sigma$-algebra could be used in a probabilistic model.
\end{itemize}

\textbf{Solution:}
\vspace{1cm}

Let the sample space be:
\[
\Omega = \{0000, 0001, 0010, 0011, 0100, 0101, 0110, 0111, 1000, 1001, 1010, 1011, 1100, 1101, 1110, 1111\}
\]
Define the generator sets:
\begin{itemize}
    \item $A_1 = \{x \in \Omega : x \text{ has even parity}\}$  
    (i.e., an even number of 1s)
    \item $A_2 = \{x \in \Omega : x \text{ starts with } 1\}$


\item {Membership table:}
\[
\begin{array}{|c|c|c|c|}
\hline
x & x \in A_1 & x \in A_2 & \text{Membership Vector} \\
\hline
0000 & 1 & 0 & (1,0) \\
0001 & 0 & 0 & (0,0) \\
0010 & 0 & 0 & (0,0) \\
0011 & 1 & 0 & (1,0) \\
0100 & 0 & 0 & (0,0) \\
0101 & 1 & 0 & (1,0) \\
0110 & 1 & 0 & (1,0) \\
0111 & 0 & 0 & (0,0) \\
1000 & 0 & 1 & (0,1) \\
1001 & 1 & 1 & (1,1) \\
1010 & 1 & 1 & (1,1) \\
1011 & 0 & 1 & (0,1) \\
1100 & 1 & 1 & (1,1) \\
1101 & 0 & 1 & (0,1) \\
1110 & 0 & 1 & (0,1) \\
1111 & 1 & 1 & (1,1) \\
\hline
\end{array}
\]

\end{itemize}

We group elements by membership vector:
\begin{align*}
A_{(1,0)} &= \{0000, 0011, 0101, 0110\} \\
A_{(0,0)} &= \{0001, 0010, 0100, 0111\} \\
A_{(0,1)} &= \{1000, 1011, 1101, 1110\} \\
A_{(1,1)} &= \{1001, 1010, 1100, 1111\}
\end{align*}

These atoms form a partition of $\Omega$, and the $\sigma$-algebra generated by $\mathcal{G} = \{A_1, A_2\}$ is:


\[
\sigma(\mathcal{G}) = \left\{ \bigcup_{i \in I} A_i : I \subseteq \{1,2,3,4\} \right\}
\]




The power set $\mathcal{P}(\Omega)$ contains $2^{16} = 65,\!536$ subsets. Our $\sigma$-algebra contains only 16 subsets.

Probabilistic interpretation
This $\sigma$-algebra partitions $\Omega$ based on two binary features:
\begin{itemize}
    \item Parity (even vs. odd)
    \item First bit (0 vs. 1)
\end{itemize}
This structure is useful in models where outcomes are grouped by these features, such as:
\begin{itemize}
    \item Binary classification tasks
    \item Feature-based probability assignments
    \item Simplified Markov models or decision trees
\end{itemize}

Now, define the generator $\mathcal{G} = \{G_1, G_2\}$:
\begin{itemize}
    \item $G_1 = \{x \in \Omega : \text{first bit is } 1\}$
    \item $G_2 = \{x \in \Omega : \text{number of 1s is even}\}$
\end{itemize}

The membership table is:
\[
\begin{array}{|c|c|c|c|}
\hline
x & x \in G_1 & x \in G_2 & \text{Membership Vector} \\
\hline
000 & 0 & 1 & (0,1) \\
001 & 0 & 0 & (0,0) \\
010 & 0 & 0 & (0,0) \\
011 & 0 & 1 & (0,1) \\
100 & 1 & 0 & (1,0) \\
101 & 1 & 1 & (1,1) \\
110 & 1 & 1 & (1,1) \\
111 & 1 & 0 & (1,0) \\
\hline
\end{array}
\]

Group elements by membership vector to get the atoms $\sigma(\mathcal{G})$:
\begin{align*}
A_{(0,0)} &= \{001, 010\} \\
A_{(0,1)} &= \{000, 011\} \\
A_{(1,0)} &= \{100, 111\} \\
A_{(1,1)} &= \{101, 110\}
\end{align*}

These atoms form a partition of $\Omega$, and the $\sigma$-algebra generated by $\mathcal{G}$ is:


\[
\sigma(\mathcal{G}) = \left\{ \bigcup_{i \in I} A_i : I \subseteq \{1,2,3,4\} \right\}
\]





\begin{mydef}{$Filtration$ }{theoexample} 
Let   $\mathscr F$  be a $\sigma$-algebra. An  increasing sequence 
\begin{equation*}
 \mathscr F_0 \subset  \mathscr F_1  \subset \cdots \subset  \mathscr F_n \subset \cdots \mathscr  \subset \mathscr F 
\end{equation*}
is a filtration of sub-$\sigma$-algebras of $\mathscr F$. The sequence is indexed by non-negative natural numbers. Other indexing sets can be used for defining a filtration (v.gr. $\mathbb R$). 
\end{mydef}

A filtration essentially is a mathematical model that represents partial knowledge about an outcome.   The filtrations tell us whether an event happened or not. One may envision the ‘filtration process’ as a sequence of filters, each filter providing us a more detailed view of the events in $\Omega$.

\subsection{Example  1 of a Probability Filtration}


As a second example,  we introduce a  problem concerning the price of given financial stock $S_n$ at time $n$.
The initial stock price is defined by $S_0$ and we have a time horizon composed of three discrete time steps ($n = 1,2,3$).  

\begin{figure}
\centering
\includegraphics[width=8cm]{fig/stock.pdf}
\caption{Stock prices.
}
\end{figure}




\begin{itemize}  
\item Possible outcomes at time $n=0$  
\begin{align*}
X &= \{uuu, uud, udu, udd, duu,dud, ddu, ddd\},               
\end{align*}
\item The corresponding sigma-algebra generated by these outcomes are:
\begin{align*}
\mathscr F_0  &= \mathscr G(X) = \{\emptyset, X\},              
\end{align*}
At $n = 0$, all paths are possible. Thus, the event set $A = \{uuu, uud, udu, udd, ddd, ddu, dud, duu\}$ – with the sequences describing the movement per time step – contains all possible
  paths; that is  $X=A$.

\item Possible outcomes at time $n=1$  
\begin{align*}
\{A_{u\cdot\cdot},A_{d\cdot\cdot}\} =  \{\{uuu, uud, udu, udd\}, \{duu,dud, ddu, ddd\}\},               
\end{align*}
The corresponding sigma-algebra generated by these outcomes are:
\begin{align*}
\mathscr F_1 &= \mathscr G(\{A_{u\cdot\cdot}, A_{d\cdot\cdot}\}) =   \{\emptyset, A_{u\cdot\cdot}, A_{d\cdot\cdot},A_{u\cdot\cdot} \cup A_{d\cdot\cdot}\} = \{\emptyset, A_{u\cdot\cdot}, A_{d\cdot\cdot},\Omega\}\\             
&= \{\emptyset, A_{u\cdot\cdot}, A_{u\cdot\cdot}^c,X\}
\end{align*}
 \item At $n=1$, we know that the stock price went either up or down. The corresponding events can be defined by $A_{u\cdot\cdot} = \{uuu, uud, udu, udd\}$ and $A_{d\cdot\cdot} = \{ddd, ddu, dud, duu\}$.
  \item  Notice: if the price went up, we know our sample-path will be in  $A_{u\cdot\cdot}$ and not in $A_{d\cdot\cdot}$.
  \item At $n = 2$, we have four event sets: $A_{uu\cdot} = \{uuu,uud\}$, $A_{ud\cdot} = \{udu,udd\}$, $A_{du\cdot} = \{duu,dud\}$, $A_{dd\cdot} = \{ddu,ddd\}$ . Observe that the information is getting increasingly fine-grained.
  \item At $n= 3$, we obviously know the exact price path that has been followed.
\end{itemize}

\begin{mybox}{$Filtration Example$ }{theoexample} 
\begin{itemize}
\item The filtration is as follows:
$\mathscr F_0  = \mathscr G(X) = \{\emptyset, X\}$,              
\item Possible outcomes at time $n=1$  
\begin{align*}
\{A_{u\cdot\cdot},A_{d\cdot\cdot}\} =  \{\{uuu, uud, udu, udd\}, \{duu,dud, ddu, ddd\}\},               
\end{align*}
\item The corresponding sigma-algebra generated by these outcomes are:
\begin{align*}
\mathscr F_1 &= \mathscr G(\{A_{u\cdot\cdot}, A_{d\cdot\cdot}\})\\ &=   \{\emptyset, A_{u\cdot\cdot}, A_{d\cdot\cdot},A_{u\cdot\cdot} \cup A_{d\cdot\cdot}\} \\ &= \{\emptyset, A_{u\cdot\cdot}, A_{d\cdot\cdot},\Omega\}\\             
&= \{\emptyset, A_{u\cdot\cdot}, A_{u\cdot\cdot}^c,X\}
\end{align*}
\item Possible outcomes at time $n=2$  
\begin{align*}
\{A_{uu\cdot},A_{ud\cdot}, A_{du\cdot}, A_{dd\cdot}\} &=  \{\{uuu, uud\}, \{udu, udd\}, \\
                                                                                      & \,\,\,\,\,\,\,\,\,\,\,\, \{duu,dud\}, \{ddu, ddd\}\}.               
\end{align*}
\item The corresponding sigma-algebra generated by these outcomes are:
\begin{align*}
\mathscr F_2 &= \mathscr G(\{A_{uu\cdot},A_{ud\cdot}, A_{du\cdot}, A_{dd\cdot}\}) \\ 
												&=\{\emptyset, A_{u\cdot\cdot}, A_{d\cdot\cdot}, A_{uu\cdot}, A_{ud\cdot},A_{du\cdot}, A_{dd\cdot}, A_{uu\cdot}^c, A_{ud\cdot}^c,A_{du\cdot}^c, A_{dd}^c,\\
                      										&A_{uu\cdot}\cup A_{du\cdot}, A_{uu\cdot}\cup A_{dd\cdot}, A_{ud\cdot}\cup A_{du\cdot}, A_{ud\cdot}\cup A_{dd\cdot},X\}.            
\end{align*}

\item Let's compute $\mathscr G(\{A_{uu\cdot},A_{ud\cdot}, A_{du\cdot}, A_{dd\cdot}\})  $.  
\bmp{0.5\tw}  
\begin{center}   
\begin{align*}
A_{uu\cdot} &= \{uuu,uud\},\\
A_{ud\cdot} &= \{udu,udd\},\\
A_{du\cdot} &= \{duu,dud\},\\
A_{dd\cdot} &= \{ddu,ddd\},\\ 
A_{uu\cdot}^c &= A_{ud\cdot} \cup A_{du\cdot} \cup A_{dd\cdot}, \\
A_{ud\cdot}^c &= A_{uu\cdot} \cup A_{du\cdot} \cup A_{dd\cdot},\\
A_{du\cdot}^c &= A_{uu\cdot} \cup A_{ud\cdot} \cup A_{dd\cdot},\\
A_{dd\cdot}^v &= A_{uu\cdot} \cup A_{ud\cdot} \cup A_{du\cdot}, 
\end{align*}
\end{center}
\emp
\bmp{0.5\tw}  
\begin{center}   
\begin{align*}
A_{u\cdot\cdot} &=A_{uu\cdot} \cup A_{ud\cdot},\\
A_{\cdot u\cdot} &=A_{uu\cdot} \cup A_{du\cdot},\\
&A_{uu\cdot} \cup A_{dd\cdot},\\
&A_{ud\cdot} \cup A_{du\cdot},\\
A_{\cdot d\cdot} &=A_{ud\cdot} \cup A_{dd\cdot},\\
A_{d\cdot\cdot} &= A_{du\cdot} \cup A_{dd\cdot},\\
\Omega &= A_{u\cdot\cdot} \cup A_{d\cdot\cdot}.
\end{align*}
\end{center}
\emp

\item  In summary: 
\begin{align*}
\mathscr F_0 &= \{\emptyset, X\},\\
\mathscr F_1 &= \{\emptyset, A_{u\cdot\cdot}, A_{u\cdot\cdot}^c,X\},\\
\mathscr F_2 &= \mathscr G(\{A_{uu\cdot},A_{ud\cdot}, A_{du\cdot}, A_{dd\cdot}\}) \\ 
								&=\{\emptyset, A_{u\cdot\cdot}, A_{d\cdot\cdot}, A_{uu\cdot}, A_{ud\cdot},A_{du\cdot}, A_{dd\cdot}, A_{uu\cdot}^c, A_{ud\cdot}^c,A_{du\cdot}^c, A_{dd}^c,\\
                      						&A_{uu\cdot}\cup A_{du\cdot}, A_{uu\cdot}\cup A_{dd\cdot}, A_{ud\cdot}\cup A_{du\cdot}, A_{ud\cdot}\cup A_{dd\cdot},\Omega \}.\\           
\mathscr F_3 &= \mathscr G(X) = \mathscr P(X)              
\end{align*}


$$\boxed{\mathscr F_0   \subseteq \mathscr F_1   \subseteq \mathscr F_2  \subseteq   \mathscr F_3}$$ 





\end{itemize}
\end{mybox}

\subsection{Example  1 of a Probability Filtration}

Consider a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ and a discrete-time stochastic process $(X_t)_{t=0}^3$ representing three coin tosses, where:


\[
X_t = 
\begin{cases}
1 & \text{if the } t\text{-th toss is heads} \\
0 & \text{if the } t\text{-th toss is tails}
\end{cases}
\]



Define a filtration $(\mathcal{F}_t)_{t=0}^3$ such that $\mathcal{F}_t$ represents the information available up to time $t$:
\begin{itemize}
    \item $\mathcal{F}_0 = \{\emptyset, \Omega\}$ (no information yet)
    \item $\mathcal{F}_1 = \sigma(X_1)$ (information from the first toss)
    \item $\mathcal{F}_2 = \sigma(X_1, X_2)$ (information from the first two tosses)
    \item $\mathcal{F}_3 = \sigma(X_1, X_2, X_3)$ (full information from all tosses)
\end{itemize}

This filtration satisfies:


\[
\mathcal{F}_0 \subseteq \mathcal{F}_1 \subseteq \mathcal{F}_2 \subseteq \mathcal{F}_3 \subseteq \mathcal{F}
\]



Each $\mathcal{F}_t$ is a $\sigma$-algebra that grows with time, modeling the accumulation of information. Provide the details
of this example.




\end{document}



\subsection{Borel Algebra}

Informally speaking, you can think of a Borel algebra as a special collection of sets that helps you organize everyday numbers in a neat and organized way.

Imagine you have all the real numbers, like 1, 2, 3, and so on, and also the numbers with decimals like 1.5, 2.75, and so forth. These numbers can be really messy, and it's challenging to sort them neatly. But with a Borel algebra, you create a system where you can group these numbers together in a smart way.

Here's how it works: you start with simple sets, like all the numbers that are less than 3, or all the numbers between 1.5 and 2.5. Then, you can combine these sets in various ways to create more sets. For example, you can combine the set of numbers less than 3 with the set of numbers between 1.5 and 2.5 to create a new set: all the numbers between 1.5 and 3.

So, a Borel algebra is like a toolbox that helps you neatly organize numbers into sets that make sense, making it easier to study and work with these numbers in mathematics and statistics. It's a bit like putting numbers into labeled boxes to keep them tidy and manageable.

%\begin{mydef}{ Borel Algebra in $\mathbb R$* }{theoexample} 

Consider the collection of all open intervals $(a, b)$ of $\mathbb R$, where $a < b$. The minimum $\sigma$-algebra generated by this collection is called the $\sigma$-algebra  of $\mathbb R$ and is denoted by $\mathcal B(\mathbb R)$,
A $\sigma$-algebra  $\mathscr F$ on a set $\Omega$ is a family of subsets of $\Omega$ such that:

\begin{marginfigure}
\centering
\includegraphics[width=4cm]{fig/ab.png}
\caption{Open interval $(a,b)$,  $a \neq b$.
}
\end{marginfigure}


\begin{equation*}
\mathcal B(\mathbb R) = \sigma\left( \{(a,b) \subseteq \mathbb R: a \leq b \right \}).
\end{equation*}

For any real numbers $a \leq b$, the following, are all elements of 
$\mathcal B(\mathbb R)$.
\begin{itemize}
\item $(-\infty, a)$, $(b, \infty)$, $ (-\infty, a) \cup (b, \infty)$.
\item $[a,b] = ( (-\infty, a) \cup (b, \infty))^c$.
\item $(-\infty, a] = \cup_{n=1}^{\infty} [a-n,a] $ and $[b, \infty) = \cup_{n=1}^{\infty} [b,b + n]$.
\item $(a,b] = (a,\infty) \cap (-\infty, b] $
\item $\{a\} = \bigcap_{n=1}^{\infty} (a-\frac{1}{n},a+\frac{1}{n}) $ and $\{a_1,a_2,\dots,a_n\} = \cup_{k=1}^n \{a_k\}$.
\end{itemize}
%\end{mydef}

\begin{marginfigure}
\centering
\includegraphics[width=4cm]{fig/abb.png}
\caption{Semi-open interval $(a,b] = (a,\infty) \cap (-\infty, b]$,  $a \neq b$.
}
\end{marginfigure}

\subsection{Borel Algebra in $\mathbb R^n$*}
%\begin{mydef}{ Borel Algebra in $\mathbb R^n$}{theoexample} 

The Borel Algebra in $\mathbb R^n$ is defined as:

$$\mathcal B(\mathbb R^n) = \sigma( \mathcal B(\mathbb R)\times \cdots \times \mathcal B(\mathbb R) ).$$ 
The Borel $\sigma$-algebra is generated by many different systems of sets. 
The most interesting are:
\begin{itemize}
\item Open rectangles: $$\mathscr I^{n,o} =\mathscr I^n(\mathbb R^n) = \{ (a_1,b_1) \times \cdots \times (a_n,b_n): a_i,b_i\in \mathbb R\}$$.
\item From the right half-open rectangles: $$\mathscr I =\mathscr I(\mathbb R^n) = \{ [a_1,b_1) \times \cdots \times [a_n,b_n): a_i,b_i\in \mathbb R\}$$.
\end{itemize}



\begin{itemize}
\item  Specific examples
\begin{itemize}
 \item $\mathscr I^2(\mathbb R^2 ) = \sigma(\{(a, b) \times (c,d) \subseteq \mathbb R^2: a \leq b, c \leq d\})$.
  \item $\mathscr I(\mathbb R^2 ) = \sigma(\{[a, b) \times [c,d) \subseteq \mathbb R^2: a \leq b, c \leq d\})$.
 \item $\mathscr I^3(\mathbb R^3) = \sigma(\{(a, b) \times (c,d) \times (e,f) \subseteq \mathbb R^3: a \leq b, c \leq d, e \leq f\})$.
 \end{itemize}
\end{itemize}
%\end{mydef}

\begin{marginfigure}
\centering
\includegraphics[width=4cm]{fig/rectangle.png}
\caption{Rectangle  $R \in \mathscr I(\mathbb R^2 )$ for the real numbers $a,b,c$ and $d$  for which $a < b$ and $c< d$.
}
\end{marginfigure}

\section{Measurable Space}

%\begin{mydef}{Measurable Space }{theoexample}  
Let  $\mathscr F$ be a  $\sigma$-algebra defined on the sample space  $\Omega$.  The pair $(\Omega, \mathscr F)$ is called  \textit{measurable space}. 
%\end{mydef}

\begin{mydef}{ Product Space}{theoexample}  
Let   $(\Omega_1, \mathscr F_1)$ and $(\Omega_2, \mathscr F_2)$ be two measurable spaces.  The  measurable spaces product is defined as
$$(\Omega_1 \times \Omega_2 , \mathscr F_1 \otimes  \mathscr F_2 ),$$ where $\mathscr F_1 \otimes  \mathscr F_2  = \sigma(\mathscr F_1 \times  \mathscr F_2)$, and $\mathcal G = \mathscr F_1 \times  \mathscr F_2$ is the $\sigma$-algebra generator. In general, $$\mathscr F_1 \times  \mathscr F_2 := \{A_1 \times A_2: A_1 \in \mathcal F_1, A_1 \in \mathcal F_2\}, $$ is not a $\sigma$-algebra.
\end{mydef}

\begin{mybox}{Product Space Example  }{theoexample}
\small
Compute $(\Omega, \mathscr F) =(\Omega_1 \times \Omega_2, \mathscr F_1 \otimes \mathscr F_2)$ given:  $$\Omega_1=\{\mathsf H,\mathsf T\},$$ $$\mathscr F_1=\{\emptyset, \{\mathsf{H}\}, \{ \mathsf{T}\}, X_1 \},$$  $$\Omega_2=\{\mathsf h, \mathsf t\},$$
$$F_2=\{\emptyset, \{\mathsf{h}\}, \{ \mathsf{t}\}, X_2 \}.$$


\begin{itemize}
\item The cartesian product $\Omega_1\times \Omega_2$ is:
\begin{align*}
\Omega = \Omega_1\times \Omega_2 &= \{(\mathsf H, \mathsf h),(\mathsf H, \mathsf t),(\mathsf T, \mathsf h),( \mathsf T, \mathsf t)\}.
\end{align*} 
\item $\sigma$-algebra generator:
\begin{align*}
\mathscr F_1 \times  \mathscr F_2 = &\{ (\emptyset,\emptyset ), (\emptyset, \mathsf h), (\emptyset,\mathsf  t), (\emptyset, \Omega_2),\\
                                                           &(\mathsf H,\emptyset ), (\mathsf H,\mathsf h), (\mathsf H, \mathsf t), (\mathsf H,\Omega_2),\\
                                                           &(\mathsf T,\emptyset) ,  (\mathsf T, \mathsf h), (\mathsf T, \mathsf t), (\mathsf H,\Omega_2),\\
                                                           &(\Omega_1,\emptyset ), (\Omega_1, \mathsf h), (\Omega_1,\mathsf  t), (\Omega_1, \Omega_2)\}.
\end{align*}
\item $\sigma$-algebra:
\begin{align*}
\mathscr F = \mathscr F_1 \otimes \mathscr F_2  = &\{\emptyset, \\ 
                                   &\{ ( \mathsf H,  \mathsf  h)\}, \{ (  \mathsf H,  \mathsf t)\},  \{(  \mathsf  T,  \mathsf  h)\},  \{( \mathsf T, \mathsf   t)\}\\
                                   & \{( \mathsf H,  \mathsf  h), (\mathsf  H,\mathsf t)\},  \{( \mathsf H, \mathsf  h), ( \mathsf T, \mathsf  h)\},    \{(\mathsf H,  \mathsf h), ( \mathsf T,\mathsf t)\},\\ 
                                   &\{(\mathsf H,\mathsf t), (\mathsf T,\mathsf   h)\}, \{( \mathsf H,\mathsf t),(  \mathsf T,\mathsf  t)\},\\  
                                   & \{( \mathsf T,\mathsf   h), ( \mathsf T, \mathsf   t)\}, \\                                    
                                   & \{(\mathsf H, \mathsf h), (\mathsf  H, \mathsf  t), ( \mathsf T,\mathsf h)\},
                                       \{(\mathsf H,\mathsf h), (\mathsf H,\mathsf t), (\mathsf T, \mathsf  t)\},\\  
                                   & \{( \mathsf H,\mathsf  t), (\mathsf T, \mathsf h), ( \mathsf T,\mathsf    t)\},\\ 
                                   & \{( \mathsf H,\mathsf  h), (\mathsf T,\mathsf h), ( \mathsf T,\mathsf   t)\},\\ 
                                   & \Omega\}.                                                                                                                                                             
\end{align*} 
\end{itemize}  
\end{mybox}

\subsection{Measures }


Intuitively, a (measure-theoretic) measure is a way of giving value to things, but it's not as simple as measuring the length of a rope or the weight of an object. Instead, it's used for more complicated situations where you want to understand and compare things that aren't always straightforward to measure.
Imagine you have a collection of different-sized buckets, and you want to know how much water each one can hold. Each bucket might have an irregular shape, and you can't just fill them to see how much they hold. A measure-theoretic measure helps you assign a value to each bucket's capacity, even when they're not simple shapes. This concept is used in advanced mathematics and statistics to tackle complex problems and understand things in a more abstract or generalized way.


Measure theory is indeed concerned with the problem of how to assign a size to certain sets. 
In daily life this is easy to do: we can count, take measurements, or calculate rates. In each case, we compare
and express the result with respect to some base unit. 



\begin{mydef}{ Measure Definition }{theoexample} 
A positive measure $\mu$ on $\Omega$ is a map $\mu: \mathscr F  \rightarrow [0,\infty] $
satisfying:
\begin{itemize}
\item $ \mathscr F $ is a $\sigma$-algebra in $\Omega$. 
\item $\mu(\emptyset) = 0$
\item If  $(A_n)_{n \in \mathbb N} \subset \mathscr F $ are pair-wise disjoint, then 
\begin{equation*}
\mu\left( \bigcupdot_{n \in \mathbb N} A_n \right) = \sum_{n \in \mathbb N} \mu(A_n). 
\end{equation*}
\end{itemize}
\end{mydef}

\subsection{Examples of Measures}

\begin{itemize}
\item The set function on $\lambda$ on $(\mathbb R, \mathcal B(\mathbb R))$ that assigns every half-open rectangle
$[a,b)$ the value
\begin{equation*}
\lambda([a,b)) = b-a.
\end{equation*}
is called the \textit{one-dimensional Lebesgue measure}.
\item Let $\Omega = \{\omega_1, \omega_2, \dots \}$ be a countable and $(p_1,p_2,\dots)$ a sequence of numbers
$p_n\in[0,1]$, such that $\sum_{n\in \mathbb N} p_n =1$. On $(\Omega, \mathcal P(\omega), P)$ the set function
\begin{equation*}
P(A) = \sum_{n:\omega_n\in A},\,\,\,\, A\in \Omega,
\end{equation*}
defines a \textit{probability measure}. The triplet $(\Omega, \mathcal P(\omega), P)$ is called 
discrete probability space.
\item Box 2 shows an example of a discrete probability measure. Here we reproduce one of the tables of Box 2.

   \bmp{0.5\tw}  
  \begin{center}
  \small
  \begin{tabular}{ |l|c|}
    \hline
     $\Omega$ & $P(\omega)$ \\\hline
    $\omega_1 = \mathsf{H^cW^cS^cR^c}$ &  $\alpha_1$\\\hline
    $\omega_2 =\mathsf{H^cW^cS^cR}$    &  $\alpha_2$\\\hline
    $\omega_3 =\mathsf{H^cW^cSR^c}$    &  $\alpha_3$\\\hline
    $\omega_4 =\mathsf{H^cW^cSR}$        &  $\alpha_4$\\\hline
    $\omega_5 =\mathsf{H^cWS^cR^c}$     & $\alpha_5$\\\hline
    $\omega_6 =\mathsf{H^cWS^cR}$        & $\alpha_6$\\\hline     
    $\omega_7 =\mathsf{H^cWSR^c}$        & $\alpha_7$\\\hline
    $\omega_8 =\mathsf{H^cWSR}$           & $\alpha_8$\\\hline
    $\omega_9 =\mathsf{HW^cS^cR^c}$    & $\alpha_9$\\ \hline    
    $\omega_{10} =\mathsf{HW^cS^cR}$       & $\alpha_{10}$\\\hline
    $\omega_{11} =\mathsf{HW^cSR^c}$       & $\alpha_{11}$\\\hline
    $\omega_{12} =\mathsf{HW^cSR}$           & $\alpha_{12}$\\ \hline    
    $\omega_{13} =\mathsf{HWS^cR^c}$       & $\alpha_{13}$\\\hline
    $\omega_{14} =\mathsf{HWS^cR}$          & $\alpha_{14}$\\\hline
    $\omega_{15} =\mathsf{HWSR^c}$          & $\alpha_{15}$\\\hline     
    $\omega_{16} =\mathsf{HWSR}$             & $\alpha_{16}$\\                  
     \hline
  \end{tabular}
  \end{center}
  
  \emp
   \bmp{0.5\tw}  
   \begin{align*}
   P(\omega_k) &= \alpha_k,\\
   \sum_{k=1}^{16 }P(\omega_k) &= \sum_{k=1}^{16 } \alpha_k = 1.
   \end{align*}
   \emp
\end{itemize}


\subsection{Probability Measures}

In non-technical terms, a probability measure is a way of describing how likely or unlikely something is to happen. It's a bit like looking at weather forecasts that tell you the chances of rain. When we say there's a 50\% probability of rain, it means that out of every two similar situations, it's expected to rain in one of them.

\begin{marginfigure}
\centering
\includegraphics[width=4cm]{fig/pm.png}
\caption{A probability measure is a special case of a measure. Suppose you have a measurable space. Then a function $\mu: \mathcal F \rightarrow [0,+\infty]$ is called a measure on $(\Omega, \mathcal F)$
 if it gives a size of zero, to the empty set and if it is countably additive. Notice how the measure takes elements of $\mathcal F$ and not elements of the sample space $\Omega$; i.e.,  it maps events, not outcomes. A probability measure is just a measure,  with the additional property that it maps to $[0,1]$ rather than to $[0,+\infty]$. In order words, it assigns probabilities to events. We can see in the diagram above how the probability measure maps the empty set to zero, the sample space to  $1$, and any other event to some number in $[0,1]$.
}
\end{marginfigure}


So, a probability measure assigns a number between 0 and 1 to events, where 0 means the event won't happen, 1 means it will definitely happen, and values in between tell us the likelihood of something occurring. It helps us understand and work with uncertainty, make predictions, and make informed decisions based on the chances of different events (Figure 19).

\begin{mydef}{ Probability Measure Definition }{theoexample} 
A probability measure is a  a map $P = \mathbb P : \mathscr F  \rightarrow [0,1] $
satisfying:
\begin{itemize}
\item $ \mathscr F $ is a $\sigma$-algebra in $\Omega$. 
\item $P(\emptyset) = 0$
\item $P(\Omega) = 1$
\item If  $(A_n)_{n \in \mathbb N} \subset \mathscr F $ are pair-wise disjoint, then 
\begin{equation*}
P\left( \bigcupdot_{n \in \mathbb N} A_n \right) = \sum_{n \in \mathbb N} P(A_n). 
\end{equation*}
\end{itemize}
\end{mydef}



%\begin{mydef}{ Probability Measure Properties }{theoexample} 
 let $(\Omega, \mathscr F, P)$ be a probability space,  $A,B, B_n, A_n\in \Omega$, $(A_n)_{n \in \mathbb N}$  pairwise disjoint. 
\begin{enumerate}
\item $A \cap B = \emptyset \Longrightarrow$ $P(A \cupdot B) = P(A) + P(B)$.
\item $A \subset B \Longrightarrow$ $P(A) \leq P(B)$.
\item$A \subset B, \Longrightarrow$ $P(B \setminus A) = P(B) - P(A)$.
\item $P(A\cup B) + P(A\cap B) = P(A) + P(B).$
\item $P(A\cup B) \leq  P(A) + P(B).$
\item $P\left( \bigcup_{n \in \mathbb N} B_n \right) \leq \sum_{n \in \mathbb N} P(B_n).$ 
\item  $\bigcupdot_{n \in \mathbb N} A_n = \Omega $ $\Longrightarrow$  $P(A)=\sum_{n \in \mathbb N} P(A\cap A_n).$
\end{enumerate}
%\end{mydef}

\begin{marginfigure}
\centering
\includegraphics[width=4cm]{fig/venn.png}
\caption{ Events in the sample space $\Omega$.
}
\end{marginfigure}

\begin{marginfigure}
\centering
\includegraphics[width=4cm]{fig/venn2.png}
\caption{ Property 7 example.
}
\end{marginfigure}



\subsection{Probability Measure Examples}

\begin{itemize}
\item We assume the outcome can be directly observed at the end of the experiment and thus $\mathscr F$ is chosen to be the largest possible $\sigma$-algebra, and we define $P$ on it. The precise definition of $P$ depends on the application:
\begin{itemize}  
\item For a finite sample space $\Omega$ where each outcome is equally likely, define $P$ on $\mathscr F = \mathscr P(\Omega)$ via $P(A)= \frac{\mid A\mid}{\mid \Omega \mid}$ for any $A \in \mathscr F$.
\item To model the number of coin flip required to obtain the first head ($\Omega = \mathsf{\{1,2,3,...\}}$), define $P$ on $\mathscr F = \mathscr P(\Omega)$ where $P$ satisfies $P(\{\omega: \omega=\mathsf k\})=\mathsf{(1-p)^{k-1}p}$. Here $\mathsf{p\in(0,1)}$ represents the chance of getting a head in a single flip.
\end{itemize}
\item To represent a uniform random number draw from $\Omega = [0, 1]$, define $P$ on $\mathscr F = \mathcal B([0, 1])$ where $P$ satisfies $P([a, b]) = b-a$ for $0 \le a \le b \le 1$. This is the Lebesgue measure on $[0, 1]$.
\end{itemize}

\subsection{Independent Events}
$A_m, A_n\in \mathscr F$ are independent $\Longrightarrow$ $P(A_m\cap A_n) = P(A_m)P(A_n)$
\begin{itemize}
\item $(A_n)_{n \in \mathbb N}$  are pair-wise independent $\Longrightarrow$ $A_m$ and $A_n$ are independent for any $n \neq m$, $n,m \in \mathbb N$
\item $(A_n)_{n \in I}$,   are independent $\Longrightarrow$ $P(\bigcap_{n \in  I} A_n) = \prod_{n \in  I}P(A_n)$
\end{itemize}


\subsection{Conditional probability}

The conditional probability of event $A$ conditioned to event $B$ is defined as
\begin{equation*}
P(A\mid B) = \frac{P(A\cap B)}{P(B)}
\end{equation*}
where $P(B) >0$. A conditional probability is a \textit{probability measure}.
If $A$ and $B$ are independent $P(A\mid B) = P(A)$ and $P(B\mid A) = P(B)$.

\subsection{Chain rule}
Consider the events $A_1, A_2,\cdots, A_n$. The chain rule is defined as follows.
$$P(A_1 \cap A_2 \cap \dots \cap A_n) = $$
$$P(A_1)  P(A_2\mid A_1)  P(A_3\mid A_1 \cap A_2) \cdots  P(A_n\mid A_1 \cap \dots \cap A_{n-1})$$
\begin{itemize}
\item Apply the chain rule to the following expression $P(A_1 \cap A_2 \cap A_3 \cap A_4)$.
\item Is the following expression correct? $P(A_1 \cap A_2 \cap A_3) = P(A_1 \mid  A_2 \cap A_3)P(A_2 \mid A_3)p(A_3)$.
\end{itemize}


\subsection{Bayes Theorem}  
\begin{equation*}
P(A\mid B) = \frac{P(B\mid A)P(A)}{P(B)}
\end{equation*}
where $P(B) >0$.
Recall that    $P(B)=\sum_{n \in \mathbb N} P(B\cap A_n)$ with  $\bigcupdot_{n \in \mathbb N} A_n = \Omega $. Therefore: 
\begin{align*}
P(A_m\mid B) &= \frac{P(B\mid A_m)P(A_m)}{P(B)}\\
&= \frac{P(B\mid A_m)P(A_m)}{\sum_{n \in \mathbb N} P(B\cap A_n)} = \frac{P(B\mid A_m)P(A_m)}{\sum_{n \in \mathbb N} P(B\mid A_n)P(A_n)} 
\end{align*}


\subsection{Applying the Bayes Theorem}

%\begin{mybox}{Sample Space Exercise }{theoexample}
%\small
\begin{itemize}
\item Mr. Holmes now lives in Los Angeles. One morning when Homes leaves his house, he realizes that his grass is wet (H). Is it due to rain ($\mathsf R$), or has he forgotten to turn off the sprinkler ($\mathsf S$)? Next, he notices that the grass of his neighbor, Dr. Watson, is also wet ($\mathsf W$). 

\begin{itemize}
 \item What is the probability that Holme's grass is wet ($\mathsf H$) given that he forgot to turn the sprinkler off ($\mathsf S$)?
 \item What is the probability that Holme's grass is wet ($\mathsf H$) given that he forgot to turn the sprinkler off ($\mathsf S$) and  Dr. Watson's grass is also wet ($\mathsf W$)?
\end{itemize}
\item  Consider the following events.
 \begin{itemize} 
 \item $A=$ "\sf Holmes' grass is wet."
 \item  $A^c=$"\sf Holmes grass is dry. " 
 \item $B=$"\sf Holmes forgot to turn the sprinkler off."
 \item $B' =$ \textsf{``Holmes forgot to turn the sprinkler off and Watson's grass is wet.'' }
 \end{itemize}
  \vspace{0.2cm}  
%\begin{enumerate}
%\item 
We write down the outcomes including these events and their corresponding probabilities in the tables shown below:

  \vspace{0.2cm}  
  \bmp{0.5\tw}  
   \begin{center}
  \begin{tabular}{ |l|c|}
    \hline
     $\Omega$ & $P(\omega)$ \\\hline
    $\omega_1 = \mathsf{H^cW^cS^cR^c}$ &  $\alpha_1$\\\hline
    $\omega_2 =\mathsf{H^cW^cS^cR}$    &  $\alpha_2$\\\hline
    $\omega_3 =\mathsf{H^cW^cSR^c}$    &  $\alpha_3$\\\hline
    $\omega_4 =\mathsf{H^cW^cSR}$        &  $\alpha_4$\\\hline
    $\omega_5 =\mathsf{H^cWS^cR^c}$     & $\alpha_5$\\\hline
    $\omega_6 =\mathsf{H^cWS^cR}$        & $\alpha_6$\\\hline     
    $\omega_7 =\mathsf{H^cWSR^c}$        & $\alpha_7$\\\hline
    $\omega_8 =\mathsf{H^cWSR}$           & $\alpha_8$\\\hline
    $\omega_9 =\mathsf{HW^cS^cR^c}$    & $\alpha_9$\\ \hline    
    $\omega_{10} =\mathsf{HW^cS^cR}$       & $\alpha_{10}$\\\hline
    $\omega_{11} =\mathsf{HW^cSR^c}$       & $\alpha_{11}$\\\hline
    $\omega_{12} =\mathsf{HW^cSR}$           & $\alpha_{12}$\\ \hline    
    $\omega_{13} =\mathsf{HWS^cR^c}$       & $\alpha_{13}$\\\hline
    $\omega_{14} =\mathsf{HWS^cR}$          & $\alpha_{14}$\\\hline
    $\omega_{15} =\mathsf{HWSR^c}$          & $\alpha_{15}$\\\hline     
    $\omega_{16} =\mathsf{HWSR}$             & $\alpha_{16}$\\                  
     \hline
  \end{tabular}
  \end{center}
  \emp
   \bmp{0.5\tw}  
  \begin{center}
  \begin{tabular}{ |l|c|}
    \hline
     $A$& $P(\omega)$ \\\hline
     $\omega_{9}$ = \textcolor{black}{$\mathsf{HW^cS^cR^c}$ }   & $\alpha_9$\\ \hline    
    $\omega_{10}= \mathsf{HW^cS^cR}$       & $\alpha_{10}$\\\hline
    $\omega_{11}=\mathsf{HW^cSR^c}$       & $\alpha_{11}$\\\hline
    $\omega_{12}=\mathsf{HW^cSR}$           & $\alpha_{12}$\\ \hline    
    $\omega_{13}=\mathsf{HWS^cR^c}$       & $\alpha_{13}$\\\hline
    $\omega_{14}=\mathsf{HWS^cR}$          & $\alpha_{14}$\\\hline
    $\omega_{15}=\mathsf{HWSR^c}$          & $\alpha_{15}$\\\hline     
    $\omega_{16}=\mathsf{HWSR}$             & $\alpha_{16}$\\                  
     \hline
  \end{tabular}    
  \end{center}
  \emp    
%\item  Events:

  \vspace{0.2cm}  
  \bmp{0.5\tw}  
  \begin{center}
  \begin{tabular}{ |l|c|}
    \hline
    $A^c$ & $P(\omega)$ \\\hline    
    $\omega_{1} = \mathsf{H^cW^cS^cR^c}$ &  $\alpha_1$\\\hline
    $\omega_{2} = \mathsf{H^cW^cS^cR}$    &  $\alpha_2$\\\hline
    $\omega_{3} = \mathsf{H^cW^cSR^c}$    &  $\alpha_3$\\\hline
    $\omega_{4} = \mathsf{H^cW^cSR}$        &  $\alpha_4$\\\hline
    $\omega_{5} = \mathsf{H^cWS^cR^c}$     & $\alpha_5$\\\hline
    $\omega_{6} = \mathsf{H^cWS^cR}$        & $\alpha_6$\\\hline     
    $\omega_{7} = \mathsf{H^cWSR^c}$        & $\alpha_7$\\\hline
    $\omega_{8} = \mathsf{H^cWSR}$           & $\alpha_8$\\                  
     \hline
  \end{tabular}    
  \end{center}
  \emp  
   \bmp{0.5\tw} 
   % \vspace{0.2cm}   
   \begin{center}
   \begin{tabular}{ |l|c|}
    \hline
    $B$& $P(\omega)$ \\\hline    
    $\omega_{3} = \mathsf{H^cW^cSR^c}$    &  $\alpha_3$\\\hline
    $\omega_{4} = \mathsf{H^cW^cSR}$        &  $\alpha_4$\\\hline   
    $\omega_{7} = \mathsf{H^cWSR^c}$        & $\alpha_7$\\\hline
    $\omega_{8} = \mathsf{H^cWSR}$           & $\alpha_8$\\\hline
    $\omega_{11} = \mathsf{HW^cSR^c}$       & $\alpha_{11}$\\\hline
    $\omega_{12} = \mathsf{HW^cSR}$           & $\alpha_{12}$\\ \hline   
    $\omega_{15} = \mathsf{HWSR^c}$          & $\alpha_{15}$\\\hline     
    $\omega_{16} = \mathsf{HWSR}$             & $\alpha_{16}$\\                 
     \hline
  \end{tabular}   
  \end{center}
  \emp
  
   \vspace{0.2cm} 
    \bmp{0.5\tw} 
   \begin{center}
   \begin{tabular}{ |l|c|}
    \hline
    $B'$ & $P(\omega)$ \\\hline    
     $\omega_{7}= \mathsf{H^cWSR^c}$        & $\alpha_7$\\\hline
     $\omega_{8}=\mathsf{H^cWSR}$           & $\alpha_8$\\\hline   
     $\omega_{15}=\mathsf{HWSR^c}$          & $\alpha_{15}$\\\hline     
     $\omega_{16}=\mathsf{HWSR}$             & $\alpha_{16}$\\             
     \hline
  \end{tabular}   
  \end{center}
  \emp
%\end{enumerate}  
\end{itemize}
%\end{mybox}
 \vspace{2cm} 
\begin{mybox}{Example solutions }{theoexample}
\small
We use conditional probability and the Bayes theorem to compute the requested probabilities.   
  \begin{align*}
  P(B\mid A)&=\frac{P(B\cap A)}{P(A)} = \frac{\alpha_{11}+\alpha_{12}+\alpha_{15}+\alpha_{16}}{\sum_{k=9}^{16}\alpha_k}.\\
  %\end{align*}
 %   \begin{equation*}
  \textcolor{DarkOrchid} {P(A\mid B)}&=\frac{P(B\mid A)P(A)}{P(B)} = \textcolor{DarkOrchid}{\frac{\alpha_{11}+\alpha_{12}+\alpha_{15}+\alpha_{16}}{\alpha_{3}+\alpha_{4}+\alpha_{7}+\alpha_{8} + \alpha_{11}+\alpha_{12}+\alpha_{15}+\alpha_{16}}}.\\
 % \end{align*}
 %\begin{equation*}
  P(B'\mid A)&=\frac{P(B'\cap A)}{P(A)} = \frac{\alpha_{15}+\alpha_{16}}{\sum_{k=9}^{16}\alpha_k}.\\
%  \end{equation*}
%      \begin{equation*}
 \textcolor{DarkOrchid}{P(A\mid B')}&=\frac{P(B'\mid A)P(A)}{P(B')} = \textcolor{DarkOrchid}{\frac{ \alpha_{15}+\alpha_{16}}{\alpha_{7} + \alpha_{8} +\alpha_{15}+\alpha_{16}}}.
  \end{align*}
\end{mybox}


\subsection{Random Variables}

\begin{marginfigure}
\centering
\includegraphics[width=7cm]{fig/rm.png}
\caption{Random variable example defined on the real line $\mathbb R$. Notice that the inverse mapping $X^{-1}$ maps real numbers into measurable events. For example  $\{HHH\}=X^{-1}(3)$ and  .
$\{\{HHT\}, \{HTH\} , \{THH\}\} =X^{-1}(2)$ belong to the $\sigma$-algebra $\mathcal F = \mathcal P(\Omega)$.
}
\end{marginfigure}

\begin{marginfigure}
\centering
\includegraphics{fig/rvv.png}
\caption{Random variable definition.
}
\end{marginfigure}
A random variable is also called a measurable function. Suppose you have two measurable spaces. One could be the pair of sample space and event space 
$(\Omega, \mathcal F)$, and the other one could be some other arbitrary pair of a set and of its $\sigma$-algebra $(E, \mathcal E)$, although usually, we choose  $E = \mathbb R^n$ and $\mathcal E = \mathcal B(\mathbb R^n)$, $n=1,2,\dots$. Then a measurable function is a function $X$ that maps elements in $\Omega$ to elements in $ E$ with some additional properties. Notice how this function maps outcomes to elements of $E$, it does not map events (Figure 20).

The mapping  $X$  guarantees a  correspondence between the events in our original event space 
 and our transformed event space. For this reason, we require the random variable $X: \Omega \rightarrow E $ ($X: \Omega \rightarrow \mathbb R $) is such that to be such that the preimage 
 $X^{-1}(B)$of any  $\mathcal E$-measurable set $B\in \mathcal B$ is a $\mathcal F$ measurable set.
\begin{equation*}
X^{-1}(B) = \{\omega \in \Omega: X(\omega) \in B \} \in \mathcal F \;\; \forall B \in \mathcal E.
\end{equation*}
In the diagram below we can see how the set $B$, which is an element of $\mathcal E$
 has a pre-image,  $X^{-1}(B)$, which is an element of $\mathcal F$. From here on, we use $E = \mathbb R^n$, and $\mathcal E = \mathcal B(\mathbb R^n)$ to define a random variable.
 \begin{figure}
\centering
\includegraphics[width=10cm]{fig/rv.png}
\caption{Random variable definition.
}
\end{figure}

\begin{mydef}{Random Variable}{theoexample} 
Let $(\Omega, \mathscr F, P)$ be a probability space.  A  \textit{random variable} $X$  is a   ($\mathcal F/\mathcal B(\mathbb R^n)$) measurable map  $X:\Omega \rightarrow \mathbb R^n$, if for every Borel set  $B \in \mathcal B(\mathbb R^n)$:
\begin{align*}
X^{-1}(B) =\{ \omega \in \Omega: X(\omega) \in B \} \in \mathscr F.
\end{align*}
Clearly,  
\begin{itemize}
\item $X^{-1}(B)\in \mathscr F \,\,\,\,\,\,\forall B\in \mathcal B(\mathbb R^n)$. %and 
%\item $P(X\in B) :=  P(X^{-1}(B)) = P\{\omega\in\Omega: X(\omega) \in B\}$.
\end{itemize}
\end{mydef}

\subsection{Types of Random Variables}
In practice, we seldom bother working with the abstract concept of a probability space $(\Omega,\mathscr F, P)$, but rather just focus on the distributional properties of a random variable $X$ representing the random phenomenon. We are interested in random variables that are {\em discrete} and {\em continuous}.
\begin{itemize}
\item A random variable $X$ is discrete if it only takes values on a countable set $\Omega = \{\omega_1,\omega_2,\omega_3,\dots\}$, which is called the support of $X$. 
%\item Formally, we would write $\Omega = \{1,2,3,...\}$, $\mathscr F = \mathscr P(X)$ and let $P$ be a probability measure satisfying $P({\omega : \omega = k}) = \mathsf{(1-p)^{k-1}p}$ for $\mathsf{k = 1,2,3...}$
%\item In practice, we would simply let U be the number of flips required, and consider $P(U = k) = \mathsf{(1-p)^{k-1}p}$ for $k = \mathsf{1, 2, 3...}$.
\item A random variable $X$ is a continuous random variable if it only takes values on a non-countable set $\Omega$.
\item  Random variables can also be mixed.
\end{itemize}

\subsection{Examples of Discrete Random Variables}
\begin{itemize}
\item Consider the experiment in which we roll a die: $\Omega=\mathsf{\{1,2,3,4,5,6\}}$.
\begin{itemize}
\item Let $\mathscr F= \mathscr P(\Omega)$. The random variable $X_1(\omega) = \omega$, is $\mathscr F/\mathcal B(\mathbb R)$ measurable. $X_1$ gives the exact outcome of the roll.
\item Let $\mathscr F_1= \{\emptyset,\Omega, \mathsf{\{1,3,5\},\{2,4,6\}\}}$. The random variable
\begin{align*}
X_2(\omega) = 
\begin{cases}
 1, & \omega \in\{1,3,5\}\\
 0, & \omega \in\{2,4,6\}
\end{cases}				
\end{align*}
is $\mathscr F_1\mathcal / \mathcal B(\mathbb R)$ measurable; its value depends on whether the roll is odd or even.
\item If we only have information on whether the roll is odd or even, we can determine the value of $X_2$ but not the value of $X_1$.
\item The random variable $X_1$, is  not  $\mathscr F_{1}/\mathcal B(\mathbb R)$ measurable. For instance, $X_{1}^{-1}(1) = \{1\} \notin \mathscr F_{1}$.
\end{itemize}
\item Consider the experiment in which we roll a three-faced dice: $\Omega=\mathsf{\{-1,0,1\}}$ and  $\mathscr F_1= \{\emptyset,\Omega, \mathsf{\{-1,1\},\{0\}\}}$.
\begin{itemize}
\item $X_1(\omega)=\omega$ is not $\mathscr F_{1}/\mathcal B(\mathbb R)$ measurable. For example, $X^{-1}(1) = \{1\} \notin \mathscr F_{1}$.
\item $X_2(\omega)=\omega^2$ is $\mathscr F_{1}/\mathcal B(\mathbb R)$ measurable.
\end{itemize} 
\item Let $(\Omega,\mathscr F)$ describe throwing two fair dice, i.e. $\Omega:= \{(i,k): \mathsf 1\leq i,k\leq \mathsf 6\}$, $\mathscr F = \mathscr P (\Omega)$. The total number of points thrown $X: \Omega \rightarrow \mathsf{\{2,3,\dots, 12\}}$, $U((i,j))=i+j$ is a measurable map.
\item *A $\sigma$-algebra generated by a random variable $U$, denoted by $\sigma(U)$, is the smallest $\sigma$-algebra for which $U$ is measurable.
For example, for $\Omega=\{\mathsf{HH,HT,TH,TT}\}$.
\begin{equation*}
X_1(\omega) = 
\begin{cases}
 1, & \omega \in \mathsf{\{HH,HT\}},\\
 0, & \omega \in \mathsf{\{TH,TT\}}.
\end{cases}\,\,\,\,\,
X_2(\omega) = 
\begin{cases}
 2, & \omega \in \mathsf{\{HH\}},\\
 1, & \omega \in \mathsf{\{HT\}},\\
 -1, & \omega \in \mathsf{\{TH\}},\\
 -2, & \omega \in \mathsf{\{TT\}}. 
\end{cases}				
\end{equation*}
 $\sigma(X_1)=\{\emptyset, \mathsf{\{HH,HT\},\{TH,TT\}}, \Omega\}$ and $\sigma(X_2)=\mathscr P(\Omega)$. In particular,
$\sigma(X_1)\subset \sigma(X_2)=\mathscr P(\Omega)$.

\end{itemize}



\subsection{Probability Distributions}


A probability distribution is also called a \textit{push-forward} ($P_X = P_* X = P \circ X^{-1}$) measure of the \textit{probability measure} $P$,
 via the random variable $X$. Suppose we have a probability space $(\Omega, \mathcal F, P)$
This means we can assign probabilities to events in $\mathcal F$. Now suppose we have a measurable space 
 $(E, \mathcal E)$ but we don’t yet have a probability measure to measure events in it. How can we go about measuring sets in 
 $\mathcal E$?

\begin{marginfigure}
\centering
\includegraphics{fig/diag1.png}

\caption{Probability distribution defined on the real line $\mathbb R$.
}
\end{marginfigure}


The key idea is that, given a set $B$ in $\mathcal E$, we can use a random variable $X$
 to find the pre-image of such set in the event space $ \mathcal F$
 and then measure this set via the probability measure $P$. This will then be our probability measurement for 
 $B$, as shown in the figure below.
 
 The \textit{probability distribution},  is defined as $P_X =P_* X = P\circ X^{-1}: \mathcal E \rightarrow [0,1]$. 
 The probability distribution is therefore a function mapping sets in $\mathcal E$  into 
$[0,1]$. Here we use $(E, \mathcal E) = (\mathbb R^n,\mathcal B(\mathbb R^n))$, that is, 
$P\circ X^{-1}: \mathcal B(\mathbb R^n)  \rightarrow [0,1]$.


\begin{figure}
\centering
\includegraphics[width=10cm]{fig/pd.png}
\caption{Probability distribution definition.
}
\end{figure}
\begin{marginfigure}
\centering
\includegraphics{fig/diag2.png}

\caption{Probability distribution defined on the $n$-dimensional real space $\mathbb R$.
}
\end{marginfigure}

\begin{mydef}{Probability Distribution}{theoexample}
Let $(\Omega, P, \mathscr F)$ be a measure space, and  $X$  a random variable $X:\Omega \rightarrow \mathbb R^n $, i.e. a   measurable map. Then
\begin{equation*}
P(X^{-1}(B)) = P(\{\omega: X(\omega) \in B\}) = P(X \in B).
\end{equation*}
is a probability measure called the \textit{law} or \textit{distribution} of the random variable $X$. Note: Here we use $(E, \mathcal E) = (\mathbb R^n,\mathcal B(\mathbb R^n))$, that is, 
$P\circ X^{-1}: \mathcal B(\mathbb R^n)  \rightarrow [0,1]$.

\end{mydef}


\begin{marginfigure}
\centering
\includegraphics{fig/disc.png}

\caption{Probability distribution  $P(K=k) $ in  Table 1.
}
\end{marginfigure}

\subsection{Examples of Probability Distributions for Discrete Random Variables}
\begin{itemize}
\item Let $(\Omega,\mathscr F)$ describe throwing two fair dice, i.e. $\Omega:= \{(i,k): \mathsf 1\leq i,k\leq \mathsf 6\}$, $\mathscr F = \mathscr P (\Omega)$, and $P(\{i,j\}) = \mathsf{\frac{1}{36}}$. The total number of points thrown $X: \Omega \rightarrow \mathsf{\{2,3,\dots, 12\}}$, $X((i,j))=i+j$ is a measurable map (Table 1 and Figure 21).
\begin{table}[tbh]
\caption{The distribution of the random variable $X$.}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{cccccccccccc} 
\hlineB{1.5}\hline\hline
$k$ & $\mathsf 2$ & $\mathsf 3$ & $\mathsf 4$ & $\mathsf 5$ & $\mathsf 6$ & $\mathsf 7$ & $\mathsf 8$ & $\mathsf 9$ & $\mathsf 10$ & $\mathsf 11$ & $\mathsf 12$ \\\hline
$P(X= k)$ & $\mathsf{\frac{1}{36}}$  & $\mathsf{\frac{1}{18}}$ & $\mathsf{\frac{1}{12}}$ & $\mathsf{\frac{1}{9}}$ & $\mathsf{\frac{5}{36}}$ & $\mathsf{\frac{1}{6}}$ & $\mathsf{\frac{5}{36}}$ & $\mathsf{\frac{1}{9}}$ & $\mathsf{\frac{1}{12}}$ & $\mathsf{\frac{1}{18}}$ & $\mathsf{\frac{1}{36}}$ \\
\hlineB{1.5}
\end{tabular}
\end{table}
\vspace{.2cm}
\item The \textit{ Bernoulli random} variable $X\in\{0,1\}$ with parameter $\mathsf p$,  $0<\mathsf p\leq 1$ has the following probability distribution:
\begin{equation*}
P(X=x\mid \mathsf p) := \mathsf{Bernoulli}(X=x\mid \mathsf p) =  \mathsf p^x(1-\mathsf p)^{1-x} .
\end{equation*}
Hint: $P(X=1\mid \mathsf p) = \mathsf p$.

\begin{marginfigure}
\centering
\includegraphics{fig/binomial.png}

\caption{Binomial probability distribution function for $N=20$ and $p=0.5$.
}
\end{marginfigure}

\item The \textit{Binomial distribution} with parameters $\mathsf N$ and $\mathsf p$ is the discrete probability distribution of the number $K$ of successes in a sequence of $\mathsf N$ independent Bernoulli trials (with parameter $\mathsf p$). The probability distribution is
\begin{marginfigure}
\centering
\includegraphics{fig/binomial2.png}

\caption{Binomial probability distribution function for $N=20$ and $p=0.6$.
}
\end{marginfigure}

\begin{equation*}
P(K=k\mid {\mathsf N},\mathsf p) := \mathsf{Binomial}(K=k\mid \mathsf p,\mathsf N)  = {{\mathsf N}\choose k} \mathsf p^k\mathsf{(1- p)}^{{N}-k} \,\,\,\, 
\end{equation*}
$ \text{for } \,\,\,\, k = 0,1,2,\dots$ where $${{\mathsf N}\choose k} = \frac{{\mathsf N}!}{({\mathsf N}-k)!k!}$$ is the number of ways of choosing $K=k$ objects out of a total of $ \mathsf N$ identical objects.
\end{itemize}

\section{Continuous Random Variables }

A random variable $X$ is a continuous random variable if there exists a non-negative function $f_X(\cdot)$ such that:
\begin{align*}
P(X\leq \omega) = \int_{-\infty}^\omega f_X(\alpha)d\alpha
\end{align*}
for any $\omega\in \mathbb R$. The function $f_X$ is called the probability density function of $X$. To simplify notation in practice we use $p(x) = f(x) = f_X(x)$. We remark on the abuse of notation.

\begin{marginfigure}
\centering
\includegraphics{fig/beta3.png}

\caption{Beta probability density function. $\alpha=2$, $\beta=5$ (dark green), $\alpha=2$, $\beta=2$ (lime green).
}
\end{marginfigure}
\section{Examples  of Continuous Random Variables } 
\begin{itemize}
\item A  random variable $M\in[0,1]$ has a Beta distribution of variable with parameters $\alpha$ and $\beta$ if the density function has the form
  \begin{equation*}
  f_M(m\mid \alpha,\beta) := \mathsf{Beta}(m \mid \alpha,\beta)  = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} m^{\alpha-1}(1-m)^{\beta-1},
  \end{equation*}
  where $\Gamma(x)$ is the Gamma function  $\Gamma(x)=\int_0^{x} u^{x-1} e^{-u}  du$.
  
\begin{marginfigure}
\centering
\includegraphics{fig/gauss2.png}

\caption{Gaussian probability density function. $\mu=0$, $\sigma^2=3$ (dark green), $\mu=2$, $\sigma^2=2$ (lime green).
}
\end{marginfigure}

 \item A  random variable $X\in \mathbb R$ has a Gaussian or Normal distribution of variable with parameters $\mu$ and $\sigma^2$ if the density function has the form
   \begin{equation*}
  f_X(x\mid \mu,\sigma^2) := \mathcal{N}(x \mid\mu,\sigma^2)  = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\{-\frac{1}{2\sigma^2} (x-\mu)^2\},
  \end{equation*}
\end{itemize}



\section{ Cummulative Distribution Function}
For a random variable $X$, its cumulative distribution function (CDF) is defined as:
\begin{align*}
F_X(  x) := P(X \leq   x), \,\,\,\,   -\infty \leq  x \leq \infty
\end{align*}
Note that $P(X \leq  x) = P\circ X^{-1}((-\infty, x])$, and:
\begin{marginfigure}
\centering
\includegraphics{fig/Fx.png}

\caption{Cumulative distribution of the random variable $X$ in Table 1.
}
\end{marginfigure}
\begin{itemize}
\item $F_X(  x)$ is non-decreasing and right-continuous.
\item $\lim_{ x\rightarrow \infty} F_X( x)= 1$ and 
\item $\lim_{  x\rightarrow -\infty} F_X( x)= 0$
\end{itemize}

Conversely, if a given function $F_X$ satisfies the above properties, then it is a CDF of some random variable.

As an example, we show below the cumulative distribution of the random variable $X$ in Table 1 (see Figures 21 and 22). 

\begin{table}[tbh] 
\caption{}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{cccccccccccc} 
\hlineB{1.5}\hline\hline
$k$ & $\mathsf 2$ & $\mathsf 3$ & $\mathsf 4$ & $\mathsf 5$ & $\mathsf 6$ & $\mathsf 7$ & $\mathsf 8$ & $\mathsf 9$ & $\mathsf 10$ & $\mathsf 11$ & $\mathsf 12$ \\\hline
$F_K(k)$ & $\mathsf{0.02}$  & $\mathsf{0.08}$ & $\mathsf{0.16}$ & $\mathsf{0.27}$ & $\mathsf{0.41}$ & $\mathsf{0.58}$ & $\mathsf{0.72}$ & $\mathsf{0.83}$ & $\mathsf{0.91}$ & $\mathsf{0.97}$ & $\mathsf{1}$ \\
\hlineB{1.5}
\end{tabular}
\end{table}
\vspace{.2cm}


\subsection{Drawing Samples from  Random Variable}

Drawing samples from a random variable refers to the process of generating individual realizations or instances of that random variable according to its probability distribution. In probability theory and statistics, a random variable is a variable whose possible values are outcomes of a random phenomenon. The probability distribution of a random variable describes the likelihood of different values it can take.

When you draw samples from a random variable, you are essentially simulating or generating data points that follow the probability distribution of that variable. This process is often used in various fields such as statistics, machine learning, and simulations to understand the behavior of random phenomena or to make predictions.

For example, if you have a random variable representing the outcome of a fair six-sided die roll, drawing samples from this random variable would involve simulating the roll of the die and obtaining values like 1, 2, 3, 4, 5, or 6 with equal probability.

The concept of drawing samples is fundamental to Monte Carlo simulations, where random sampling is used to estimate numerical results and analyze complex systems that involve randomness. In statistical terms, the more samples you draw, the better your approximation of the true underlying distribution or properties of the random variable.

\subsection{Expected Value and Variance of a Discrete Random Variable}

For a discrete random variable  $X$, its expected value is defined as:
\begin{align*}
\mu_X = \mathrm E\left[X\right] = \sum_{\omega \in \Omega} X(\omega) p_X(\omega) = \sum_x xp(x).
\end{align*}
For example, for the random variable $K$ of Table 1, the expected value is
\begin{align*}
\small
 \mu_K &= 2(\frac{1}{36})+3(\frac{1}{18})+4(\frac{1}{12})+5(\frac{1}{9})+6(\frac{5}{36})+7(\frac{1}{6})+8(\frac{5}{36})+9(\frac{1}{9})+10(\frac{1}{12})+11(\frac{1}{18})+ 12(\frac{1}{36}) \\
&= 7.
\end{align*}



More generally, for a given function $g(\cdot)$ we define:
\begin{align*}
\mu_{g(X)} = \mathrm E\left[g(X)\right] = \sum_{\omega \in \Omega} g(X(\omega)) p_X(\omega) = \sum_x g(x)p(x).
\end{align*}
The variance of $X$ is defined as:
\begin{align*}
\mathrm{var}\left[X\right] = \sum_{\omega \in \Omega} (X(\omega)- E\left[X\right] )^2  p_X(\omega)= \sum_x (x-\mu_X)^2p(x) = E\left[X^2\right]-(E\left[X\right])^2 \\
\end{align*}
For example, for the random variable $K$ of Table 1, the variance is computed as follows
\begin{align*}
\mathrm E[K^2] &= 2^2(\frac{1}{36})+3^2(\frac{1}{18})+4^2(\frac{1}{12})+5^2(\frac{1}{9})+6^2(\frac{5}{36})+7^2(\frac{1}{6})+8^2(\frac{5}{36})+9^2(\frac{1}{9})+10^2(\frac{1}{12})+11^2(\frac{1}{18})+ 12^2(\frac{1}{36})\\
&= 54.83\\
\mathrm (E[K])^2 &= 49\\
\mathrm{var}[K] &= 5.83.
\end{align*}


\subsection{Expected Value and Variance of a Continuous Random Variable}


For a continuous random variable  $X$ with probability density $f_X(x)$, its expected value is defined as:
\begin{align*}
\mu_x = \mathrm E\left[X\right] = \int_{-\infty}^\infty \omega f_X(\omega) d\omega.\,\,\,\,\, \,\,\,\,%E\left[U\right] = \int U dP(\omega).
\end{align*}
More generally, for a given function $g(\cdot)$ we define:
\begin{align*}
\mathrm E\left[g(X)\right] = \int_{-\infty}^\infty f_X(\omega) f_X(\omega) d\omega.
\end{align*}
The variance of $X$ is defined as:
\begin{align*}
\mathrm{var}\left[X\right] = \int_{-\infty}^\infty  (\omega- E\left[X\right] )^2 f_X(\omega) d(\omega) = E\left[X^2\right]-(E\left[X\right])^2.
\end{align*}

\subsection{Joint Probability Distribution}

{When dealing with multiple random variables}, it is sometimes useful to use vector and matrix notations.
Let $X_1,X_2,\dots,X_N$ be $N$ discrete random variables. 
\begin{itemize}
\item The \textit{joint probability function} of  $X_1,X_2,\dots,X_N$ is  denoted as
\begin{equation*}
p(x_1,x_2,\dots,x_N) := p_{X_1,X_2,\dots,X_N}(x_1,x_2,\dots,x_N) =P(X_1=x_1,X_2=x_2,\dots,X_N=x_N).
\end{equation*}
\item Using vector notation we can write this distribution as
\begin{equation*}
p(\mathbf x) := p_{\mathbf X}(\mathbf x) =P(\mathbf X = \mathbf x)
\end{equation*}
where $\mathbf X = [X_1,X_2,\dots,X_N]^T$ is a column (random) vector having $X_1,X_2,\dots,X_N$ as its components.
Similarly $\mathbf x = [x_1,x_2,\dots,x_N]^T$.
Note the abuse of notation in defining $p(\mathbf x)$ and $p(x_1,x_2,\dots,x_N)$ above.
\item   Let $X_1, X_2, \dots X_N$ be a set of discrete random variables with joint distribution $ p(x_1,x_2,\dots,x_N)$.  Variable $X_n$ can be \textit{marginalized} 
from the joint distribution function as follows. 
 \begin{align*}  
 p(\mathbf x_{\neg n}) &= p(x_1,x_2,\dots,x_{n-1},x_{n+1},\dots, x_N) 
                             &= \sum_{x_n} p(x_1,x_2,\dots,x_N).
 \end{align*} 
 
 \bmp{0.5\tw}
\begin{center}
\small
\begin{tabular}{ccc|c}
$x_1$ & $x_2$ & $x_3$ & $p(x_1, x_2,x_3)$\\
 \hline  0 & 0 & 0 & $\theta_1$\\ 
 0 & 0 & 1& $\theta_2$ \\  
 0 & 1 & 0 & $\theta_3$ \\  
 0 & 1 & 1 & $\theta_4$ \\ 
 1 & 0 & 0 & $\theta_5$\\
 1 & 0 & 1& $\theta_6$ \\ 
 1 & 1 & 0 & $\theta_7$ \\ 
 1 & 1 & 1 & $\theta_8$ \\ 
\end{tabular}
\end{center}
\emp
 \bmp{0.5\tw}
\begin{center}
\small
\begin{tabular}{cc|c}
$x_1$ & $x_2$ & $p(x_1, x_2)= \sum_{x_3}p(x_1, x_2,x_3)$\\
 \hline  0 & 0 & $\theta_1+\theta_2$\\ 
 0 & 1 & $\theta_3+\theta_4$ \\  
 1 & 0 & $\theta_5+\theta_6$\\
 1 & 1 & $\theta_7+\theta_8$ \\ 
\end{tabular}
\end{center}
\emp
  \vspace{0.5cm}
 \item   Let $X_1, X_2$ be two discrete random variables. The \textit{conditional distribution function} of $X_1$ given  $X_2$ is  defined as 
 \begin{align*}  
 p(x_1\mid x_2) = \frac{p(x_1,x_2)}{p(x_2)},\\
 p(x_1,x_2) =  p(x_1\mid x_2)p(x_2) = p(x_2\mid x_1)p(x_1), 
 \end{align*} 
 with $p(x_2)> 0$.
 
 \vspace{0.5cm}
 \bmp{0.5\tw}
\begin{center}
\small
\begin{tabular}{cc|c}
$x_1$ & $x_2$ & $p(x_1, x_2)$\\
 \hline  0 & 0 & $\theta_1+\theta_2$\\ 
 0 & 1 & $\theta_3+\theta_4$ \\  
 1 & 0 & $\theta_5+\theta_6$\\
 1 & 1 & $\theta_7+\theta_8$ \\ 
\end{tabular}
\end{center}
\emp
\bmp{0.5\tw}
\begin{center}
\small
\begin{tabular}{c|c}
 $x_2$ &$p(x_2)= \sum_{x_1}p(x_1, x_2)$\\
 \hline  0 &  $\theta_1+\theta_2+\theta_5+\theta_6$\\ 
 1 & $\theta_3+\theta_4+\theta_7+\theta_8$ \\  
\end{tabular}
\end{center}
\emp

  \vspace{0.5cm} 
  \bmp{0.5\tw}
\begin{center}
\small
\begin{tabular}{cc|c}
$x_1$ & $x_2$ & $p(x_1 \mid x_2)$\\
 \hline  0 & 0 & $(\theta_1+\theta_2)/(\theta_1+\theta_2+\theta_5+\theta_6)$\\ 
 0 & 1 & $(\theta_3+\theta_4)/(\theta_3+\theta_4+\theta_7+\theta_8)$ \\  
 1 & 0 & $(\theta_5+\theta_6)/(\theta_1+\theta_2+\theta_5+\theta_6)$\\
 1 & 1 & $(\theta_7+\theta_8)/(\theta_3+\theta_4+\theta_7+\theta_8)$ \\ 
\end{tabular}
\end{center}
\emp
 \bmp{0.5\tw}
\begin{center}
\small
$\sum_{x_1} p(x_1\mid x_2) = 1$
\end{center}
\emp

  \vspace{0.5cm}
  \item   Let $X_1, X_2$, and $X_3$ be three discrete random variables. Compute $p(x_2\mid x_3)$ from the table given below. Compute $\sum_{x_2} p(x_2\mid x_3,x_1)$.
  
   \vspace{0.5cm}
 \bmp{0.5\tw}
\begin{center}
\begin{tabular}{ccc|c}
$x_2$ & $x_1$ & $x_3$ & $ p(x_2\mid x_3,x_1)$\\ \hline  0 & 0 & 0 & $\theta_1$\\ 0 & 0 & 1& $\theta_2$ \\  0 & 1 & 0 & $\theta_3$ \\  0 & 1 & 1 & $\theta_4$ \\ 
 1 & 0 & 0 & $1 - \theta_1$\\1 & 0 & 1& $1- \theta_2$ \\ 1 & 1 & 0 & $1-\theta_3$ \\ 1 & 1 & 1 & $1-\theta_4$ \\ 
\end{tabular}
\end{center}
\emp
 \bmp{0.5\tw}
\begin{center}
\small
\begin{tabular}{cc|c}
$x_2$ & $x_3$ & $p(x_2\mid x_3)$\\
 \hline  0 & 0 & $\theta_1+\theta_3$\\ 
 0 & 1 & $\theta_2+\theta_4$ \\  
 1 & 0 & $1-(\theta_1+\theta_3)$\\
 1 & 1 & $1-(\theta_2+\theta_4)$ \\ 
\end{tabular}
\end{center}
\emp
 
  \vspace{0.5cm}
 \item  Let $\mathbf X = \{X_1, X_2,\dots,X_N\}$, and $\mathbf Y = \{Y_1, Y_2,\dots,Y_M\}$ be two sets of discrete random variables. The \textit{conditional distribution function} of $\mathbf X$ given  $\mathbf Y $ is  defined as 
 \begin{equation*}  
 p(\mathbf x\mid \mathbf y) = \frac{p(\mathbf x,\mathbf y)}{p(\mathbf y)}
 \end{equation*} 
 with $p(\mathbf y)> 0$.
 
 \item   Let $X_1, X_2$ be two discrete random variables. The Bayes rule establishes that
 \begin{equation*}  
 p(x_1\mid x_2) = \frac{p(x_1,x_2)}{p(x_2)}=\frac{p(x_2\mid x_1)p(x_1)}{p(x_2)}=\frac{p(x_2\mid x_1)p(x_1)}{\sum_{x_1} p(x_2\mid x_1)p(x_1)}
 \end{equation*} 
 with $p(x_2)> 0$.

 \item Let $\mathbf X = \{X_1, X_2,\dots,X_N\}$, and $\mathbf Y = \{Y_1, Y_2,\dots,Y_M\}$ be two sets of discrete random variables. The \textit{Bayes rule} establishes that
 \begin{equation*}  
 p(\mathbf x \mid \mathbf y ) = \frac{p(\mathbf x, \mathbf y)}{p(\mathbf x)} = \frac{p(\mathbf y \mid \mathbf x )p(\mathbf x )}{p(\mathbf y )}=\frac{p(\mathbf y \mid\mathbf x )p(\mathbf y )}{\sum_{\mathbf x } p(\mathbf y \mid \mathbf x )p(\mathbf x )}
 \end{equation*} 
 with $p(\mathbf y )> 0$. 
 
 \item Let $X_1, X_2$ be two discrete random variables. These variables are independent if 
 \begin{equation*}  
 p(x_1, x_2) = p(x_1)p(x_2),
 \end{equation*}  
 or alternatively,
 \begin{equation*}  
 p(x_1\mid x_2) = p(x_1).
  \end{equation*} 
  
\item Let $\mathbf X = \{X_1, X_2,\dots,X_N\}$, and $\mathbf Y = \{Y_1, Y_2,\dots,Y_M\}$ be two sets of discrete random variables. These sets of  variables are \textit{independent} if 
 \begin{equation*}  
 p(\mathbf x, \mathbf y) = p(\mathbf x)p(\mathbf y),
 \end{equation*}  
 or alternatively,
 \begin{equation*}  
 p(\mathbf x\mid \mathbf y) = p(\mathbf x).
  \end{equation*}   
  
 \item Let $X_1, X_2, X_3$ be three discrete random variables. We say that $X_1$ is \textit{conditionally independent} of $X_2$ given $X_3$ (denoted by $X_1 \ci X_2 \mid X_3$)  if
 \begin{equation*}  
 p(x_1, x_2\mid x_3) = p(x_1\mid x_3)p(x_2\mid x_3)
 \end{equation*}  
or alternatively,
 \begin{equation*}  
 p(x_1\mid x_2, x_3) = p(x_1\mid x_3).
  \end{equation*}  
  
\item  Let $\mathbf X = \{X_1, X_2,\dots,X_N\}$, $\mathbf Y = \{Y_1, Y_2,\dots,Y_M\}$ and $\mathbf Z = \{Z_1, Z_2,\dots,Z_L\}$ be three sets of discrete random variables. We say that $\mathbf X$ is \textit{conditionally independent} of $\mathbf Y$ given $\mathbf Z$ (denoted by $\mathbf X \ci \mathbf Y \mid \mathbf Z$)  if
 \begin{equation*}  
 p(\mathbf x, \mathbf y\mid \mathbf z) = p(\mathbf x\mid \mathbf z)p(\mathbf y\mid \mathbf z)
 \end{equation*}  
or alternatively,
 \begin{equation*}  
 p(\mathbf x\mid \mathbf y, \mathbf z) = p(\mathbf x\mid \mathbf z).
  \end{equation*}   

\item  \textit{Bayesian networks in brief}  A Bayesian network is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). 
Bayesian networks are directed acyclic graphs (DAGs) whose nodes represent variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters, or hypotheses. Each edge represents a direct conditional dependency. Any pair of nodes that are not connected (i.e. no path connects one node to the other) represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. Bayesian networks having $X_1,X_2,\dots,X_N$  variables factorize as
\begin{align*}
p(x_1,x_2,\dots,x_N) = \prod_{n=1}^N p(x_n|\mathsf{pa}(X_n)),
\end{align*}
where $\mathsf{pa(x_n)}$ represent the parents of variable $X_a$ in the associated DAG.

\begin{mybox}{Bayesian Network Example }{theoexample}
Consider the example of a Bayesian network having three random variables  $X_1,X_2,X_3$.
\vspace{0.8cm}
 \begin{center}
\scalebox{0.9}{\begin{tikzpicture}[dgraph]
\node[cont] (A) at (0,1) {$X_1$};
\node[cont] (C) at (1,0) {$X_2$};
\node[cont] (B) at (2,1) {$X_3$};
\draw(A) -- (C);\draw(B) -- (C);
\end{tikzpicture}}
\end{center}
$$p(x_1,x_2,x_3) = p(x_2\mid x_1, x_3)p(x_3)p(x_1)$$
\bmp{0.5\tw}
\begin{center}
\begin{tabular}{ccc|c}
$x_2$ & $x_1$ & $x_3$ & $ p(x_2\mid x_3,x_1)$\\ \hline  0 & 0 & 0 & $\theta_1$\\ 0 & 0 & 1& $\theta_2$ \\  0 & 1 & 0 & $\theta_3$ \\  0 & 1 & 1 & $\theta_4$ \\ 
 1 & 0 & 0 & $1 - \theta_1$\\1 & 0 & 1& $1- \theta_2$ \\ 1 & 1 & 0 & $1-\theta_3$ \\ 1 & 1 & 1 & $1-\theta_4$ \\ 
\end{tabular}
\end{center}
\emp
\bmp{0.5\tw}
\begin{center}
\begin{tabular}{c|c}
$x_1$ & $p(x_1)$ \\ \hline  0 & $\theta_5$\\  1 & $1-\theta_5$  \\
\end{tabular}
\end{center}
\begin{center}
\begin{tabular}{c|c}
$x_3$ & $p(x_3)$ \\ \hline  0 & $\theta_6$\\  1 & $1-\theta_6$  \\ 
\end{tabular}
\end{center}
\emp
\end{mybox}
\vspace{0.5cm}
\item \textit{Bayesian Network Examples  }


Solution.
\vspace{0.5cm}

Draw the DAG associated with the following probability distributions:
	\begin{enumerate}
		\item $p(x_1,x_2,x_3) = p(x_2)p(x_3)p(x_2\mid x_1,x_3)$. Show that $X_1 \ci X_3 $.
\begin{center}
\scalebox{0.8}{
\begin{tikzpicture}[dgraph]
\node[contblk,label=above:{$X_1$}] (x1) at (0,0) {};
\node[contblk, label=above:{$X_2$}] (x2) at (1.5, 0) {};
\node[contblk, label=above:{$X_3$}] (x3) at (3,-0) {};
\draw [-{latex[slant=0]}]  (x1) -- (x2);
\draw [-{latex[slant=0]}] (x3) -- (x2);
\end{tikzpicture}}
\end{center}	
		\item $p(x_1,x_2,x_3) = p(x_1)p(x_2|x_1)p(x_3|x_2)$. Show that $X_1 \ci X_3 | X_2$.		
\begin{center}
\scalebox{0.8}{
\begin{tikzpicture}[dgraph]
\node[contblk,label=above:{$X_1$}] (x1) at (0,0) {};
\node[contblk, label=above:{$X_2$}] (x2) at (1.5, 0) {};
\node[contblk, label=above:{$X_3$}] (x3) at (3,-0) {};
\draw [-{latex[slant=0]}]  (x1) -- (x2);
\draw [-{latex[slant=0]}] (x2) -- (x3);
\end{tikzpicture}}
\begin{align*}
p(x_1,x_3\mid x_2) &= \frac{p(x_1,x_2,x_3)}{p(x_2)}\\
&= \frac{p(x_1)p(x_2|x_1)p(x_3|x_2)}{p(x_2)}\\
&= \frac{p(x_1,x_2)p(x_3|x_2)}{p(x_2)}\\
&= \frac{p(x_1\mid x_2)p(x_2)p(x_3|x_2)}{p(x_2)}\\
&= p(x_1|x_2)p(x_3|x_2)\\ & \Longrightarrow X_1 \ci X_3 | X_2
\end{align*}
\end{center}		
		\item $p(x_1,x_2,x_3) = p(x_2)p(x_1|x_2)p(x_3|x_2)$. Show that $X_1\ci X_3 | X_2$.		
\begin{center}
\scalebox{0.8}{
\begin{tikzpicture}[dgraph]
\node[contblk,label=above:{$X_1$}] (x1) at (0,0) {};
\node[contblk, label=above:{$X_2$}] (x2) at (1.5, 0) {};
\node[contblk, label=above:{$X_3$}] (x3) at (3,-0) {};
\draw [-{latex[slant=0]}]  (x2) -- (x1);
\draw [-{latex[slant=0]}] (x2) -- (x3);
\end{tikzpicture}}
\end{center}
\begin{align*}
p(x_1,x_3\mid x_2) &= \frac{p(x_1,x_2,x_3)}{p(x_2)}\\
&= \frac{p(x_2)p(x_1|x_2)p(x_3|x_2)}{p(x_2)}\\
&= p(x_1|x_2)p(x_3|x_2)\\ & \Longrightarrow X_1 \ci X_3 | X_2
\end{align*}		
		\item $p(x_1,x_2,x_3,x_4) = p(x_1)p(x_2)p(x_3|x_1)p(x_4|x_1,x_2)$. Show that $X_3 \ci X_4 | X_1$.
\begin{center}
\scalebox{0.8}{
\begin{tikzpicture}[dgraph]
\node[contblk,label=above:{$X_3$}] (x3) at (0,0) {};
\node[contblk, label=above:{$X_1$}] (x1) at (1.5, 0) {};
\node[contblk, label=above:{$X_4$}] (x4) at (3, -0) {};
\node[contblk, label=above:{$X_2$}] (x2) at (4.5,-0) {};
\draw [-{latex[slant=0]}]  (x1) -- (x3);
\draw [-{latex[slant=0]}] (x1) -- (x4);
\draw [-{latex[slant=0]}]  (x2) -- (x4);
\end{tikzpicture}}
\end{center}		
		\item $p(x_1,x_2,x_3,x_4,x_5) = p(x_1)p(x_2)p(x_3|x_1)p(x_4|x_1,x_2)p(x_5|x_4)$. Show that $X_1 \ci X_5 | X_4$.
\begin{center}
\scalebox{0.8}{
\begin{tikzpicture}[dgraph]
\node[contblk,label=above:{$X_3$}] (x3) at (0,0) {};
\node[contblk, label=above:{$X_1$}] (x1) at (1.5, 0) {};
\node[contblk, label=above:{$X_4$}] (x4) at (3, -0) {};
\node[contblk, label=above:{$X_2$}] (x2) at (4.5,-0) {};
\node[contblk, label=left:{$X_5$}] (x5) at (3,-1.5) {};
\draw [-{latex[slant=0]}]  (x1) -- (x3);
\draw [-{latex[slant=0]}] (x1) -- (x4);
\draw [-{latex[slant=0]}]  (x2) -- (x4);
\draw [-{latex[slant=0]}]  (x4) -- (x5);
\end{tikzpicture}}
\end{center}		
		\item $p(x_1,x_2,x_3,x_4,x_5,x_6) = p(x_1)p(x_2|x_1)p(x_3|x_1)p(x_4|x_2)p(x_5|x_3)p(x_6|x_2,x_5).$ Show that $X_2 \ci X_3 | X_1$.
\begin{center}
\scalebox{0.8}{
\begin{tikzpicture}[dgraph]
\node[contblk,label=above:{$X_1$}] (x1) at (0,0) {};
\node[contblk,label=above:{$X_2$}] (x2) at (2, 0) {};
\node[contblk,label=left:{$X_3$}] (x3) at (0,-2) {};
\node[contblk,label=right:{$X_4$}] (x4) at (3,1.5) {};
\node[contblk,label=above:{$X_5$}] (x5) at (2,-2) {};
\node[contblk,label=above:{$X_6$}] (x6) at (3.5,-1) {};
\draw(x1) -- (x2);
\draw(x1) -- (x3);
\draw(x2) -- (x4);
\draw(x3) -- (x5);
\draw(x2) -- (x6);
\draw(x5) -- (x6);
\end{tikzpicture}}
\end{center}	

\item Write down the factorization of $p(x_1,x_2,x_3,x_4,x_5,x_6,x_7) $ encoded in the following DAG.
\begin{center}
\scalebox{.9}{
\begin{tikzpicture}[dgraph]
\node[contblk,label=above:{$X_3$}] (x3) at (0,0) {};
\node[contblk, label=above:{$X_1$}] (x1) at (1.5, 1.5) {};
\node[contblk, label=above:{$X_4$}] (x4) at (3, -0) {};
\node[contblk, label=above:{$X_2$}] (x2) at (4.5,1.5) {};
\node[contblk, label=above:{$X_5$}] (x5) at (6, -0) {};
\node[contblk, label=left:{$X_7$}] (x7) at (6, -1.5) {};
\node[contblk, label=above:{$X_6$}] (x6) at (7.5,1.5) {};

\draw [-{latex[slant=0]}]  (x1) -- (x3);
\draw [-{latex[slant=0]}] (x1) -- (x4);
\draw [-{latex[slant=0]}]  (x2) -- (x4);
\draw [-{latex[slant=0]}]  (x2) -- (x5);
\draw [-{latex[slant=0]}]  (x6) -- (x5);
\draw [-{latex[slant=0]}]  (x5) -- (x7);
\end{tikzpicture}}
\end{center}

	\end{enumerate}

\end{itemize}




\begin{mybox}{Application Example (Was it the burglar?) }{theoexample}
 Mary lives in San Francisco City. One afternoon, she is driving back home and receives a phone call from 
 her neighbor Jane. She told her that her house alarm was set off ($\mathsf  A$). While driving, she also heard on the radio
 ($\mathsf  R$) that a small earthquake ($\mathsf  E$) hit the city.  Small earthquakes sometimes activate the alarm, and perhaps
 this is the reason why the alarm was sounding.
 \begin{itemize}
 \item What is the probability that a burglar ($\mathsf B$) broke into the house?
 \item What is the probability that a burglar broke into the house
 given that the alarm was set off?
 \item What is the probability that a burglar broke into the house
 given that the alarm was set off and a small earthquake hit the city?
 \end{itemize} 
\end{mybox}

To solve this problem we make several considerations.
\begin{enumerate}
\item Define the random variables $R(\mathsf R )=1$, $R(\mathsf R^c )=0$, $A(\mathsf A )=1$, $A(\mathsf A^c )=0$,
$E(\mathsf E )=1$, $E(\mathsf E^c )=0$, and $B(\mathsf B )=1$, $B(\mathsf B^c )=0$.
\item Assume that the joint distribution of random variables $R$, $A$, $E$, and $B$ factorizes as 
 $$\small p(A,R,E,B)=p(E=e)p(B=b)p(A=a\mid B=b,E=e)p(R=r\mid E=e)$$ our using our simplified 
notation
 $$p(a,r,e,b)=p(e)p(b)p(a\mid b,e)p(r\mid e).$$
\end{enumerate}




The factors of the probability distribution correspond to the following tables.
 
  \bmp{0.4\tw}  
  \begin{center}
  \begin{tabular}{l l l }
    \hline
            & $b$ & $p(b)$ \\\hline\hline
     $\mathsf{B^c}$ & $\mathsf 0$ & $\mathsf{b_1 = .9}$\\
     $\mathsf{B}$    &  $\mathsf 1$ & $1-\mathsf{b_1}$\\     
     \hline
  \end{tabular}
  
  \vspace{0.2cm}
  \begin{tabular}{l l l }
    \hline
            & $e$ & $p(e)$ \\\hline\hline
     $\mathsf{E^c}$ & $\mathsf 0$ & $\mathsf{e_1 = 0.95}$\\
     $\mathsf{E}$    &  $\mathsf 1$ & $\mathsf{e_2=1-e_1}$ \\     
     \hline
  \end{tabular}
    \vspace{0.2cm}
%  \begin{tabular}{l l l }
%    \hline
%            & $r$ & $p(r)$ \\\hline\hline
%     $\mathsf{R^c}$ & $\mathsf 0$ & $\mathsf{c_1 = 0.05}$\\
%     $\mathsf{R}$    &  $\mathsf 1$ & $\mathsf{c_2=1-c_1}$\\     
%     \hline
%  \end{tabular}  
\end{center}
\emp
 \bmp{0.6\tw}  
  \begin{center}
  \begin{tabular}{l l l l l}
    \hline
                              &                           & $b$  & $e $ & $p(b,e)$ \\\hline\hline
     $\mathsf{B^c}$ &  $\mathsf{E^c}$ &  $\mathsf 0$     &   $\mathsf 0$    &  $\mathsf{b_1e_1}$\\
     $\mathsf{B^c}$ &  $\mathsf{E}$    &  $\mathsf 0$     &   $\mathsf 1$    &  $\mathsf{b_1e_2}$\\  
     $\mathsf{B}$    &  $\mathsf{E^c}$ &  $\mathsf 1$     &   $\mathsf 0$    &  $\mathsf{b_2e_1}$\\
     $\mathsf{B}$.   &  $\mathsf{E}$    &  $\mathsf 1$        &   $\mathsf 1$    &  $\mathsf{b_2e_2}$\\     
        
     \hline
  \end{tabular}
\end{center}
  \begin{center}
  \begin{tabular}{l l l l l}
    \hline
                              &                           & $r$  & $e $ & $p(r\mid e)$ \\\hline\hline
     $\mathsf{R^c}$ &  $\mathsf{E^c}$ &  $\mathsf 0$     &   $\mathsf 0$    &  $\mathsf f_1=0.99$\\
     $\mathsf{R^c}$ &  $\mathsf{E}$    &  $\mathsf 0$     &   $\mathsf 1$    &  $\mathsf f_2=0.01$\\  
     $\mathsf{R}$    &  $\mathsf{E^c}$ &  $\mathsf 1$     &   $\mathsf 0$    &  $\mathsf{f_3=1-f_1}$\\
     $\mathsf{R}$.   &  $\mathsf{E}$    &  $\mathsf 1$        &   $\mathsf 1$    &  $\mathsf {f_4=1-f_2}$\\     
        
     \hline
  \end{tabular}
\end{center}
\emp
\vspace{0.5cm}

   \bmp{0.6\tw}  
  \begin{center}
  \begin{tabular}{l l l l  l l l l l }
    \hline
               &       &     &  $ { a}$ & $ { b}$ & $ { e}$ &$ p(a\mid b,e)$ \\ \hline\hline
     $\mathsf{A^c}$ & $\mathsf{B^c}$ & $\mathsf{E^c}$   & $ \mathsf{ 0}$ & $\mathsf {0}$ & $  \mathsf {0}$ &$ \mathsf{q_1}$ \\ %\hline
     $\mathsf{A^c}$ & $\mathsf{B^c}$ & $\mathsf{E}$    & $ \mathsf{0}$ & $\mathsf {0}$ & $  \mathsf {1}$ &  $  \mathsf{q_2}$ \\ %\hline
     $\mathsf{A^c}$ & $\mathsf{B}$ & $\mathsf{E^c}$     & $\mathsf{ 0}$ & $\mathsf {1}$ & $ \mathsf {0}$ &  $ \mathsf{q_3}$ \\ %\hline
     $\mathsf{A^c}$ & $\mathsf{B}$ & $\mathsf{E}$    & $ \mathsf{ 0}$ & $\mathsf{1}$ & $  \mathsf{1}$ &  $ \mathsf{q_4}$ \\ %\hline
     $\mathsf{A}$ & $\mathsf{B^c}$ & $\mathsf{E^c}$     & $ \mathsf{ 1}$ & $\mathsf {0}$ & $  \mathsf {0}$ &  $  \mathsf{q_5=1-q_1}$\\ %\hline
     $\mathsf{A}$ & $\mathsf{B^c}$ & $\mathsf{E}$    & $ \mathsf{ 1}$ & $\mathsf {0}$ & $  \mathsf{1}$ &  $ \mathsf{q_6=1-q_2}$ \\ %\hline
     $\mathsf{A}$ & $\mathsf{B}$ & $\mathsf{E^c}$     & $ \mathsf{ 1}$ & $\mathsf{1}$ & $ \mathsf {0}$ &  $  \mathsf{q_7=1-q_3}$ \\ %\hline
     $\mathsf{A}$ & $\mathsf{B}$ & $\mathsf{E}$     & $ \mathsf{ 1}$ & $\mathsf{1}$ & $  \mathsf{1}$ &  $  \mathsf{q_8=1-q_4}$ \\ %\hline
    \hline
  \end{tabular}
\end{center}
\emp
\bmp{0.4\tw}  
  \begin{center}
  


\end{center}
\emp

\vspace{0.5cm}

The table representing the joint probability function is 
\vspace{0.5cm}

   \bmp{0.7\tw}  
  \begin{center}
  \begin{tabular}{l l l l  l l l l l }
    \hline
        &       &       &     &  $ {  a}$ & $ { r}$ & $ { b}$ & $ { e}$ &$ p(r,a,e,b)$ \\ \hline\hline
   $\mathsf{A^c}$ & $\mathsf{R^c}$ & $\mathsf{B^c}$ & $\mathsf{E^c}$ &  $ \mathsf{ 0}$ & $ \mathsf{ 0}$ & $\mathsf {0}$ & $  \mathsf {0}$ &$ \mathsf{p_1}=     \mathsf{b_1e_1f_1q_1}$ \\ %\hline
   $\mathsf{A^c}$ & $\mathsf{R^c}$ & $\mathsf{B^c}$ & $\mathsf{E}$ &  $ \mathsf{ 0}$  & $ \mathsf{0}$ & $\mathsf {0}$ & $  \mathsf {1}$ &  $  \mathsf{p_2}=    \mathsf{b_2e_1f_1q_1}$ \\ %\hline
   $\mathsf{A^c}$ & $\mathsf{R^c}$ & $\mathsf{B}$ & $\mathsf{E^c}$ &  $ \mathsf{ 0}$  & $\mathsf{ 0}$ & $\mathsf {1}$ & $ \mathsf {0}$ &  $ \mathsf{p_3}=     \mathsf{b_1e_2f_2q_2}$ \\ %\hline
   $\mathsf{A^c}$ & $\mathsf{R^c}$ & $\mathsf{B}$ & $\mathsf{E}$ &  $\mathsf{ 0}$  & $ \mathsf{ 0}$ & $\mathsf{1}$ & $  \mathsf{1}$ &  $ \mathsf{p_4}=      \mathsf{b_2e_2f_2q_2}$ \\ %\hline
   $\mathsf{A^c}$ & $\mathsf{R}$ & $\mathsf{B^c}$ & $\mathsf{E^c}$ &  $\mathsf{ 0}$  & $ \mathsf{ 1}$ & $\mathsf {0}$ & $  \mathsf {0}$ &  $  \mathsf{p_5}=     \mathsf{b_1e_1f_3q_3}$\\ %\hline
   $\mathsf{A^c}$ & $\mathsf{R}$ & $\mathsf{B^c}$ & $\mathsf{E}$ &  $\mathsf{ 0}$  & $ \mathsf{ 1}$ & $\mathsf {0}$ & $  \mathsf{1}$ &  $ \mathsf{p_6}=     \mathsf{b_2e_1f_3q_3}$ \\ %\hline
   $\mathsf{A^c}$ & $\mathsf{R}$ & $\mathsf{B}$ & $\mathsf{E^c}$ &  $\mathsf{ 0}$  & $ \mathsf{ 1}$ & $\mathsf{1}$ & $ \mathsf {0}$ &  $  \mathsf{p_7}=      \mathsf{b_1e_2f_4q_4}$ \\ %\hline
   $\mathsf{A^c}$ & $\mathsf{R}$ & $\mathsf{B}$ & $\mathsf{E}$ &  $\mathsf{0}$   & $ \mathsf{ 1}$ & $\mathsf{1}$ & $  \mathsf{1}$ &  $  \mathsf{p_8}=       \mathsf{b_2e_2f_4q_4}$ \\ %\hline
   $\mathsf{A}$ & $\mathsf{R^c}$ & $\mathsf{B^c}$ & $\mathsf{E^c}$ &  $ \mathsf{1}$  & $ \mathsf{ 0}$ & $\mathsf {0}$ & $ \mathsf {0}$ &  $ \mathsf{p_9}=     \mathsf{b_1e_1f_1q_5}$ \\ %\hline
   $\mathsf{A}$ & $\mathsf{R^c}$ & $\mathsf{B^c}$ & $\mathsf{E}$ &  $\mathsf{ 1}$  & $ \mathsf{ 0}$ & $\mathsf {0}$ & $  \mathsf{1}$ &  $  \mathsf{p_{10}}= \mathsf{b_2e_1f_1q_5}$ \\ %\hline
   $\mathsf{A}$ & $\mathsf{R^c}$ & $\mathsf{B}$ & $\mathsf{E^c}$ &  $\mathsf{ 1}$  & $ \mathsf{ 0}$ & $\mathsf{1}$ & $ \mathsf {0}$ &  $  \mathsf{p_{11}}=  \mathsf{b_1e_2f_2q_6}$ \\ %\hline
   $\mathsf{A}$ & $\mathsf{R^c}$ & $\mathsf{B}$ & $\mathsf{E}$ &  $\mathsf{ 1}$  & $ \mathsf{ 0}$ & $\mathsf{1}$ & $ \mathsf {1}$ &  $  \mathsf{p_{12}}=  \mathsf{b_2e_2f_2q_6}$ \\ %\hline
   $\mathsf{A}$ & $\mathsf{R}$ & $\mathsf{B^c}$ & $\mathsf{E^c}$ &  $\mathsf{ 1}$  & $ \mathsf{ 1}$ & $\mathsf{0}$ & $ \mathsf {0}$ &  $ \mathsf{p_{13}}=  \mathsf{b_1e_1f_3q_7}$ \\ %\hline
   $\mathsf{A}$ & $\mathsf{R}$ & $\mathsf{B^c}$ & $\mathsf{E}$ &  $\mathsf{ 1}$  & $ \mathsf{ 1}$ & $\mathsf{0}$ & $ \mathsf {1}$ &  $  \mathsf{p_{14}}=  \mathsf{b_2e_1f_3q_7}$ \\ %\hline
   $\mathsf{A}$ & $\mathsf{R}$ & $\mathsf{B}$ & $\mathsf{E^c}$ &  $\mathsf{ 1}$  & $ \mathsf{ 1}$ & $\mathsf{1}$ & $ \mathsf {0}$ &  $ \mathsf{p_{15}}=  \mathsf{b_1e_2f_4q_8}$ \\ %\hline
   $\mathsf{A}$ & $\mathsf{R}$ & $\mathsf{B}$ & $\mathsf{E}$ &  $\mathsf{ 1}$  & $ \mathsf{ 1}$ & $\mathsf{1}$ & $ \mathsf {1}$ &  $  \mathsf{p_{16}}=  \mathsf{b_2e_2f_4q_8}$ \\ %\hline

    \hline
  \end{tabular}
\end{center}
\emp
\bmp{0.3\tw}  
  \begin{center}
  
\end{center}
\emp  

\begin{marginfigure}
\centering
\begin{center}
\scalebox{.9}{
\begin{tikzpicture}[dgraph]
\node[contblk,label=above:{$R$}] (x3) at (0,0) {};
\node[contblk, label=above:{$E$}] (x1) at (1.5, 1.5) {};
\node[contblk, label=above:{$A$}] (x4) at (3, -0) {};
\node[contblk, label=above:{$B$}] (x2) at (4.5,1.5) {};
\draw [-{latex[slant=0]}]  (x1) -- (x3);
\draw [-{latex[slant=0]}] (x1) -- (x4);
\draw [-{latex[slant=0]}]  (x2) -- (x4);
\end{tikzpicture}}
\end{center}
\caption{Directed Acyclic Graph encoding the 
probability distribution $p(a,r,e,b)=p(e)p(b)p(a\mid b,e)p(r\mid e).$ Dark circles represent random variables $R$, $A$, $E$, and $B$. 
}

\end{marginfigure}

\vspace{0.5cm}

\begin{itemize}
\item The \textit{graphical model}  (a Directed Acyclic Graph or DAG) encoding the 
probability distribution factorization is shown in Figure 21.
\item Assume that  $q_1=0.98$, $q_2=0.9$, $q_3=0.1$, $q_4=0.01$.
\item What is the probability that a burglar broke into the house?
\begin{align*}
P(B=\mathsf 1) &= P(\mathsf{B}) = \\ &= \mathsf{ p_3+p_4+p_7+p_8+p_{11}+p_{12}+p_{15}+p_{16},}
\end{align*}
\begin{align*}
P(B=\mathsf 1) &= \sum_{a}\sum_{r}\sum_{e} p(r,a,B=1,e) = 0.1.
\end{align*}
%\item What is the computational complexity of computing $p(B)$?
\end{itemize}

\begin{itemize}
\item What is the probability that a burglar broke into the house
 given that the alarm was set off?
\begin{align*}
P(B=\mathsf 1\mid A=1) &= \frac{P(B=1,A=1 )}{P(A=1)}\\ &= \frac{\mathsf{ p_{11}+p_{12}+p_{15}+p_{16}}}{\mathsf{p_{9}+p_{10}+p_{11}+p_{12}+p_{13}+p_{14}+p_{15}+p_{16}}},
\end{align*}
\begin{align*}
P(B=\mathsf 1\mid A=1) &= \frac{\sum_{r}\sum_{e} p(r,A=1,B=1,e)}{\sum_{r}\sum_{b}\sum_{e} p(r,A=1,b,e) } &= 0.81.
\end{align*}
\end{itemize}

\begin{itemize}
\item What is the probability that a burglar broke into the house
 given that the alarm was set off and a small earthquake hit the city?
\begin{align*}
P(B=\mathsf 1\mid A=1, E=1) &= \frac{P(B=1,A=1,E=1 )}{P(A=1,E=1)}\\ &= \frac{\mathsf{p_{12} +p_{16}}}{\mathsf{p_{10}+ p_{12}+p_{14}+p_{16}}},
\end{align*}
\begin{align*}
P(B=\mathsf 1\mid A=1,E=1) &= \frac{\sum_{r}p(r,A=1,B=1,E=1)}{\sum_{r}\sum_{b} p(r,A=1,b,E=1) } \\&= 0.51.
\end{align*}
\end{itemize}

\begin{mybox}{Solution Summary }{theoexample}
\begin{itemize}
 \item What is the probability that a burglar broke into the house?
 \begin{align*}
P(B=\mathsf 1) &= 0.1.
\end{align*}
 \item What is the probability that a burglar broke into the house
 given that the alarm was set off?
 \begin{align*}
P(B=\mathsf 1\mid A=1) &= 0.81.
\end{align*}
 \item What is the probability that a burglar broke into the house given that the alarm was set off and a small earthquake hit the city?
 \begin{align*}
P(B=\mathsf 1\mid A=1,E=1) &= 0.51.
\end{align*}
\item The \textit{a priori} probability that the burglar broke into the house is $0.1$. The probability that a burglar broke into the house
 given that the alarm was set off is much higher  ($0.81$). However, knowing that a small earthquake hit the city \textit{explains away} the observation that the alarm was set off,
 diminishing the probability that a burglar broke ($0.51$). 
 \end{itemize} 
\end{mybox}
\begin{itemize}
 \item Note that the factorization features of the probability distribution and the sum-product distributive property help us reduce the computational complexity. 
 \begin{align*}
 p(b\mid a) = \frac{p(a,b)}{p(a)} &= \frac{\sum_r\sum_ep(r,a,e,b)}{\sum_b\sum_r\sum_ep(r,a,e,b)}\\
 					       &= \frac{\sum_r\sum_e p(e)p(b)p(a\mid e,b)p(r\mid e)}{\sum_b\sum_r\sum_e p(e)p(b)p(a\mid e,b)p(r\mid e)}\\
                                                 &= \frac{p(b)\sum_e p(e)p(a\mid e,b)\sum_rp(r\mid e)}{\sum_bp(b)\sum_e p(e)p(a\mid e,b)\sum_rp(r\mid e)}\\
                                                 &= \frac{p(b)\sum_e p(e)p(a\mid e,b)\phi_1(e)}{\sum_b p(b)\sum_e p(e)p(a\mid e,b)\phi_1(e)}\\
                                                  &= \frac{p(b)\phi_2(a,b)}{\sum_b p(b)\phi_2(a,b)}\\
                                                 &= \frac{p(b)\phi_2(a,b)}{\phi_3(a)}\\
 \end{align*}
\end{itemize}

\begin{mybox}{Application Example (Pairs of dice) }{theoexample}
\small
You are told that there are two pairs of coins. The first pair is fair, $$\theta_1(\mathsf H)=\theta_1(\mathsf T)=\frac{1}{2}.$$ The second pair is biased as both coins have probability $$\theta_2(\mathsf H)=\frac{2}{3},\,\, \theta_2(\mathsf T)=\frac{1}{3}$$  of producing heads  and  tails, respectively. One of the two pairs is chosen at random with probability 0.5 and thrown 10 times. The sum of the coins is recorded for each throw.
 \begin{enumerate}
\item If the throw results in the sequence "1010101010"? Which pair of dice was picked for the throw?
\item Repeat (1) for the sequence "2222200000". 
\item Assume that the result of the second coin is flipped ($\mathsf F$) with probability $\sf p=0.5$ before recording the result of the throw with probability. Repeat  1 and 2 under this assumption. Consider the case for which  $\sf p = 0.2$. Repeat exercises 1 and 2.
\vspace{0.2cm}

\noindent The graphical model associated with this problem is as follows.

\begin{center}
\scalebox{0.8}{
\begin{tikzpicture}[dgraph]
\node[contblk,label=above:{$\Theta$}] (theta) at (2.3,1.5) {};
\node[contblk, label=left:{$X_n$}] (xn) at (1, 0) {};
\node[contblk, label=right:{$Z_n$}] (zn) at (3.5, 0) {};
\node[contblk, label=below:{$Y_n$}] (yn) at (3.5,-2) {};
\node[contblk, label=left:{$S_n$}] (sn) at (1,-2) {};
\node[contblk, label=above:{$P$}] (p) at (5.0,-2) {};
\draw [-{latex[slant=0]}]  (theta) -- (xn);
\draw [-{latex[slant=0]}] (theta) -- (zn);
\draw [-{latex[slant=0]}]  (zn) -- (yn);
\draw [-{latex[slant=0]}]  (xn) -- (sn);
\draw [-{latex[slant=0]}]  (yn) -- (sn);
\draw [-{latex[slant=0]}]  (p) -- (yn);
\boxit{$N$}{(sn)}{(zn)}{0.9};
\end{tikzpicture}}
\end{center}
\end{enumerate}
\begin{itemize}
\item $X_n$ represents the $n$-th result from the throw of the first coin; $X_n(\sf H) = 1$ and $X_n(\sf T) = 0$, and
$$p(x_n\mid \theta) = \theta^{x_n}(1-\theta)^{(1-x_n)}. $$ 
\item $Z_n$ epresents the unobserved   result from the $n$-th throw of the second coin;  $Z_n(\sf F) = 1$ and $Z_n(\sf F^c) = 0$, and
$$p(z_n\mid \theta) = \theta^{z_n}(1-\theta)^{(1-z_n)}. $$
\item $Y_n$ represents the recorded result  from the $n$-th throw of the second coin; $Y_n(\sf H) = 1$ and $Y_n(\sf T) = 0$, and 
$$p(y_n\mid z_n, p) = p^{I(y_n \neq z_n)} (1-p)^{ I(y_n = z_n)}.$$
\item $S_n$ is a deterministic function of $X_n$ and $Y_n$,  $$S_n=X_n+Y_n,$$ $$p(s_n\mid x_n, y_n) = \mathrm I(s_n == x_n+y_n),$$ and
\begin{equation*}
 I(e) =
    \begin{cases}
      1, & \textrm{if}\,\,e = \mathsf{true};\\
      0, & \textrm{otherwise}.\\
    \end{cases}       
\end{equation*}
 
\end{itemize}
\end{mybox}

Let  $\mathbf X = (X_1,X_2, \dots, X_N)$,  $\mathbf Y = (Y_1,Y_2, \dots, Y_N)$,  $\mathbf Z = (Z_1,Z_2, \dots, Z_N)$,
$\mathbf S = (S_1,S_2, \dots, S_N)$, $f_n= I(y_n \neq z_n) = 1 - I(y_n = z_n)$, and $\mathbf f = (f_1,f_2, \dots, f_N)$. ($f_n$ indicates whether the second was flipped or not.) 
The joint probability distribution for  $\mathbf x, \mathbf z, \mathbf y, \mathbf s $ given $\theta$ and $p$ is

\begin{align*}
p(\mathbf x, \mathbf z, \mathbf y, \mathbf s, \theta,  p) &= p(\theta)p(p)\prod_{n=1}^{N} p(x_n\mid \theta)p(z_n\mid \theta)p(y_n\mid z_n, p) p(s_n \mid x_n,y_n)\\
p(\mathbf x, \mathbf z, \mathbf y, \mathbf s \mid \theta,  p) &= \frac{p(\mathbf x, \mathbf z, \mathbf y, \mathbf s, \theta,  p)}{p(\theta)p(p)}\\
p(\mathbf x, \mathbf z, \mathbf y, \mathbf s \mid \theta,  p)  &= \prod_{n=1}^{N} p(x_n\mid \theta)p(z_n\mid \theta)p(y_n\mid z_n, p) p(s_n \mid x_n,y_n)\\
&=  \prod_{n=1}^{N}  \theta^{x_n}(1-\theta)^{(1-x_n)}  \theta^{z_n}(1-\theta)^{(1-z_n)} p^{I(y_n \neq z_n)} (1-p)^{ I(y_n = z_n)}  I(s_n = x_n + y_n).\\
&=  \theta^{\sum_{n=1}^N x_n}(1-\theta)^{\sum_{n=1}^N(1-x_n)}  \theta^{\sum_{n=1}^N z_n}(1-\theta)^{\sum_{n=1}^N(1-z_n)} p^{\sum_{n=1}^N I(y_n \neq z_n)} (1-p)^{ \sum_{n=1}^N I(y_n = z_n)} \\
\end{align*}

Notice that 

$$x_n = s_n - y_n,$$
$$y_n = z_n(1-f_n)+(1-z_n)f_n,$$ and
$$x_n = s_n - z_n + 2z_nf_n - f_n.$$

We define $N_x$, and $N_z$ as follows,

$$ N_x = \sum_{n=1}^{N} (s_n - z_n + 2z_nf_n - f_n) = N_s - N_z + 2N_{zf}-N_f  \geq 0,$$
$$ N_s = \sum_{n=1}^{N} s_n,$$
$$ N_z = \sum_{n=1}^{N} z_n,$$
$$ N_{zf} = \sum_{n=1}^{N} z_nf_n,$$and
$$N_f = \sum_{n=1}^{N} f_n.$$

Therefore we can write the probability distribution function as follows,
\begin{align*}
p(\mathbf x, \mathbf z, \mathbf y, \mathbf s \mid \theta,  p) &=  \theta^{\sum_{n=1}^N x_n}(1-\theta)^{\sum_{n=1}^N(1-x_n)}  \theta^{\sum_{n=1}^N z_n}(1-\theta)^{\sum_{n=1}^N(1-z_n)} p^{\sum_{n=1}^N f_n} (1-p)^{ \sum_{n=1}^N (1-f_n)} \\
&=  \theta^{N_x}(1-\theta)^{(N-N_x)} \theta^{N_z}(1-\theta)^{(N-N_z)}p^{N_f}(1-p)^{N-N_f}\\
&=  \theta^{N_x + N_z }(1-\theta)^{(2N-(N_x+N_z))} p^{N_f}(1-p)^{N-N_f}\\
&= p(\mathbf s, \mathbf z, \mathbf f \mid \theta, p)
\end{align*}
Observe  that 
$N_x = N_x(\mathbf s,\mathbf z, \mathbf f),$
$N_z = N_z(\mathbf z),$ and therefore

$$ p(\mathbf x, \mathbf z, \mathbf y, \mathbf s \mid \theta, p) =  p(\mathbf s, \mathbf z, \mathbf f \mid \theta, p). $$  Also,

$$ p(\mathbf s \mid \theta, p) =  \sum_{\mathbf x}  \sum_{\mathbf z} \sum_{\mathbf y} p(\mathbf x, \mathbf z, \mathbf y, \mathbf s \mid \theta, p) =  \sum_{\mathbf f} \sum_{\mathbf z} p(\mathbf s, \mathbf z, \mathbf f \mid \theta, p),$$ and

\begin{align*}
p(\theta\mid \mathbf s, p) &= \frac{p(\mathbf s, \theta, p)}{p( \mathbf s, p)}= \frac{p(\mathbf s\mid \theta, p)p(\theta,p)}{p( \mathbf s, p)} \\
                                         &=  \frac{p(\mathbf s\mid \theta, p)p(\theta\mid p)p(p)}{p( \mathbf s\mid p)p(p)}\\
                                         &=  \frac{p(\mathbf s\mid \theta, p)p(\theta\mid p)}{p( \mathbf s\mid p)}\\
                                         &=  \frac{p(\mathbf s\mid \theta, p)p(\theta)}{p( \mathbf s\mid p)}. 
\end{align*}
since $\theta \ci p$.

 \begin{mybox}{Decision Rule }{theoexample}

$$ \mathrm I\left(\frac{p(\theta_1 \mid ,\mathbf s,  p)}  {p(\theta_2 \mid ,\mathbf s,  p)} > 1 \right)  \Rightarrow \text{choose $ \theta_1$},$$ 
which is equivalent to
$$ \mathrm I\left(\frac{p(\mathbf s \mid \theta_1, p)p(\theta_1)}  {p(\mathbf s \mid \theta_2, p)p(\theta_2)} > 1 \right)  \Rightarrow \text{choose $ \theta_1$},$$ 
since
$$p(\theta \mid ,\mathbf s,  p) \propto  p(\mathbf s \mid \theta, p)p(\theta) =    \sum_{\mathbf f} \sum_{\mathbf z} p(\mathbf s, \mathbf z, \mathbf f \mid \theta, p) p(\theta).$$

Notice that 

$$ p(\mathbf s|p) = \sum_{\theta} \sum_{\mathbf f} \sum_{\mathbf z} p(\mathbf s, \mathbf z, \mathbf f \mid \theta, p)p(\theta),$$ with $\theta \in \{\theta_1,\theta_2\}.$

\end{mybox}

\begin{marginfigure}
\centering
\includegraphics[width=4cm]{fig/12.png}
\caption{Probability distribution $p(N_s)$ in Box for $\theta= \frac{1}{2}$ for problem 3 in Box 9.
The expected value $\mu_{N_s}= 5$ and the entropy is $E(N_s)=2.71$.
}
\end{marginfigure}




These equations lead to the following results regarding question 3.

\begin{align*}
\frac{p([2, 2, 2, 2, 2, 0, 0, 0, 0, 0]\mid \frac{2}{3}, \frac{1}{2})p(\frac{2}{3})}{p([2, 2, 2, 2, 2, 0, 0, 0, 0, 0]\mid  \frac{1}{2}, \frac{1}{2})p(\frac{1}{2})} &= 1.80 \Longrightarrow  \theta_2 = \frac{2}{3}.\\
\frac{p([1,0,1,0,1,0,1,0,1,0]\mid \frac{2}{3}, \frac{1}{2})p(\frac{2}{3})}{p([1,0,1,0,1,0,1,0,1,0]\mid  \frac{1}{2}, \frac{1}{2})p(\frac{1}{2})} &= 0.13 \Longrightarrow  \theta_1 = \frac{1}{2}. 
\end{align*}

\begin{figure}
\centering
\includegraphics[width=3.8cm]{fig/1-99.100.png}\includegraphics[width=3.8cm]{fig/1-8.9.png}\includegraphics[width=3.8cm]{fig/1-4.5.png}

\includegraphics[width=3.8cm]{fig/4.5.png}\includegraphics[width=3.8cm]{fig/8.9.png}\includegraphics[width=3.8cm]{fig/99.100.png}
\caption{From left to right and from top to bottom, probability distributions $p(N_s)$ in Box for $\theta= \frac{1}{100}$,  $\theta$ are $\frac{1}{9}$, $ \frac{1}{5}$, $ \frac{99}{100}$,  $\frac{8}{9}$, $\frac{4}{5}$ for problem 3 in Box 9. The corresponding mean values, $\mu_{N_s}$, are $2.55$, $3.06$, $3.5$, $6.5$, $6.9$, respectively.
and $\mu_{N_s}=7.45$, respectively. The corrresponding entropies, $E(N_s)$, are $2.44$,  $2.56$,  $2.56$,  $2.44$, and  $2.22$, respectively. 
}
\end{figure}


Also, notice that
\begin{align*}
\mathrm E[\mathbf s\mid \theta, p] &=  \sum_{\mathbf s} \sum_{\mathbf f} \sum_{\mathbf z} \mathbf s p(\mathbf s, \mathbf z, \mathbf f \mid \theta, p),\\ 
\mathrm E[N_s\mid \theta, p] &=   \sum_{N_s}  N_s p(N_s\mid \theta, p).
\end{align*}
The  \textit{effect} of $\theta$ on $N_s$ is
\begin{align*}
e(\theta_1,\theta_2\mid p) = \mathrm E[N_s\mid \theta_1, p] - E[N_s\mid \theta_2, p]\\
\end{align*}

\begin{mydef}{ Entropy }{theoexample}  
The average amount of information of a discrtete random variable $X$ is the expectation of $\mathrm I(x) = -\log_2(p(x))$ with respect to the distribution $p(x)$ and is given by

\begin{align*}
H(X) = \mathrm E[\mathrm I(x))]  =  \mathrm E[-\log_2(p(x))] = -\sum_x p(x)\log_2(p(x)).
\end{align*}
\end{mydef}

\begin{marginfigure}
\centering
\includegraphics[width=4cm]{fig/Entropy.png}
\caption{Entropy of the random variable $X\sim \mathsf{Bernoulli(x\mid p)}$ .
}
\end{marginfigure}

The entropy of a Bernoulli  random variable  $X\sim \mathsf{Bernoulli(x\mid p)}$ is $$H(X\mid p) = -{p}\log_2({p})-(1-{p})\log_2(1-{p}).$$

\subsection{Kullback–Leibler Divergence}

The Kullback–Leibler divergence (also called KL-divergence,  relative entropy, and $I$-divergence) is a  measure of how one probability distribution $P_1$ is different from a second, reference probability distribution $P_2$. For discrete probability distributions, $P_1$ and $P_2$ defined on the same sample space, the relative entropy from $P_2$ to $P_1$ is defined to be
$$D_{KL}(P_1\| P_2) = -\sum_x p_1(x) \log_2(\frac{p_2}{p_1}).$$
In other words, it is the expectation of the logarithmic difference between the probabilities $P_1$ and $P_2$, where the expectation is taken using the probabilities $P_1$.

\subsection{Conjugate Priors}
Let $X$ and $\Theta$ be two random variables. In Bayesian probability theory, if the posterior distribution $p(\theta|x)$ is in the same probability distribution family as the prior probability distribution $p(\theta)$, the prior and posterior are then called conjugate distributions, and the prior is called a {\em conjugate prior} for the likelihood function $p(x| \theta)$. Recall that:

$$ \underbrace{p(\theta \mid  x)}_{posterior} =  \frac{\overbrace{p( x\mid \theta)}^{likelihood} \;\;\overbrace{p(\theta)}^{prior}}{\underbrace{p(x)}_{evidence}}, $$
where
$$ p(x) = \underbrace{\int p( x \mid \theta) p(\theta)d\theta}_{marginalization}.$$ 

For example, if $p(\theta)$ has a Beta distribution, and $p( x\mid \theta)$ has a binomial distribution, then $p(\theta \mid  x)$ also has a Beta distribution. More specifically,

\begin{align*}
& p( x\mid \theta) \sim \textsf{Binomial}(x, \mid N, p)\\
& p(\theta\mid \alpha, \beta) \sim \textsf{Beta}(\theta\mid \alpha, \beta)\\
& p(\theta \mid  x) \sim  \textsf{Beta}(x\mid  \alpha + \sum_{n=1}^N x_i, \beta + N- \sum_{n=1}^N x_i)\\
\end{align*}

Consider the probability distribution from Box 9:
\begin{align*}
p(\mathbf s, \mathbf z, \mathbf f \mid \theta, p) &=  \theta^{N_x + N_z }(1-\theta)^{(2N-(N_x+N_z))} p^{N_f}(1-p)^{N-N_f}\\
\end{align*}
Clearly,
\begin{align*}
p(\theta, p \mid \mathbf s, \mathbf z, \mathbf f ) &= \textsf{Beta}(\theta\mid \alpha + \alpha_1  , \beta + \beta_1 ) \textsf{Beta}(p\mid  \gamma + \gamma_1  , \delta + \delta_1) \\
\end{align*}
where $\alpha_1 =  N_x + N_z$, $\beta_1 = N - (N_x + N_z )$, $\delta_1 = N_f$, $\gamma_2 = N-N_f$,
$p(\theta\mid \alpha, \beta) =  \textsf{Beta}(\theta\mid \alpha, \beta)$ and $p(s\mid \gamma, \delta) = \textsf{Beta}(s\mid \gamma, \delta)$.

Marginalizing with respect to $p$, $\mathbf z$, and $\mathbf f$ we obtain:
\vspace{0.1cm}
\begin{align*}
p(\theta \mid \mathbf s) &= \sum_{\mathbf z}\sum_{\mathbf f}\textsf{Beta}(\theta\mid \alpha + \alpha_1  , \beta + \beta_1 )\int_{p} \textsf{Beta}(p\mid  \gamma + \gamma_1  , \delta + \delta_1) dp, \\
 &= \sum_{\mathbf z}\sum_{\mathbf f}\textsf{Beta}(\theta\mid \alpha + \alpha_1  , \beta + \beta_1 ).
\end{align*}




\subsection{Expected Value and Covariance Matrix of Random Vectors}

Let $X_1,X_2,\dots,X_N$ be $N$ discrete random variables with joint probability distribution
$p(x_1,x_2,\dots,x_N)$. Let  $\mathbf X= [X_1,X_2,\dots,X_N]^T$

\begin{itemize}
\item The expected value of $\mathbf X$ is given by  
$$\mathrm E[\mathbf X] =  \sum_{\mathbf x} \mathbf x p(\mathbf x)  = \left[\mathrm E[X_1], \mathrm E[X_2],\dots, \mathrm E[X_N] \right]^T.$$

\item The \textit{covariance} of two discrete random variables $X$ and $Y$ is given by
\begin{equation*}
\mathrm {cov}(X,Y) =  \mathrm E[(X-\mu_x)(Y-\mu_y)] = \mathrm  E[XY]  - \mu_x\mu_y,
\end{equation*}
where 
$$\mathrm E[XY] = \sum_{\omega_x\in \Omega_x}\sum_{\omega_y\in \Omega_y} X(\omega_x)Y( \omega_y) P(X=X(\omega_x),Y=Y(\omega_y)).$$
Using the notation defined in the previous section we can write
$$\mathrm E[XY] = \sum_{x}\sum_{y} xy p(x,y).$$


\item  The covariance of vector $\mathbf X$ is defined as

$$
 \mathrm {cov}(\mathbf X) =
\begin{bmatrix}
\mathrm {cov}(X_1) & \mathrm{ cov}(X_1,X_2)  & \dots & \mathrm{ cov}(X_1,X_N) \\
 \mathrm{ cov}(X_1,X_2) & \mathrm {var}(X_2)  & \dots &   \mathrm{ cov}(X_2,X_N) \\
\cdots &\cdots  &\cdots  & \cdots\\
 \mathrm{ cov}(X_1,X_N) &  \mathrm{ cov}(X_2,X_N)  & \dots &   \mathrm {var}(X_N)
\end{bmatrix}
$$

\item  The covariance of the random vectors $\mathbf X$ and $\mathbf Y$ is defined as
$$
 \mathrm {cov}(\mathbf X,\mathbf Y) =
\begin{bmatrix}
\mathrm {cov}(X_1,Y_1) & \mathrm{ cov}(X_1,Y_2)  & \dots & \mathrm{ cov}(X_1,Y_N) \\
 \mathrm{cov}(X_2,Y_1) & \mathrm {cov}(X_2,Y_2)  & \dots &   \mathrm{ cov}(X_2,Y_N) \\
\cdots &\cdots  &\cdots  & \cdots\\
 \mathrm{ cov}(X_N,Y_1) &  \mathrm{ cov}(X_N,Y_2)  & \dots &   \mathrm {cov}(X_N,Y_N)
\end{bmatrix}
$$

\end{itemize}

\subsection{The Multivariate Gaussian Distribution}

The Gaussian, also known as the normal distribution, is a widely used model for the distribution of continuous variables. In the case of a single variable x, the Gaussian distribution (probability density function) can be written in the form
  \begin{equation*}
 \mathcal{N}(x \mid\mu,\sigma^2)  = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\{-\frac{1}{2\sigma^2} (x-\mu)^2\},
  \end{equation*}
where $\mu$ is the mean and $\sigma^2$ is the variance. For a $N$-dimensional vector $\mathbf x$, the multivariate Gaussian distribution takes the form
  \begin{equation*}
 \mathcal{N}(\mathbf x \mid \boldsymbol \mu,\mathbf \Sigma)  = \frac{1}{(2\pi)^{\frac{D}{2}}} 
 \frac{1}{\|\boldsymbol \Sigma\|^{\frac{1}{2}}}   \exp\{-(\mathbf x-\boldsymbol \mu)^T\boldsymbol \Sigma^{-1} (\mathbf x-\boldsymbol \mu)\},
  \end{equation*}
where $\boldsymbol \mu$ is a $N$-dimensional mean vector, $\boldsymbol \Sigma$ is a $N \times N$ covariance matrix, and $\|\boldsymbol \Sigma\|$ denotes the determinant of $\Sigma$.



\begin{marginfigure}
\centering
\includegraphics[width=4cm]{fig/G2D.png}
\caption{Surface plot of a two-dimensional Gaussian density.
}
\end{marginfigure}


%\begin{figure}
%\centering
%\def\centerx{2}
%\def\centery{-1}
%\begin{tikzpicture}
%  \begin{axis}
%    \addplot3[surf,domain=-2:6,domain y=-5:3] 
%        {exp(-( (x-\centerx)^2 + (y-\centery)^2)/3 )};
    %\node[circle,inner sep=1pt,fill=blue,pin=90:$\mu$] 
%        at (axis cs:\centerx,\centery,1) {};
%   \end{axis}
%\end{tikzpicture}
%\caption{Histogram and KDE plots  of $(X_1 + \dots + X_N )/N$ for $N=50,000$, with $X_n\sim \mathsf{Bernoulli}(x\mid p)$.
%The estimated mean and variance are $0.49$,  and $5.04e-06$, respectively.
%}
%\end{figure}


The Gaussian distribution arises in many different contexts and can be motivated from a variety of different perspectives. For example,  the Gaussian distribution arises when we consider the sum of multiple random variables. The central limit theorem (due to Laplace) tells us that, subject to certain mild conditions, the sum of a set of random variables, which is of course itself a random variable, has a distribution that becomes increasingly Gaussian as the number of terms in the sum increases. We can illustrate this by considering $N$ variables $N_1, \dots, X_N$ each of which has a uniform distribution over the interval $[0, 1]$, and then considering the density of the mean $(X_1 + \dots + X_N )/N$. For large $N$, this density tends to be a Gaussian density (Figures 31 and 32). 

\begin{marginfigure}
\centering
\includegraphics[width=4cm]{fig/G.png}
\caption{Histogram and KDE plots from $20,000$ samples of $(X_1 + \dots + X_N )/N$ for $N=50,000$, with $X_n\sim \mathsf{Bernoulli}(x\mid p)$.
The estimated mean and variance are $0.49$,  and $5.04e-06$, respectively.
}
\end{marginfigure}

\subsection{The Central Limit Theorem}

\begin{figure}[b]
\centering
\includegraphics[width=2.9 cm]{fig/A.png}\includegraphics[width=2.9cm]{fig/B.png}\includegraphics[width=2.9cm]{fig/C.png}\includegraphics[width=2.9 cm]{fig/D.png}
\caption{Top: Histogram and KDE plots  of $Z_N$  for various values of $N$: $1,2,10,$ and $100$ (see Definition 15). We observe that as $N$ increases, the density tends towards a Gaussian density. Bottom: Corresponding cumulative distribution functions. Here $X_n\sim \mathsf{Uniform}(x_n\mid 0,1)$ for wich $\mu = 0.5$ and  $\sigma^2 = \frac{1}{12}$ .
}
\end{figure}

In practice, the convergence to a Gaussian as $N$ increases can be rapid. One consequence of this result is that the binomial distribution, which is the sum of $N$ observations of the Bernoulli random variable, will tend to a Gaussian as $N\rightarrow \infty$ (Figure 31).

\begin{mythe}{The Central Limit Theorem (CLT) }{theoexample} 


Let $X_1, X_2,...,X_n$ be i.i.d. random variables with expected value $\mathrm E[X_i] = \mu < \infty$
 and variance $0<\mathrm{var}(X_i)=\sigma^2 < \infty$. Then, the random variable

\begin{align*}
Z_N &= \frac{\left[\sum_{n=1}^N X_n\right] - \mu}{\sqrt{N}\sigma}
\end{align*}
converges to the standard Gaussian (Normal) random variable as $N \rightarrow \infty$,


\begin{align*}
\lim_{N \rightarrow \infty} F_{Z_N}(z)  &= \lim_{N \rightarrow \infty} P(Z_N \leq z) 
= \Phi(z)\;\;\,, \forall z \in \mathbb R,
\end{align*}
where $\Phi(z)$ is the standard Gaussian cummulative distribution function 
$$\Phi(z) = \int_{-\infty}^z \mathcal N(\alpha\mid, 0,1) d\alpha.$$ That is,
$$ Z_{\infty} \sim \mathcal N(z\mid,0,1).$$

\end{mythe} 

Let $X_1, X_2,...,X_n$ be i.i.d. random variables with expected value $\mathrm E[X_i] = \mu < \infty$
 and variance $0<\mathrm{var}(X_i)=\sigma^2 < \infty$. Then, the random variable
\begin{align*}
\bar X_N = \frac{1}{N} \sum_{n=1}^N X_n, 
\end{align*} 
 i.e. the average of $X_1, X_2,..., X_N$, has a Gaussian probability density with mean $\mu$ and variance $\frac{\sigma^2}{N}$:
\begin{align*}
\bar X_N \sim \mathcal N(x\mid \mu, \frac{\sigma^2}{N}) \,\, \text{as} \,\, N\rightarrow \infty.
\end{align*}  
 Clearly,   $\lim_{N\rightarrow \infty} \bar X_N = \mu$, so the average is an \textit{unbiased estimator} of the mean.


\subsection{Chebyshev's Inequality}

Let $X$  be a random variable with a finite expected value $\mu$ and finite non-zero variance $\sigma^2$. Then for any real number $k > 0$,
\begin{align*}
P(\mid X-\mu \mid \geq k\sigma) \leq \frac{1}{k^2}.
\end{align*}
Only the case $k>1$ is useful. When $k \leq 1$, right-hand side $\frac{1}{k^2} \geq 1$ and the inequality is trivial as all probabilities are  $\leq 1$.

 \begin{mybox}{Example }{theoexample}
Suppose that an unbiased coin is thrown 100 times. What is the bound that the
number of heads will be greater than 70 or less than 30?
\begin{itemize}
\item Let $K$ be the number of heads. Because $K$  has a binomial distribution with $\mu=0.5$:
\item $\mathrm E[K]= N\mu=50$.
\item $ \mathrm{var}[K] = {N\mu(1-\mu)}=25$.
\item The standard deviation is  $ \mathrm{std}[K] = \sqrt{\mathrm{var}[K] }=\sqrt{25} = 5$.
\item The values $70$ and $30$ are $20$ units from the average, which is $4$ standard deviations (i.e., 
$20/5$).
\begin{align*}
P(\mid K-\mathrm E[K] \mid \geq 4 \mathrm{var}[K] ) \leq \frac{1}{4^2} = 0.0625.
\end{align*}
\item Repeat the previous exercise for a) $\mu=0.6$ and b) $\mu=0.4$.
\end{itemize}
\end{mybox}

\vspace{1cm}

\vspace{1cm}
\end{document}


